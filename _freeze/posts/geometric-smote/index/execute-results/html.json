{
  "hash": "811ea1c2b1c80fc2d1d0a12d67522eb7",
  "result": {
    "markdown": "---\ntitle: Geometric SMOTE algorithm\ndescription: Extending SMOTE's data generation mechanism.\nauthor: Georgios Douzas\ndate: '2022-05-01'\ncategories:\n  - Project\n  - Imbalanced Learning\nimage: featured.png\n---\n\n![Geometric SMOTE algorithm in action](featured.png)\n\n# Introduction\n\n[SMOTE](https://arxiv.org/pdf/1106.1813.pdf) is the most popular\n[oversampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) algorithm, while many variants of it\nhave been developed. It generates synthetic data between the line segment that connects two randomly chosen neighboring minority\nclass instances.\n\nOn the other hand, [Geometric SMOTE](https://www.sciencedirect.com/science/article/abs/pii/S0020025519305353) expands the data\ngeneration area by generating synthetic data inside a hypersphere defined by a randomly chosen minority class instance and one of\nits neighbors, either from the minority or majority class.\n\nThe following figure illustrates a typical sample generated from the two algorithms:\n\n![SMOTE vs Geometric SMOTE](smote_vs_gsmote.png)\n\n## Implementation\n\nI have developed a Python implementation of the Geometric SMOTE oversampler called\n[geometric-smote](https://github.com/georgedouzas/geometric-smote), which integrates seamlessly with the\n[Scikit-Learn](https://scikit-learn.org/stable/) and [Imbalanced-Learn](https://imbalanced-learn.org/stable/) ecosystems. You can\ncheck the [documentation](https://geometric-smote.readthedocs.io/en/latest/?badge=latest) for more details on installation and the\nAPI.\n\n## Functionality\n\nLet's first generate a binary class imbalanced dataset, represented by the input matrix `X` and the target vector `y`: \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Imports\nfrom sklearn.datasets import make_classification\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced data\nX, y = make_classification(\n  n_samples=1000,\n  n_features=10,\n  n_classes=2,\n  weights=[0.95, 0.05],\n  random_state=rnd_seed,\n  n_informative=7,\n  class_sep=0.1\n)\n```\n:::\n\n\nThe function `print_characteristics` extracts and prints the main characteristics of a binary class dataset. Specifically, it\nprints the number of samples, the number of features, the labels, and the number of samples for the majority and minority classes\nas well as the Imbalance Ratio, defined as the ratio between the number of instances of the majority and minority classes.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Imports\nfrom collections import Counter\n\n# Define function to print dataset's characteristics\ndef print_characteristics(X, y):\n  n_samples, n_features = X.shape\n  count_y = Counter(y)\n  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()\n  ir = n_samples_maj / n_samples_min\n  print(\n    f'Number of samples: {n_samples}',\n    f'Number of features: {n_features}',\n    f'Majority class label: {maj_label}',\n    f'Number of majority class samples: {n_samples_maj}',\n    f'Minority class label: {min_label}',\n    f'Number of minority class samples: {n_samples_min}',\n    f'Imbalance Ratio: {ir:.1f}',\n    sep='\\n'\n  )\n```\n:::\n\n\nI use the above function to print the main characteristics of the generated imbalanced dataset:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint_characteristics(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of samples: 1000\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 946\nMinority class label: 1\nNumber of minority class samples: 54\nImbalance Ratio: 17.5\n```\n:::\n:::\n\n\nThe class that represents the Geometric SMOTE oversampler is called `GeometricSMOTE`. The most basic functionality is to resample\nan imbalanced dataset. Following the Imbalanced-Learn API, the `fit_resample` method of a `GeometricSMOTE` instance can be used to\nresample the imbalanced dataset:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Imports\nfrom gsmote import GeometricSMOTE\n\n# Create GeometricSMOTE instance\ngeometric_smote = GeometricSMOTE(random_state=rnd_seed + 5)\n\n# Fit and resample imbalanced data\nX_res, y_res = geometric_smote.fit_resample(X, y)\n```\n:::\n\n\nAgain we can print the main characteristics of the rebalanced dataset:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint_characteristics(X_res, y_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of samples: 1892\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 946\nMinority class label: 1\nNumber of minority class samples: 946\nImbalance Ratio: 1.0\n```\n:::\n:::\n\n\nAs expected, the default behavior of the `GeometricSMOTE` instance is to generate the appropriate number of minority class\nsamples so that the resampled dataset is perfectly balanced.\n\nAs mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The\nfunction `calculate_cv_scores` calculates the average 5-fold cross-validation geometric mean and accuracy scores across 10 runs\nof a logistic regression classifier that is optionally combined with an oversampler through a pipeline:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.metrics import geometric_mean_score\n\n# Define function that calculates out-of-sample scores\ndef calculate_cv_scores(oversampler, X, y):\n  cv_scores= []\n  scorers = {'g_mean': make_scorer(geometric_mean_score), 'accuracy': make_scorer(accuracy_score)}\n  n_runs = 10\n  for ind in range(n_runs):\n    rnd_seed = 10 * ind\n    classifier = LogisticRegression(random_state=rnd_seed)\n    if oversampler is not None:\n      classifier = make_pipeline(\n        oversampler.set_params(random_state=rnd_seed + 4),\n        classifier\n      )\n    scores = cross_validate(\n      estimator=classifier,\n      X=X,\n      y=y,\n      scoring=scorers,\n      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)\n    )\n    cv_scores.append([scores[f'test_{scorer}'].mean() for scorer in scorers])\n  return np.mean(cv_scores, axis=0)\n```\n:::\n\n\nUsing the above function, we can calculate the out-of-sample performance when no oversampling is applied as well as when random\noversampling, SMOTE and Geometric SMOTE are used as oversamplers:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Imports\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\n\n# Calculate cross-validation scores\nmapping = {'No oversampling': None, 'Random Oversampling': RandomOverSampler(), 'SMOTE': SMOTE(), 'Geometric SMOTE': GeometricSMOTE()}\ncv_scores = {}\nfor name, oversampler in mapping.items():\n  cv_scores[name] = calculate_cv_scores(oversampler, X, y)\ncv_scores = pd.DataFrame(cv_scores, index = ['Geometric Mean', 'Accuracy'])\ncv_scores\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No oversampling</th>\n      <th>Random Oversampling</th>\n      <th>SMOTE</th>\n      <th>Geometric SMOTE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Geometric Mean</th>\n      <td>0.132235</td>\n      <td>0.592189</td>\n      <td>0.583983</td>\n      <td>0.602286</td>\n    </tr>\n    <tr>\n      <th>Accuracy</th>\n      <td>0.949400</td>\n      <td>0.670400</td>\n      <td>0.673700</td>\n      <td>0.662700</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nGeometric SMOTE outperforms the other methods when the geometric mean score is used as an evaluation metric. At the same time, the\nhighest accuracy is achieved when no oversampling is applied, although if the goal is to consider performance equally on all\nclasses, it is not a suitable metric for imbalanced data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}