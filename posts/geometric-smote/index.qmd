---
title: "Geometric SMOTE algorithm"
description: "Extending SMOTE's data generation mechanism."
author: "Georgios Douzas"
date: "2022-05-01"
categories: [Project, Imbalanced Learning]
image: "featured.png"
jupyter: python3
---

![Geometric SMOTE algorithm in action](featured.png)

# Introduction

[SMOTE](https://arxiv.org/pdf/1106.1813.pdf) is the most popular
[oversampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) algorithm, while many variants of it
have been developed. It generates synthetic data between the line segment that connects two randomly chosen neighboring minority
class instances.

On the other hand, [Geometric SMOTE](https://www.sciencedirect.com/science/article/abs/pii/S0020025519305353) expands the data
generation area by generating synthetic data inside a hypersphere defined by a randomly chosen minority class instance and one of
its neighbors, either from the minority or majority class.

The following figure illustrates a typical sample generated from the two algorithms:

![SMOTE vs Geometric SMOTE](smote_vs_gsmote.png)

## Implementation

I have developed a Python implementation of the Geometric SMOTE oversampler called
[geometric-smote](https://github.com/georgedouzas/geometric-smote), which integrates seamlessly with the
[Scikit-Learn](https://scikit-learn.org/stable/) and [Imbalanced-Learn](https://imbalanced-learn.org/stable/) ecosystems. You can
check the [documentation](https://geometric-smote.readthedocs.io/en/latest/?badge=latest) for more details on installation and the
API.

## Functionality

Let's first generate a binary class imbalanced dataset, represented by the input matrix `X` and the target vector `y`: 

```{python}
# Imports
from sklearn.datasets import make_classification

# Set random seed
rnd_seed = 43

# Generate imbalanced data
X, y = make_classification(
  n_samples=1000,
  n_features=10,
  n_classes=2,
  weights=[0.95, 0.05],
  random_state=rnd_seed,
  n_informative=7,
  class_sep=0.1
)
```

The function `print_characteristics` extracts and prints the main characteristics of a binary class dataset. Specifically, it
prints the number of samples, the number of features, the labels, and the number of samples for the majority and minority classes
as well as the Imbalance Ratio, defined as the ratio between the number of instances of the majority and minority classes.

```{python}
# Imports
from collections import Counter

# Define function to print dataset's characteristics
def print_characteristics(X, y):
  n_samples, n_features = X.shape
  count_y = Counter(y)
  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()
  ir = n_samples_maj / n_samples_min
  print(
    f'Number of samples: {n_samples}',
    f'Number of features: {n_features}',
    f'Majority class label: {maj_label}',
    f'Number of majority class samples: {n_samples_maj}',
    f'Minority class label: {min_label}',
    f'Number of minority class samples: {n_samples_min}',
    f'Imbalance Ratio: {ir:.1f}',
    sep='\n'
  )
```

I use the above function to print the main characteristics of the generated imbalanced dataset:

```{python}
print_characteristics(X, y)
```

The class that represents the Geometric SMOTE oversampler is called `GeometricSMOTE`. The most basic functionality is to resample
an imbalanced dataset. Following the Imbalanced-Learn API, the `fit_resample` method of a `GeometricSMOTE` instance can be used to
resample the imbalanced dataset:

```{python}
# Imports
from gsmote import GeometricSMOTE

# Create GeometricSMOTE instance
geometric_smote = GeometricSMOTE(random_state=rnd_seed + 5)

# Fit and resample imbalanced data
X_res, y_res = geometric_smote.fit_resample(X, y)
```

Again we can print the main characteristics of the rebalanced dataset:

```{python}
print_characteristics(X_res, y_res)
```

As expected, the default behavior of the `GeometricSMOTE` instance is to generate the appropriate number of minority class
samples so that the resampled dataset is perfectly balanced.

As mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The
function `calculate_cv_scores` calculates the average 5-fold cross-validation geometric mean and accuracy scores across 10 runs
of a logistic regression classifier that is optionally combined with an oversampler through a pipeline:

```{python}
# Imports
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate, StratifiedKFold
from sklearn.metrics import make_scorer, accuracy_score
from imblearn.pipeline import make_pipeline
from imblearn.metrics import geometric_mean_score

# Define function that calculates out-of-sample scores
def calculate_cv_scores(oversampler, X, y):
  cv_scores= []
  scorers = {'g_mean': make_scorer(geometric_mean_score), 'accuracy': make_scorer(accuracy_score)}
  n_runs = 10
  for ind in range(n_runs):
    rnd_seed = 10 * ind
    classifier = LogisticRegression(random_state=rnd_seed)
    if oversampler is not None:
      classifier = make_pipeline(
        oversampler.set_params(random_state=rnd_seed + 4),
        classifier
      )
    scores = cross_validate(
      estimator=classifier,
      X=X,
      y=y,
      scoring=scorers,
      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)
    )
    cv_scores.append([scores[f'test_{scorer}'].mean() for scorer in scorers])
  return np.mean(cv_scores, axis=0)
```

Using the above function, we can calculate the out-of-sample performance when no oversampling is applied as well as when random
oversampling, SMOTE and Geometric SMOTE are used as oversamplers:

```{python}
# Imports
from imblearn.over_sampling import RandomOverSampler, SMOTE

# Calculate cross-validation scores
mapping = {'No oversampling': None, 'Random Oversampling': RandomOverSampler(), 'SMOTE': SMOTE(), 'Geometric SMOTE': GeometricSMOTE()}
cv_scores = {}
for name, oversampler in mapping.items():
  cv_scores[name] = calculate_cv_scores(oversampler, X, y)
cv_scores = pd.DataFrame(cv_scores, index = ['Geometric Mean', 'Accuracy'])
cv_scores
```

Geometric SMOTE outperforms the other methods when the geometric mean score is used as an evaluation metric. At the same time, the
highest accuracy is achieved when no oversampling is applied, although if the goal is to consider performance equally on all
classes, it is not a suitable metric for imbalanced data.
