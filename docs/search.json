[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Georgios Douzas. I am a machine learning researcher at Nova IMS, University of Lisbon, and a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence, with multiple publications in machine learning and high-energy physics journals. My professional experience includes working as a software and machine learning engineer for various companies. Additionally, I often maintain or contribute to open-source projects."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nMachine Learning Engineer\nTrasys, Greece\n\\(8/20\\) - Present\nDesigned and implemented Parallel Distribution, a software tool for the European Medicines Agency that applies OCR on PDF documents and generates a comparison report. The primary language of implementation was Python, while various text mining and machine learning libraries were used. The frontend of the tool used HTML, CSS, and JavaScript to provide an interactive dashboard of the results.\nMachine Learning Researcher\nUniversity of Lisbon Nova IMS, Portugal\n\\(09/13\\) - \\(09/14\\) & \\(09/18\\) - Present\nDesigned, implemented, and tested various new approaches for the class imbalance problem. Research focused on clustering-based over-sampling methods that deal with the within-the-class imbalance problem. Additionally, Geometric SMOTE, an extension of the SMOTE algorithm, was proposed and implemented. The final publication presented results showing a significant improvement over SMOTE and its variations. Deep learning models, particularly Conditional Generative Adversarial Networks (CGANs), were also used as over-sampling methods with great success. The frameworks of the implementation were TensorFlow, Keras, and PyTorch. Work is published in high-impact machine learning journals. Implementation of the above algorithms was developed and made available as open-source software. Work in progress includes comparative experiments between variations of CGANs as over-samplers and the investigation of novel algorithms in the context of reinforcement learning.\nMachine Learning Engineer\nTripsta, Greece\n\\(10/17\\) - \\(08/18\\)\nDesigned and implemented the main parts of the company’s automated pricing system. These parts included machine learning estimators for the add-ons and the competitor’s prices and the application of metaheuristic algorithms for the budget multi-objective optimization problem. The training data of the various estimators were at the order of TB while the prediction time of the automated pricing system was required to be less than \\(100\\) msec for the incoming \\(50\\)K requests/sec. The implementation languages were Python, Java, and Scala, while Spark, Dask, Scikit-Learn and jMetal were used as distributed data processing, machine learning, and optimization frameworks/libraries.\nData Scientist\nQuantum Retail, Remote \\(12/16\\) - \\(09/17\\)\nWorked on demand forecasting and clustering for retail companies. Proposed and applied machine learning methods to improve the company’s main forecasting solution based on exponential smoothing of the time series data and adjustments guided by a seasonality curve. Boosting trees were selected as the final machine-learning model. Applied feature extraction that integrated the business logic and extensive model hyperparameter tuning, the forecasting precision was improved by \\(30\\)% compared to the original model.\nMachine Learning Engineer\nCERN, Remote\n\\(05/16\\) - \\(09/16\\)\nDeveloped the parallelization of various features for TMVA, the Toolkit for Multivariate Data Analysis with ROOT, as a part of a project funded by Google. ROOT is the main framework developed by CERN to deal with the big data processing, statistical analysis, visualization, and storage of massive amounts of data produced from particle physics experiments. The legacy version was implemented in C++. The parallelized features included the application of brute-force and metaheuristic algorithms to the hyperparameter grid search of machine learning algorithms. The implementation was based on Python and Spark.\nScientific Software Engineer IRI, Greece\n\\(01/14\\) - \\(05/16\\)\nMember of the IRI’s “Solutions and Innovation Team” (R&D) working on the company’s transition towards Open Source and Elastic Computing. Participated in an agile team migrating IRI’s leading US “Price & Promo Analytics” Solution, generating more than \\(\\$25\\)M Annual Revenues, to Hadoop distributed storage and Spark cluster computing. Python was the core language of the implementation, but integration with R and Julia was performed to leverage unique functionality. The legacy version was implemented in SAS. The project’s main objectives were the design of the parallelization schema, the enhancement of data manipulation with the use of distributed processing, and the migration of the statistical modeling algorithms (regression mixed models). The final system processed \\(5\\) years of data for more than \\(300\\) categories containing \\(1\\) million products."
  },
  {
    "objectID": "posts/prefect/index.html",
    "href": "posts/prefect/index.html",
    "title": "Prefect",
    "section": "",
    "text": "Introduction\nPrefect is a workflow orchestration tool. It makes accessible the creation, scheduling, and monitoring of complex data pipelines. The workflows are defined as Python code, while Prefect provides error handling, retry mechanisms, and a user-friendly dashboard for monitoring.\n\n\nWorkflow for soccer data\nAs an example, let’s assume that we would like to create a data workflow that downloads, stores and updates historical and fixtures soccer data from Football-Data.co.uk. The URL of each of those main leagues has the following form:\n\nbase_url = 'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv'\nbase_url\n\n'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv'\n\n\nwhere season is the season of the league and league_id is the league ID. Let’s select a few of those seasons and leagues:\n\nSEASONS = ['1819', '1920', '2021', '2122', '2223', '2324']\nLEAGUES_MAPPING = {\n    'E0': 'English',\n    'SC0': 'Scotish',\n    'D1': 'German',\n    'I1': 'Italian',\n    'SP1': 'Spanish',\n    'F1': 'French',\n    'N1': 'Dutch',\n}\nURLS_MAPPING = {\n    f'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv': (\n        league,\n        '-'.join([season[0:2], season[2:]]),\n    )\n    for season in SEASONS\n    for league_id, league in LEAGUES_MAPPING.items()\n}\nFIXTURES_URL = 'https://www.football-data.co.uk/fixtures.csv'\n\nOur workflow will include the following tasks:\n\nCheck if a local SQLite database exists. If not, then create it.\nCheck if the database is updated with the latest historical data. If the historical data do not exist, download all the data and store them to the database while if the historical data are not updated, download only the latest data and update the database.\nDownload the latest fixtures data and store them to the database.\n\n\nTasks\nThe above tasks represent discrete units of work, and they will receive the task decorator. We will also use an asynchronous httpx client to concurrently download the data since we have multiple files.\nThe function create_db implements the first task:\n\nimport sqlite3\nfrom prefect import task\nfrom prefect.logging import get_run_logger\nfrom pathlib import Path\nfrom tempfile import mkdtemp\n\nTEMP_DIR = Path(mkdtemp())\n\n\n@task(name='Create database', description='Create the database to store the data')\ndef create_db():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    try:\n        con = sqlite3.connect(f'file:{db_path}?mode=rw', uri=True)\n        logger.info('Database exists.')\n    except sqlite3.OperationalError:\n        con = sqlite3.connect(db_path)\n        logger.info('Database created.')\n    finally:\n        con.close()\n\nThe function update_historical_data implements the second task:\n\nimport httpx\nimport asyncio\nimport pandas as pd\nfrom io import StringIO\n\n\nasync def request_csv_data(client: httpx.Client, url: str, **kwargs):\n    return await client.get(url=url)\n\n\nasync def download_csvs_data(urls_mapping: dict[str, tuple[str, str]]):\n    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=30)) as client:\n        requests = [\n            request_csv_data(client, url, league=league, season=season)\n            for url, (league, season) in urls_mapping.items()\n        ]\n        responses = await asyncio.gather(*requests)\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    return csvs_data\n\n\n@task(\n    name='Update historical data',\n    description='Fetch latest data to update historical data',\n)\nasync def update_historical_data(urls_mapping):\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    with sqlite3.connect(db_path) as con:\n        try:\n            data = pd.read_sql('SELECT * FROM historical', con)\n            logger.info(f'Table with historical data exists. Shape: {data.shape}')\n        except pd.errors.DatabaseError:\n            logger.info('Table with historical data does not exist.')\n            csvs_data = await download_csvs_data(urls_mapping)\n            data = pd.concat(\n                [\n                    pd.read_csv(csv_data, encoding='windows-1254')\n                    for csv_data in csvs_data\n                ],\n                ignore_index=True,\n            )\n            data.to_sql('historical', con=con, index=False)\n            logger.info(f'Table with historical data was created. Shape: {data.shape}')\n            return None\n    urls_mapping = {\n        url: (league, season)\n        for url, (league, season) in urls_mapping.items()\n        if season == '23-24'\n    }\n    latest_csvs_data = await download_csvs_data(urls_mapping)\n    latest_data = pd.concat(\n        [\n            pd.read_csv(csv_data, encoding='windows-1254')\n            for csv_data in latest_csvs_data\n        ],\n        ignore_index=True,\n    )\n    data = pd.concat([data, latest_data], ignore_index=True).drop_duplicates(\n        subset=['Div', 'Date', 'HomeTeam', 'AwayTeam', 'Time'], ignore_index=True\n    )\n    data.to_sql('historical', con=con, index=False, if_exists='replace')\n    logger.info(f'Table with historical data was updated. Shape: {data.shape}')\n\nThe function update_fixtures_data implements the third task:\n\n@task(name='Update fixtures data', description='Fetch latest fixtures data')\nasync def update_fixtures_data():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    data = pd.read_csv(FIXTURES_URL)\n    with sqlite3.connect(db_path) as con:\n        data.to_sql('fixtures', con=con, index=False, if_exists='replace')\n        logger.info(f'Fixtures data were updated. Shape: {data.shape}')\n\n\n\nFlow\nThe full data workflow will receive the flow decorator.\n\nfrom prefect import flow\nfrom prefect.task_runners import ConcurrentTaskRunner\n\n\n@flow(\n    name='Download asynchronously the data and update the database',\n    validate_parameters=True,\n    task_runner=ConcurrentTaskRunner(),\n    log_prints=True,\n)\nasync def update_db(urls_mapping: dict[str, tuple[str, str]]):\n    create_db()\n    await update_historical_data(urls_mapping)\n    await update_fixtures_data()\n\n\n\n\nResults\nWe can run the above flow:\n\nawait update_db(URLS_MAPPING)\n\n18:19:50.135 | INFO    | prefect.engine - Created flow run 'transparent-prawn' for flow 'Download asynchronously the data and update the database'\n\n\n\n18:19:50.169 | INFO    | Flow run 'transparent-prawn' - Created task run 'Create database-0' for task 'Create database'\n\n\n\n18:19:50.170 | INFO    | Flow run 'transparent-prawn' - Executing 'Create database-0' immediately...\n\n\n\n18:19:50.191 | INFO    | Task run 'Create database-0' - Database created.\n\n\n\n18:19:50.202 | INFO    | Task run 'Create database-0' - Finished in state Completed()\n\n\n\n18:19:50.213 | INFO    | Flow run 'transparent-prawn' - Created task run 'Update historical data-0' for task 'Update historical data'\n\n\n\n18:19:50.214 | INFO    | Flow run 'transparent-prawn' - Executing 'Update historical data-0' immediately...\n\n\n\n18:19:50.232 | INFO    | Task run 'Update historical data-0' - Table with historical data does not exist.\n\n\n\n18:19:52.155 | INFO    | Task run 'Update historical data-0' - Table with historical data was created. Shape: (13862, 124)\n\n\n\n18:19:52.169 | INFO    | Task run 'Update historical data-0' - Finished in state Completed()\n\n\n\n18:19:52.181 | INFO    | Flow run 'transparent-prawn' - Created task run 'Update fixtures data-0' for task 'Update fixtures data'\n\n\n\n18:19:52.181 | INFO    | Flow run 'transparent-prawn' - Executing 'Update fixtures data-0' immediately...\n\n\n\n18:19:52.522 | INFO    | Task run 'Update fixtures data-0' - Fixtures data were updated. Shape: (148, 102)\n\n\n\n18:19:52.538 | INFO    | Task run 'Update fixtures data-0' - Finished in state Completed()\n\n\n\n18:19:52.554 | INFO    | Flow run 'transparent-prawn' - Finished in state Completed('All states completed.')\n\n\n\n[Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`'))]\n\n\nLet’s read the data from the database:\n\nfrom shutil import rmtree\n\ndb_path = TEMP_DIR / 'soccer_data.db'\nwith sqlite3.connect(db_path) as con:\n    historical_data = pd.read_sql('SELECT * FROM historical', con)\n    fixtures_data = pd.read_sql('SELECT * FROM fixtures', con)\nrmtree(TEMP_DIR)\n\nThe historical data:\n\nhistorical_data\n\n\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\n...\nAvgC&lt;2.5\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\n0\nE0\n10/08/2018\nMan United\nLeicester\n2\n1\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE0\n11/08/2018\nBournemouth\nCardiff\n2\n0\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nE0\n11/08/2018\nFulham\nCrystal Palace\n0\n2\nA\n0\n1\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nE0\n11/08/2018\nHuddersfield\nChelsea\n0\n3\nA\n0\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nE0\n11/08/2018\nNewcastle\nTottenham\n1\n2\nA\n1\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13857\nN1\n19/05/2024\nPSV Eindhoven\nWaalwijk\n3\n1\nH\n1\n1\nD\n...\n5.69\n-2.50\n1.73\n2.08\n1.77\n2.09\n1.98\n2.17\n1.85\n1.99\n\n\n13858\nN1\n19/05/2024\nSparta Rotterdam\nHeerenveen\n2\n1\nH\n0\n0\nD\n...\n2.87\n-0.75\n1.85\n2.05\n1.86\n2.03\n1.90\n2.12\n1.83\n2.00\n\n\n13859\nN1\n19/05/2024\nVitesse\nAjax\n2\n2\nD\n1\n1\nD\n...\n3.43\n1.00\n1.84\n2.06\n1.84\n2.06\n1.88\n2.11\n1.82\n2.02\n\n\n13860\nN1\n19/05/2024\nVolendam\nGo Ahead Eagles\n1\n2\nA\n1\n1\nD\n...\n3.52\n1.25\n1.78\n2.03\n1.83\n2.07\n1.85\n2.12\n1.81\n2.02\n\n\n13861\nN1\n19/05/2024\nZwolle\nTwente\n1\n2\nA\n0\n0\nD\n...\n3.30\n1.50\n1.97\n1.93\n1.97\n1.92\n2.14\n1.93\n2.01\n1.83\n\n\n\n\n13862 rows × 124 columns\n\n\n\nThe fixtures data:\n\nfixtures_data\n\n\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nReferee\nB365H\nB365D\nB365A\nBWH\n...\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\nBFECAHH\nBFECAHA\n\n\n\n\n0\nB1\n29/11/2024\n19:45\nKortrijk\nMechelen\nNone\n3.40\n3.60\n2.05\n3.40\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n1\nB1\n30/11/2024\n17:15\nClub Brugge\nDender\nNone\n1.25\n7.00\n8.00\n1.25\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n2\nB1\n30/11/2024\n19:45\nCharleroi\nStandard\nNone\n1.95\n3.40\n3.90\n1.98\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n3\nB1\n01/12/2024\n12:30\nSt Truiden\nGenk\nNone\n4.50\n4.00\n1.73\n4.33\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n4\nB1\n01/12/2024\n15:00\nBeerschot VA\nCercle Brugge\nNone\n3.90\n3.50\n1.90\n3.60\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n143\nT1\n30/11/2024\n16:00\nBuyuksehyr\nGoztep\nNone\n2.38\n3.25\n3.00\n2.35\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n144\nT1\n01/12/2024\n13:00\nRizespor\nKayserispor\nNone\n1.83\n3.60\n4.20\n1.82\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n145\nT1\n01/12/2024\n16:00\nGalatasaray\nEyupspor\nNone\n1.25\n7.00\n8.50\n1.26\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n146\nT1\n02/12/2024\n17:00\nFenerbahce\nGaziantep\nNone\n1.25\n6.50\n9.00\n1.24\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n147\nT1\n02/12/2024\n17:00\nHatayspor\nBesiktas\nNone\n4.10\n3.60\n1.83\n3.80\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\n\n148 rows × 102 columns\n\n\n\n\n\nFinal thoughts\nYou can spin up a local Prefect server UI with the prefect server start command in the shell and explore the characteristics of the above Prefect flow we ran. The data are stored in the Prefect database which by default is a local SQLite database.\nPrefect also supports deployments i.e. packaging workflow code, settings, and infrastructure configuration so that the data workflow can be managed via the Prefect API and run remotely by a Prefect agent.\nYou can read more at the official Prefect documentation."
  },
  {
    "objectID": "posts/mechanics_affine_spaces/index.html",
    "href": "posts/mechanics_affine_spaces/index.html",
    "title": "Mechanics",
    "section": "",
    "text": "Introduction\nIt’s quite common to think of physical space as \\(\\mathbb{R}^3\\), but this perspective can be misleading, both from a mathematical and a physical standpoint. The main issue is that the structure of \\(\\mathbb{R}^3\\) isn’t naturally invariant under displacements or other critical transformations, known as isometries, which are fundamental to Euclidean geometry. Instead, the three-dimensional space we encounter in Classical Physics (and even the four-dimensional spacetime in Special Relativity) is more accurately captured by the concept of affine spaces.\n\n\nAffine spaces\nTo understand this better, let’s dive into what a real affine space is. A real affine space of finite dimension \\(n\\), denoted by \\(\\mathbb{A}^n\\), is a collection of elements known as points, but with some additional structure:\n\nThere’s an associated \\(n\\)-dimensional vector space \\(V\\), which we call the space of displacements or the space of free vectors.\nA mapping \\(\\mathbb{A}^n \\times \\mathbb{A}^n \\ni (P, Q) \\mapsto P - Q \\in V\\) that respects certain conditions:\n\nFor every point \\(Q\\in \\mathbb{A}^n\\) and every vector \\(\\mathbf{v} \\in V\\), there’s a unique point \\(P \\in \\mathbb{A}^n\\) such that \\(P - Q =\n  \\mathbf{v}\\).\nFor any three points \\(P, Q, R \\in \\mathbb{A}^n\\), the equation \\(P - Q + Q - R = P - R\\) always holds.\n\n\n\nAdditional definitions\nLet’s define a few useful concepts:\n\nGiven a point \\(Q \\in \\mathbb{A}^n\\) and a vector \\(\\mathbf{v} \\in V\\), the unique point \\(P \\in \\mathbb{A}^n\\) that satisfies \\(P - Q\n= \\mathbf{v}\\) is denoted as \\(Q + \\mathbf{v}\\).\nA line in \\(\\mathbb{A}^n\\) originating from a point \\(P\\) with direction given by a vector \\(\\mathbf{v} \\in V\\) is described by the map \\(\\mathbb{R} \\ni t \\mapsto P + t\\mathbf{v} \\in \\mathbb{A}^n\\).\nA line segment is simply a restriction of the above map to a specific interval.\n\n\n\nProperties\nThe following interesting properties hold for any points \\(P, Q \\in \\mathbb{A}^n\\) and vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\):\n\n\\(P - P = \\mathbf{0}\\).\n\\((Q + \\mathbf{u}) + \\mathbf{v} = Q + (\\mathbf{u} + \\mathbf{v})\\).\n\\(P - Q = -(Q - P)\\).\n\\(P - Q = (P + \\mathbf{u}) - (Q + \\mathbf{u})\\).\n\n\nYou can try proving these properties to deepen your understanding.\n\n\n\n\nCoordinate systems on affine spaces\nNow that we’ve established what an affine space is, let’s discuss how we can introduce coordinate systems on such spaces. A local coordinate system on an affine space \\(\\mathbb{A}^n\\) is a map \\(\\psi: U \\subset \\mathbb{A}^n \\rightarrow \\mathbb{R}^n\\) that satisfies:\n\nThe map \\(\\psi\\) is injective.\nThe image \\(\\psi(U)\\) is an open subset of \\(\\mathbb{R}^n\\).\n\nIf \\(U = \\mathbb{A}^n\\), we call the coordinate system global.\n\nCartesian coordinate systems\nEvery affine space has a family of global coordinate systems known as Cartesian coordinate systems. To define one, we need to:\n\nChoose a point \\(O\\) (the origin) and a basis \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\) for the vector space \\(V\\).\nDefine a map \\(f: \\mathbb{A}^n \\rightarrow \\mathbb{R}^n\\) by \\(f(P) = \\left((P - O)^1, \\ldots, (P - O)^n\\right)\\).\n\n\nTry proving that this map \\(f\\) is bijective, which will confirm that \\(f\\) is indeed a global coordinate system.\n\nNon-Cartesian local coordinate systems, on the other hand, are referred to as curvilinear coordinate systems.\n\n\nProperties of Cartesian coordinate systems\nSuppose we have two Cartesian coordinate systems: \\(\\left(\\mathbb{A}^n, f\\right)\\) with coordinates \\(x^1, \\cdots, x^n\\), and another \\(\\left(\\mathbb{A}^n, g\\right)\\) with coordinates \\(x^{\\prime 1}, \\cdots, x^{\\prime n}\\), origin \\(O^{\\prime}\\), and basis vectors \\(\\mathbf{e}_1^{\\prime}, \\ldots, \\mathbf{e}_n^{\\prime}\\). If the relation between the basis vectors is:\n\\[\n\\mathbf{e}_i = \\sum_j B^j{}_i \\mathbf{e}_j^{\\prime}\n\\]\nand \\(O - O^{\\prime} = \\sum_i b^i \\mathbf{e}_i\\), then the following properties hold:\n\nThe transformation map \\(g \\circ f^{-1}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is given by \\(x^{\\prime j} = \\sum_{i=1}^n\nB^j{}_i\\left(x^i + b^i\\right)\\).\nConversely, the map \\(f \\circ g^{-1}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is expressed as \\(x^i = -b^i +\n\\sum_{j=1}^n\\left(B^{-1}\\right)^i{}_j x^{\\prime j}\\).\n\n\nAgain, try proving these transformations to see how coordinate systems relate to each other.\n\n\n\n\nAffine transformations between affine spaces\nLet’s now explore how we can map one affine space to another. A map \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^m\\) between two affine spaces, with associated vector spaces \\(V_1\\) and \\(V_2\\), is called an affine transformation if:\n\nThe map \\(\\psi\\) is invariant under displacements, meaning that for any points \\(P, Q \\in \\mathbb{A}_1^n\\) and any vector \\(\\mathbf{u} \\in V_1\\), the equality \\(\\psi(P + \\mathbf{u}) - \\psi(Q + \\mathbf{u}) = \\psi(P) - \\psi(Q)\\) holds.\nThe derivative map \\(d\\psi: V_1 \\rightarrow V_2\\), defined by \\(d\\psi(P - Q) = \\psi(P) - \\psi(Q)\\), is a linear transformation between the vector spaces \\(V_1\\) and \\(V_2\\).\n\n\nProving that \\(d\\psi\\) is well-defined and linear will help you understand the relationship between these spaces.\n\n\nIsomorphism of affine spaces\nAn affine transformation \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^m\\) is called an isomorphism if it is bijective. This concept is particularly important because it tells us when two affine spaces are structurally the same.\n\n\nProperties of affine transformations\nAffine transformations have several key properties:\n\nThe inverse of an isomorphism is itself an affine transformation, making it an isomorphism as well.\nIf \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^n\\) is an isomorphism, then the derivative map \\(d\\psi: V_1 \\rightarrow V_2\\) is a vector space isomorphism.\nAffine transformations map straight lines to straight lines. Specifically, if \\(P(t) := P + t \\mathbf{u}\\) describes a line in \\(\\mathbb{A}_1^n\\), then \\(\\psi(P(t))\\) defines a line in \\(\\mathbb{A}_2^m\\).\nGiven Cartesian coordinate systems on \\(\\mathbb{A}_1^n\\) and \\(\\mathbb{A}_2^m\\), the affine transformation \\(\\psi\\) has a particularly simple form in these coordinates: \\(x_2^i = c^i + \\sum_{j=1}^n L^i{}_j x_1^j\\), where the coefficients \\(L^i{}_j\\) and \\(c^i\\) depend on \\(\\psi\\) and the chosen coordinate systems.\n\n\nProving these properties gives you a clearer view of how affine spaces relate to each other under transformations.\n\n\n\n\nGroup of displacements of affine spaces\nIn the context of affine spaces, the concept of displacements is crucial. For a given vector \\(\\mathbf{v} \\in V\\), we can define a mapping \\(T_{\\mathbf{v}}: \\mathbb{A}^n \\rightarrow \\mathbb{A}^n\\) that shifts every point \\(P\\) in the affine space by \\(\\mathbf{v}\\). Formally, this map is defined as \\(T_{\\mathbf{v}}(P) = P + \\mathbf{v}\\). The collection of all such transformations, denoted \\(\\left\\{T_{\\mathbf{v}}\\right\\}_{\\mathbf{v} \\in V}\\), forms what is known as the group of displacements of \\(\\mathbb{A}^n\\). This group operates under the composition of mappings.\n\nTo explore this concept further, you can try proving that the set \\(\\left\\{T_{\\mathbf{v}}\\right\\}_{\\mathbf{v} \\in V}\\) indeed forms a group under composition.\n\n\nProperties\nThe group of displacements comes with several interesting properties:\n\nThe group of displacements of \\(\\mathbb{A}^n\\) is abelian, meaning that the order in which you apply displacements doesn’t matter; that is, \\(T_{\\mathbf{v}} \\circ T_{\\mathbf{u}} = T_{\\mathbf{u}} \\circ T_{\\mathbf{v}}\\) for any vectors \\(\\mathbf{u}, \\mathbf{v} \\in\nV\\).\nThe map \\(V \\ni \\mathbf{v} \\mapsto T_{\\mathbf{v}}\\) is injective, which means that each vector \\(\\mathbf{v}\\) corresponds to a unique transformation. Moreover, this mapping is a group isomorphism when \\(V\\) is viewed as an abelian group under vector addition.\nOnly the zero vector \\(\\mathbf{v} = \\mathbf{0}\\) satisfies \\(T_{\\mathbf{v}}(P) = P\\) for some \\(P \\in \\mathbb{A}^n\\). In fact, this holds for every point \\(P\\), so the action of the group of displacements is free.\nFor any two points \\(P, Q \\in \\mathbb{A}^n\\), there exists a displacement \\(T_{\\mathbf{v}}\\) such that \\(T_{\\mathbf{v}}(P) = Q\\). This property highlights the fact that the group of displacements acts transitively on the affine space.\n\n\nThese properties are foundational to understanding the structure of affine spaces. You might find it helpful to prove these properties yourself.\n\n\n\nGroup action\nTo generalize the idea of how a group can interact with a set, we use the concept of a group action. Let’s consider a set \\(S\\) and a group \\(G\\) with a neutral element \\(e\\) and a group operation \\(\\circ\\). A group action is a map \\(A: G \\times S \\ni (g, s) \\mapsto\nA_g(s) \\in S\\), where \\(A_g \\in \\mathcal{G}_S\\) (the group of bijections on \\(S\\) under composition). For this map to be a valid group action, it must satisfy two key properties:\n\nThe action of the neutral element is the identity: \\(A_e = \\text{id}\\).\nThe action respects the group operation: \\(A_g \\circ A_{g^{\\prime}} = A_{g \\circ g^{\\prime}}\\) for all \\(g, g^{\\prime} \\in G\\).\n\nThere are a few special types of group actions that are worth noting:\n\nThe action is called free if \\(A_g(s) = s\\) for some \\(s \\in S\\) implies \\(g = e\\).\nThe action is transitive if for any two elements \\(s, s^{\\prime} \\in S\\), there exists a group element \\(g \\in G\\) such that \\(A_g(s)\n= s^{\\prime}\\).\nThe action is faithful if the map \\(G \\ni g \\mapsto A_g \\in \\mathcal{G}_S\\) is injective, meaning that different elements of \\(G\\) induce different bijections on \\(S\\).\n\nWhenever we have a group action, it naturally defines a group homomorphism from \\(G\\) to \\(\\mathcal{G}_S\\). The image of this homomorphism, denoted \\(G_S = \\left\\{A_g\\right\\}_{g \\in G}\\), is a subgroup of \\(\\mathcal{G}_S\\). If the action is faithful, then this homomorphism is actually an isomorphism between \\(G\\) and \\(G_S\\).\n\nTo deepen your understanding, try proving the property that an action defines a group homomorphism and explore the implications of free, transitive, and faithful actions."
  },
  {
    "objectID": "posts/imbalanced-learn-extra/index.html",
    "href": "posts/imbalanced-learn-extra/index.html",
    "title": "imbalanced-learn-extra",
    "section": "",
    "text": "Introduction\nThe library imbalanced-learn-extra is a Python package that extends imbalanced-learn. It implements algorithms that are not included in imbalanced-learn due to their novelty or lower citation number. The current version includes the following:\n\nA general interface for clustering-based oversampling algorithms.\nThe Geometric SMOTE algorithm.\n\n\n\nClustering-based oversampling\nClustering-based oversampling algorithms deal with the within-classes imbalance issue, since SMOTE and its variants addresses only the between-classes imbalance. To present the API, let’s first load some data:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\n\nThe data are imbalanced:\n\n# Imports\nfrom collections import Counter\n\n# Classes distribution\ncounter = Counter(y)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 212.\n\n\nI will use KMeans and SMOTE to create a clustering-based oversampler, but any other combination would work:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom imblearn_extra.clover.over_sampling import ClusterOverSampler\n\n# Create KMeans-SMOTE instance\nrnd_seed = 14\nsmote = SMOTE(random_state=rnd_seed + 1)\nkmeans = KMeans(n_clusters=10, random_state=rnd_seed + 3, n_init=50)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\nNow we can use the fit_resample method to get the resampled data:\n\n_, y_res = kmeans_smote.fit_resample(X, y)\ncounter = Counter(y_res)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 357.\n\n\nThe clustering-based oversamplers can be used in machine learning pipelines:\n\n# Imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\n\n# Cross validation score\nclassifier = RandomForestClassifier(random_state=rnd_seed)\nclassifier = make_pipeline(kmeans_smote, classifier)\nscore = cross_val_score(estimator=classifier, X=X, y=y, scoring=\"f1\").mean()\nprint(f\"The cross-validation F-score is {score}.\")\n\nThe cross-validation F-score is 0.9664262119887302.\n\n\n\n\nGeometric SMOTE\nGeometric SMOTE is not just another member of the SMOTE’s family since it expands the data generation area and does not just use linear interpolation of existing samples to generate for new samples. To test its performance, let’s first simulate various imbalanced datasets:\n\n# Imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import ParameterGrid\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced datasets\ndatasets = []\ndatasets_params = ParameterGrid(\n    {\"weights\": [[0.8, 0.2], [0.9, 0.1]], \"class_sep\": [0.01, 0.1]}\n)\nfor data_params in datasets_params:\n    datasets.append(\n        make_classification(\n            random_state=rnd_seed,\n            n_informative=10,\n            n_samples=2000,\n            n_classes=2,\n            **data_params,\n        )\n    )\n\nWe will also create pipelines of various oversamplers, classifiers and their hyperparameters:\n\n# Imports\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn_extra.gsmote import GeometricSMOTE\n\n# Pipelines\nclassifiers = [LogisticRegression(), KNeighborsClassifier()]\noversamplers = [None, RandomOverSampler(), SMOTE(), GeometricSMOTE()]\npipelines = []\noversamplers_param_grids = {\n    \"SMOTE\": {\n        \"smote__k_neighbors\": [\n            NearestNeighbors(n_neighbors=2),\n            NearestNeighbors(n_neighbors=3),\n        ]\n    },\n    \"GeometricSMOTE\": {\n        \"geometricsmote__k_neighbors\": [2, 3],\n        \"geometricsmote__deformation_factor\": [0.0, 0.25, 0.5, 0.75, 1.0],\n    },\n}\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=rnd_seed + 5)\nfor classifier in classifiers:\n    for oversampler in oversamplers:\n        oversampler_name = (\n            oversampler.__class__.__name__ if oversampler is not None else None\n        )\n        param_grid = oversamplers_param_grids.get(oversampler_name, {})\n        estimator = (\n            make_pipeline(oversampler, classifier)\n            if oversampler is not None\n            else make_pipeline(classifier)\n        )\n        pipelines.append(GridSearchCV(estimator, param_grid, cv=cv, scoring=\"f1\"))\n\nFinally, we will calculate the nested cross-validation scores of the above pipelines using F-score as evaluation metric:\n\nn_runs = 3\ncv_scores = []\nfor run_id in range(n_runs):\n    for dataset_id, (X, y) in enumerate(datasets):\n        for pipeline_id, pipeline in enumerate(pipelines):\n            for param in pipeline.get_params():\n                if param.endswith(\"__n_jobs\") and param != \"estimator__smote__n_jobs\":\n                    pipeline.set_params(**{param: -1})\n                if param.endswith(\"__random_state\"):\n                    pipeline.set_params(\n                        **{\n                            param: rnd_seed\n                            * (run_id + 1)\n                            * (dataset_id + 1)\n                            * (pipeline_id + 1)\n                        }\n                    )\n            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=10 * run_id)\n            scores = cross_val_score(\n                estimator=pipeline,\n                X=X,\n                y=y,\n                scoring=\"f1\",\n                cv=cv,\n            )\n            print(f\"Run: {run_id} | Dataset: {dataset_id} | Pipeline: {pipeline_id}\")\n            pipeline_name = '-'.join(\n                [\n                    estimator.__class__.__name__\n                    for _, estimator in pipeline.get_params()['estimator'].get_params()[\n                        'steps'\n                    ]\n                ]\n            )\n            cv_scores.append((run_id, dataset_id, pipeline_name, scores.mean()))\n\nLet’s see the final results of the experiment:\n\ncv_scores = (\n    pd.DataFrame(cv_scores, columns=[\"Run\", \"Dataset\", \"Pipeline\", \"Score\"])\n    .groupby([\"Dataset\", \"Pipeline\"])[\"Score\"]\n    .mean()\n    .reset_index()\n)\ncv_scores\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n1\n0\nGeometricSMOTE-LogisticRegression\n0.281625\n\n\n2\n0\nKNeighborsClassifier\n0.515543\n\n\n3\n0\nLogisticRegression\n0.001622\n\n\n4\n0\nRandomOverSampler-KNeighborsClassifier\n0.586250\n\n\n5\n0\nRandomOverSampler-LogisticRegression\n0.282728\n\n\n6\n0\nSMOTE-KNeighborsClassifier\n0.579605\n\n\n7\n0\nSMOTE-LogisticRegression\n0.281004\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n9\n1\nGeometricSMOTE-LogisticRegression\n0.186105\n\n\n10\n1\nKNeighborsClassifier\n0.316577\n\n\n11\n1\nLogisticRegression\n0.003130\n\n\n12\n1\nRandomOverSampler-KNeighborsClassifier\n0.460189\n\n\n13\n1\nRandomOverSampler-LogisticRegression\n0.188722\n\n\n14\n1\nSMOTE-KNeighborsClassifier\n0.428110\n\n\n15\n1\nSMOTE-LogisticRegression\n0.189665\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n17\n2\nGeometricSMOTE-LogisticRegression\n0.296189\n\n\n18\n2\nKNeighborsClassifier\n0.522802\n\n\n19\n2\nLogisticRegression\n0.006476\n\n\n20\n2\nRandomOverSampler-KNeighborsClassifier\n0.592432\n\n\n21\n2\nRandomOverSampler-LogisticRegression\n0.290737\n\n\n22\n2\nSMOTE-KNeighborsClassifier\n0.580532\n\n\n23\n2\nSMOTE-LogisticRegression\n0.294199\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n25\n3\nGeometricSMOTE-LogisticRegression\n0.191214\n\n\n26\n3\nKNeighborsClassifier\n0.323485\n\n\n27\n3\nLogisticRegression\n0.006260\n\n\n28\n3\nRandomOverSampler-KNeighborsClassifier\n0.454507\n\n\n29\n3\nRandomOverSampler-LogisticRegression\n0.195133\n\n\n30\n3\nSMOTE-KNeighborsClassifier\n0.428896\n\n\n31\n3\nSMOTE-LogisticRegression\n0.192810\n\n\n\n\n\n\n\nThe next table shows the pipeline with the highest F-score per dataset:\n\ncv_scores_best = cv_scores.loc[cv_scores.groupby(\"Dataset\")[\"Score\"].idxmax()]\ncv_scores_best\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n\n\n\n\n\nTherefore, Geometric SMOTE outperforms the other methods in all datasets when the F-score is used as an evaluation metric."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Mechanics\n\n\n\n\n\n\nPhysics\n\n\nMechanics\n\n\n\nIntroduction to affine spaces.\n\n\n\n\n\nAug 22, 2024\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\nPrefect\n\n\n\n\n\n\nSoftware Engineering\n\n\nOpen Source\n\n\nData Engineering\n\n\n\nBuild, deploy and observe data workflows.\n\n\n\n\n\nJun 22, 2023\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\nimbalanced-learn-extra\n\n\n\n\n\n\nMachine Learning\n\n\nOpen Source\n\n\nImbalanced Data\n\n\n\nImplementation of novel oversampling algorithms.\n\n\n\n\n\nMay 2, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\nNo matching items"
  }
]