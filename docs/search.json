[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Georgios Douzas. I am a machine learning researcher at Nova IMS, University of Lisbon, and a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence, with multiple publications in machine learning and high-energy physics journals. My professional experience includes working as a software and machine learning engineer for various companies. Additionally, I often maintain or contribute to open-source projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nPh.D. in Theoretical Particle Physics\nNational Technical University of Athens\n\\(09/03\\) - \\(09/08\\)\nM.Sc. in Physics\nNational Technical University of Athens\n\\(09/01\\) - \\(09/03\\)\nB.Sc. in Physics\nNational and Kapodistrian University of Athens\n\\(09/97\\) - \\(09/01\\)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nMachine Learning Engineer\nTrasys, Greece\n\\(8/20\\) - Present\nDesigned and implemented Parallel Distribution, a software tool for the European Medicines Agency that applies OCR on PDF documents and generates a comparison report. The primary language of implementation was Python, while various text mining and machine learning libraries were used. The frontend of the tool used HTML, CSS, and JavaScript to provide an interactive dashboard of the results.\nMachine Learning Researcher\nUniversity of Lisbon Nova IMS, Portugal\n\\(09/13\\) - \\(09/14\\) & \\(09/18\\) - Present\nDesigned, implemented, and tested various new approaches for the class imbalance problem. Research focused on clustering-based over-sampling methods that deal with the within-the-class imbalance problem. Additionally, Geometric SMOTE, an extension of the SMOTE algorithm, was proposed and implemented. The final publication presented results showing a significant improvement over SMOTE and its variations. Deep learning models, particularly Conditional Generative Adversarial Networks (CGANs), were also used as over-sampling methods with great success. The frameworks of the implementation were TensorFlow, Keras, and PyTorch. Work is published in high-impact machine learning journals. Implementation of the above algorithms was developed and made available as open-source software. Work in progress includes comparative experiments between variations of CGANs as over-samplers and the investigation of novel algorithms in the context of reinforcement learning.\nMachine Learning Engineer\nTripsta, Greece\n\\(10/17\\) - \\(08/18\\)\nDesigned and implemented the main parts of the company’s automated pricing system. These parts included machine learning estimators for the add-ons and the competitor’s prices and the application of metaheuristic algorithms for the budget multi-objective optimization problem. The training data of the various estimators were at the order of TB while the prediction time of the automated pricing system was required to be less than \\(100\\) msec for the incoming \\(50\\)K requests/sec. The implementation languages were Python, Java, and Scala, while Spark, Dask, Scikit-Learn and jMetal were used as distributed data processing, machine learning, and optimization frameworks/libraries.\nData Scientist\nQuantum Retail, Remote \\(12/16\\) - \\(09/17\\)\nWorked on demand forecasting and clustering for retail companies. Proposed and applied machine learning methods to improve the company’s main forecasting solution based on exponential smoothing of the time series data and adjustments guided by a seasonality curve. Boosting trees were selected as the final machine-learning model. Applied feature extraction that integrated the business logic and extensive model hyperparameter tuning, the forecasting precision was improved by \\(30\\)% compared to the original model.\nMachine Learning Engineer\nCERN, Remote\n\\(05/16\\) - \\(09/16\\)\nDeveloped the parallelization of various features for TMVA, the Toolkit for Multivariate Data Analysis with ROOT, as a part of a project funded by Google. ROOT is the main framework developed by CERN to deal with the big data processing, statistical analysis, visualization, and storage of massive amounts of data produced from particle physics experiments. The legacy version was implemented in C++. The parallelized features included the application of brute-force and metaheuristic algorithms to the hyperparameter grid search of machine learning algorithms. The implementation was based on Python and Spark.\nScientific Software Engineer IRI, Greece\n\\(01/14\\) - \\(05/16\\)\nMember of the IRI’s “Solutions and Innovation Team” (R&D) working on the company’s transition towards Open Source and Elastic Computing. Participated in an agile team migrating IRI’s leading US “Price & Promo Analytics” Solution, generating more than \\(\\$25\\)M Annual Revenues, to Hadoop distributed storage and Spark cluster computing. Python was the core language of the implementation, but integration with R and Julia was performed to leverage unique functionality. The legacy version was implemented in SAS. The project’s main objectives were the design of the parallelization schema, the enhancement of data manipulation with the use of distributed processing, and the migration of the statistical modeling algorithms (regression mixed models). The final system processed \\(5\\) years of data for more than \\(300\\) categories containing \\(1\\) million products."
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html",
    "href": "projects/software-engineering/prefect/notebooks/index.html",
    "title": "Prefect",
    "section": "",
    "text": "Prefect is a workflow orchestration tool. It makes accessible the creation, scheduling, and monitoring of complex data pipelines. The workflows are defined as Python code, while Prefect provides error handling, retry mechanisms, and a user-friendly dashboard for monitoring.\nAs an example, let’s assume that we would like to create a data workflow that downloads, stores and updates historical and fixtures soccer data from Football-Data.co.uk. The URL of each of those main leagues has the following form:\n'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv'\nwhere season is the season of the league and league_id is the league ID. Let’s select a few of those seasons and leagues:\n\nSEASONS = ['1819', '1920', '2021', '2122', '2223', '2324']\nLEAGUES_MAPPING = {\n    'E0': 'English',\n    'SC0': 'Scotish',\n    'D1': 'German',\n    'I1': 'Italian',\n    'SP1': 'Spanish',\n    'F1': 'French',\n    'N1': 'Dutch',\n}\nURLS_MAPPING = {\n    f'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv': (\n        league,\n        '-'.join([season[0:2], season[2:]]),\n    )\n    for season in SEASONS\n    for league_id, league in LEAGUES_MAPPING.items()\n}\nFIXTURES_URL = 'https://www.football-data.co.uk/fixtures.csv'\n\nOur workflow will include the following tasks:\n\nCheck if a local SQLite database exists. If not, then create it.\nCheck if the database is updated with the latest historical data. If the historical data do not exist, download all the data and store them to the database while if the historical data are not updated, download only the latest data and update the database.\nDownload the latest fixtures data and store them to the database.\n\nThe above tasks represent discrete units of work, and they will receive the task decorator. We will also use an asynchronous httpx client to concurrently download the data since we have multiple files.\nThe function create_db implements the first task:\n\nimport sqlite3\nfrom prefect import task\nfrom prefect.logging import get_run_logger\nfrom pathlib import Path\nfrom tempfile import mkdtemp\n\nTEMP_DIR = Path(mkdtemp())\n\n\n@task(name='Create database', description='Create the database to store the data')\ndef create_db():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    try:\n        con = sqlite3.connect(f'file:{db_path}?mode=rw', uri=True)\n        logger.info('Database exists.')\n    except sqlite3.OperationalError:\n        con = sqlite3.connect(db_path)\n        logger.info('Database created.')\n    finally:\n        con.close()\n\nThe function update_historical_data implements the second task:\n\nimport httpx\nimport asyncio\nimport pandas as pd\nfrom io import StringIO\n\n\nasync def request_csv_data(client: httpx.Client, url: str, **kwargs):\n    return await client.get(url=url)\n\n\nasync def download_csvs_data(urls_mapping: dict[str, tuple[str, str]]):\n    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=30)) as client:\n        requests = [\n            request_csv_data(client, url, league=league, season=season)\n            for url, (league, season) in urls_mapping.items()\n        ]\n        responses = await asyncio.gather(*requests)\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    return csvs_data\n\n\n@task(\n    name='Update historical data',\n    description='Fetch latest data to update historical data',\n)\nasync def update_historical_data(urls_mapping):\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    with sqlite3.connect(db_path) as con:\n        try:\n            data = pd.read_sql('SELECT * FROM historical', con)\n            logger.info(f'Table with historical data exists. Shape: {data.shape}')\n        except pd.errors.DatabaseError:\n            logger.info('Table with historical data does not exist.')\n            csvs_data = await download_csvs_data(urls_mapping)\n            data = pd.concat(\n                [\n                    pd.read_csv(csv_data, encoding='windows-1254')\n                    for csv_data in csvs_data\n                ],\n                ignore_index=True,\n            )\n            data.to_sql('historical', con=con, index=False)\n            logger.info(f'Table with historical data was created. Shape: {data.shape}')\n            return None\n    urls_mapping = {\n        url: (league, season)\n        for url, (league, season) in urls_mapping.items()\n        if season == '23-24'\n    }\n    latest_csvs_data = await download_csvs_data(urls_mapping)\n    latest_data = pd.concat(\n        [\n            pd.read_csv(csv_data, encoding='windows-1254')\n            for csv_data in latest_csvs_data\n        ],\n        ignore_index=True,\n    )\n    data = pd.concat([data, latest_data], ignore_index=True).drop_duplicates(\n        subset=['Div', 'Date', 'HomeTeam', 'AwayTeam', 'Time'], ignore_index=True\n    )\n    data.to_sql('historical', con=con, index=False, if_exists='replace')\n    logger.info(f'Table with historical data was updated. Shape: {data.shape}')\n\nThe function update_fixtures_data implements the third task:\n\n@task(name='Update fixtures data', description='Fetch latest fixtures data')\nasync def update_fixtures_data():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    data = pd.read_csv(FIXTURES_URL)\n    with sqlite3.connect(db_path) as con:\n        data.to_sql('fixtures', con=con, index=False, if_exists='replace')\n        logger.info(f'Fixtures data were updated. Shape: {data.shape}')\n\nThe full data workflow will receive the flow decorator.\n\nfrom prefect import flow\nfrom prefect.task_runners import ConcurrentTaskRunner\n\n\n@flow(\n    name='Download asynchronously the data and update the database',\n    validate_parameters=True,\n    task_runner=ConcurrentTaskRunner(),\n    log_prints=True,\n)\nasync def update_db(urls_mapping: dict[str, tuple[str, str]]):\n    create_db()\n    await update_historical_data(urls_mapping)\n    await update_fixtures_data()\n\nWe can run the above flow:\n\nawait update_db(URLS_MAPPING)\n\n18:01:10.889 | INFO    | prefect.engine - Created flow run 'paper-ringtail' for flow 'Download asynchronously the data and update the database'\n\n\n\n18:01:10.972 | INFO    | Flow run 'paper-ringtail' - Created task run 'Create database-0' for task 'Create database'\n\n\n\n18:01:10.974 | INFO    | Flow run 'paper-ringtail' - Executing 'Create database-0' immediately...\n\n\n\n18:01:11.029 | INFO    | Task run 'Create database-0' - Database created.\n\n\n\n18:01:11.065 | INFO    | Task run 'Create database-0' - Finished in state Completed()\n\n\n\n18:01:11.093 | INFO    | Flow run 'paper-ringtail' - Created task run 'Update historical data-0' for task 'Update historical data'\n\n\n\n18:01:11.095 | INFO    | Flow run 'paper-ringtail' - Executing 'Update historical data-0' immediately...\n\n\n\n18:01:11.146 | INFO    | Task run 'Update historical data-0' - Table with historical data does not exist.\n\n\n\n18:01:14.527 | INFO    | Task run 'Update historical data-0' - Table with historical data was created. Shape: (12533, 124)\n\n\n\n18:01:14.563 | INFO    | Task run 'Update historical data-0' - Finished in state Completed()\n\n\n\n18:01:14.590 | INFO    | Flow run 'paper-ringtail' - Created task run 'Update fixtures data-0' for task 'Update fixtures data'\n\n\n\n18:01:14.592 | INFO    | Flow run 'paper-ringtail' - Executing 'Update fixtures data-0' immediately...\n\n\n\n18:01:15.065 | INFO    | Task run 'Update fixtures data-0' - Fixtures data were updated. Shape: (169, 93)\n\n\n\n18:01:15.100 | INFO    | Task run 'Update fixtures data-0' - Finished in state Completed()\n\n\n\n18:01:15.137 | INFO    | Flow run 'paper-ringtail' - Finished in state Completed('All states completed.')\n\n\n\n[Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`'))]\n\n\nLet’s read the data from the database:\n\nfrom shutil import rmtree\n\ndb_path = TEMP_DIR / 'soccer_data.db'\nwith sqlite3.connect(db_path) as con:\n    historical_data = pd.read_sql('SELECT * FROM historical', con)\n    fixtures_data = pd.read_sql('SELECT * FROM fixtures', con)\nrmtree(TEMP_DIR)\n\nThe historical data:\n\nhistorical_data\n\n\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\n...\nAvgC&lt;2.5\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\n0\nE0\n10/08/2018\nMan United\nLeicester\n2\n1\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE0\n11/08/2018\nBournemouth\nCardiff\n2\n0\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nE0\n11/08/2018\nFulham\nCrystal Palace\n0\n2\nA\n0\n1\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nE0\n11/08/2018\nHuddersfield\nChelsea\n0\n3\nA\n0\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nE0\n11/08/2018\nNewcastle\nTottenham\n1\n2\nA\n1\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12528\nN1\n09/12/2023\nZwolle\nNijmegen\n1\n3\nA\n0\n1\nA\n...\n2.20\n-0.25\n1.85\n2.05\n1.86\n2.05\n1.88\n2.11\n1.84\n1.99\n\n\n12529\nN1\n09/12/2023\nAZ Alkmaar\nAlmere City\n4\n1\nH\n1\n0\nH\n...\n2.42\n-1.75\n1.92\n1.98\n1.93\n1.96\n1.98\n2.00\n1.88\n1.94\n\n\n12530\nN1\n10/12/2023\nGo Ahead Eagles\nUtrecht\n0\n2\nA\n0\n2\nA\n...\n2.11\n-0.25\n2.03\n1.87\n2.04\n1.88\n2.09\n1.96\n1.99\n1.84\n\n\n12531\nN1\n10/12/2023\nFor Sittard\nWaalwijk\n1\n0\nH\n0\n0\nD\n...\n2.07\n-0.25\n1.86\n2.04\n1.85\n2.07\n1.87\n2.15\n1.81\n2.02\n\n\n12532\nN1\n10/12/2023\nVitesse\nHeracles\n2\n0\nH\n1\n0\nH\n...\n2.10\n-0.25\n1.90\n2.00\n1.93\n1.98\n1.93\n2.05\n1.88\n1.94\n\n\n\n\n12533 rows × 124 columns\n\n\n\nThe fixtures data:\n\nfixtures_data\n\n\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\n...\nAvgC&lt;2.5\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\n0\nB1\n08/12/2023\n19:45\nKortrijk\nWesterlo\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n1\nB1\n09/12/2023\n15:00\nCercle Brugge\nAntwerp\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n2\nB1\n09/12/2023\n17:15\nSt Truiden\nOud-Heverlee Leuven\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n3\nB1\n09/12/2023\n19:45\nGent\nRWD Molenbeek\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n4\nB1\n10/12/2023\n12:30\nMechelen\nClub Brugge\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n164\nT1\n10/12/2023\n13:00\nBuyuksehyr\nHatayspor\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n165\nT1\n10/12/2023\n13:00\nKayserispor\nPendikspor\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n166\nT1\n10/12/2023\n16:00\nGaziantep\nTrabzonspor\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n167\nT1\n11/12/2023\n17:00\nAnkaragucu\nRizespor\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n168\nT1\n11/12/2023\n17:00\nKonyaspor\nSivasspor\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\n\n169 rows × 93 columns\n\n\n\nYou can spin up a local Prefect server UI with the prefect server start command in the shell and explore the characteristics of the above Prefect flow we ran. The data are stored in the Prefect database which by default is a local SQLite database.\nPrefect also supports deployments i.e. packaging workflow code, settings, and infrastructure configuration so that the data workflow can be managed via the Prefect API and run remotely by a Prefect agent.\nYou can read more at the official Prefect documentation."
  },
  {
    "objectID": "projects/machine-learning/geometric-smote/notebooks/index.html",
    "href": "projects/machine-learning/geometric-smote/notebooks/index.html",
    "title": "Geometric SMOTE algorithm",
    "section": "",
    "text": "The SMOTE algorithm is the most popular oversampler, with many proposed variants. On the other hand, Geometric SMOTE is not just another member of the SMOTE’s family since it expands the data generation area and does not just use linear interpolation of existing samples to generate for new samples. You can check the following figure for a visual representation of the their difference:\n\nI have developed a Python implementation of the Geometric SMOTE oversampler called geometric-smote, which integrates with the Scikit-Learn and Imbalanced-Learn ecosystems. To run a comparison experiment, let’s first create various imbalanced binary datasets with different characteristics:\n\n# Imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import ParameterGrid\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced datasets\ndatasets = []\ndatasets_params = ParameterGrid(\n    {\"weights\": [[0.8, 0.2], [0.9, 0.1]], \"class_sep\": [0.01, 0.1]}\n)\nfor data_params in datasets_params:\n    datasets.append(\n        make_classification(\n            random_state=rnd_seed,\n            n_informative=10,\n            n_samples=2000,\n            n_classes=2,\n            **data_params,\n        )\n    )\n\nWe will also create pipelines of various oversamplers, classifiers and their hyperparameters:\n\n# Imports\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom gsmote import GeometricSMOTE\n\n# Pipelines\nclassifiers = [LogisticRegression(), KNeighborsClassifier()]\noversamplers = [None, RandomOverSampler(), SMOTE(), GeometricSMOTE()]\npipelines = []\noversamplers_param_grids = {\n    \"SMOTE\": {\n        \"smote__k_neighbors\": [\n            NearestNeighbors(n_neighbors=2),\n            NearestNeighbors(n_neighbors=3),\n        ]\n    },\n    \"GeometricSMOTE\": {\n        \"geometricsmote__k_neighbors\": [2, 3],\n        \"geometricsmote__deformation_factor\": [0.0, 0.25, 0.5, 0.75, 1.0],\n    },\n}\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=rnd_seed + 5)\nfor classifier in classifiers:\n    for oversampler in oversamplers:\n        oversampler_name = (\n            oversampler.__class__.__name__ if oversampler is not None else None\n        )\n        param_grid = oversamplers_param_grids.get(oversampler_name, {})\n        estimator = (\n            make_pipeline(oversampler, classifier)\n            if oversampler is not None\n            else make_pipeline(classifier)\n        )\n        pipelines.append(GridSearchCV(estimator, param_grid, cv=cv, scoring=\"f1\"))\n\nFinally, we will calculate the nested cross-validation scores of the above pipelines using F-score as evaluation metric:\n\nn_runs = 3\ncv_scores = []\nfor run_id in range(n_runs):\n    for dataset_id, (X, y) in enumerate(datasets):\n        for pipeline_id, pipeline in enumerate(pipelines):\n            for param in pipeline.get_params():\n                if param.endswith(\"__n_jobs\") and param != \"estimator__smote__n_jobs\":\n                    pipeline.set_params(**{param: -1})\n                if param.endswith(\"__random_state\"):\n                    pipeline.set_params(\n                        **{\n                            param: rnd_seed\n                            * (run_id + 1)\n                            * (dataset_id + 1)\n                            * (pipeline_id + 1)\n                        }\n                    )\n            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=10 * run_id)\n            scores = cross_val_score(\n                estimator=pipeline,\n                X=X,\n                y=y,\n                scoring=\"f1\",\n                cv=cv,\n            )\n            print(f\"Run: {run_id} | Dataset: {dataset_id} | Pipeline: {pipeline_id}\")\n            pipeline_name = '-'.join([estimator.__class__.__name__ for _, estimator in pipeline.get_params()['estimator'].get_params()['steps']])\n            cv_scores.append((run_id, dataset_id, pipeline_name, scores.mean()))\n\nLet’s see the final results of the experiment:\n\ncv_scores = (\n    pd.DataFrame(cv_scores, columns=[\"Run\", \"Dataset\", \"Pipeline\", \"Score\"])\n    .groupby([\"Dataset\", \"Pipeline\"])[\"Score\"]\n    .mean()\n    .reset_index()\n)\ncv_scores\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n1\n0\nGeometricSMOTE-LogisticRegression\n0.281554\n\n\n2\n0\nKNeighborsClassifier\n0.515543\n\n\n3\n0\nLogisticRegression\n0.001622\n\n\n4\n0\nRandomOverSampler-KNeighborsClassifier\n0.586250\n\n\n5\n0\nRandomOverSampler-LogisticRegression\n0.282944\n\n\n6\n0\nSMOTE-KNeighborsClassifier\n0.579605\n\n\n7\n0\nSMOTE-LogisticRegression\n0.280936\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n9\n1\nGeometricSMOTE-LogisticRegression\n0.188074\n\n\n10\n1\nKNeighborsClassifier\n0.316577\n\n\n11\n1\nLogisticRegression\n0.003130\n\n\n12\n1\nRandomOverSampler-KNeighborsClassifier\n0.460189\n\n\n13\n1\nRandomOverSampler-LogisticRegression\n0.188727\n\n\n14\n1\nSMOTE-KNeighborsClassifier\n0.428110\n\n\n15\n1\nSMOTE-LogisticRegression\n0.189548\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n17\n2\nGeometricSMOTE-LogisticRegression\n0.295562\n\n\n18\n2\nKNeighborsClassifier\n0.522802\n\n\n19\n2\nLogisticRegression\n0.006476\n\n\n20\n2\nRandomOverSampler-KNeighborsClassifier\n0.592432\n\n\n21\n2\nRandomOverSampler-LogisticRegression\n0.290656\n\n\n22\n2\nSMOTE-KNeighborsClassifier\n0.580532\n\n\n23\n2\nSMOTE-LogisticRegression\n0.294270\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n25\n3\nGeometricSMOTE-LogisticRegression\n0.191214\n\n\n26\n3\nKNeighborsClassifier\n0.323485\n\n\n27\n3\nLogisticRegression\n0.006260\n\n\n28\n3\nRandomOverSampler-KNeighborsClassifier\n0.454507\n\n\n29\n3\nRandomOverSampler-LogisticRegression\n0.195238\n\n\n30\n3\nSMOTE-KNeighborsClassifier\n0.428896\n\n\n31\n3\nSMOTE-LogisticRegression\n0.193547\n\n\n\n\n\n\n\nThe next table shows the pipeline with the highest F-score per dataset:\n\ncv_scores_best = cv_scores.loc[cv_scores.groupby(\"Dataset\")[\"Score\"].idxmax()]\ncv_scores_best\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n\n\n\n\n\nTherefore, Geometric SMOTE outperforms the other methods in all datasets when the F-score is used as an evaluation metric."
  },
  {
    "objectID": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html",
    "href": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html",
    "title": "cluster-over-sampling",
    "section": "",
    "text": "The library cluster-over-sampling implements a general interface for clustering based over-sampling algorithms, a class of algorithms that deal with the within-classes imbalance issue, since SMOTE and its variants addresses only the between-classes imbalance. To present the API, let’s first load some data:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\n\nThe data are imbalanced:\n\n# Imports\nfrom collections import Counter\n\n# Classes distribution\ncounter = Counter(y)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 212.\n\n\nI will use KMeans and SMOTE to create a clustering-based oversampler, but any other combination would work:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom clover.over_sampling import ClusterOverSampler\n\n# Create KMeans-SMOTE instance\nrnd_seed = 14\nsmote = SMOTE(random_state=rnd_seed + 1)\nkmeans = KMeans(n_clusters=10, random_state=rnd_seed + 3, n_init=50)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\nNow we can use the fit method to calculate statistics of the oversampling process:\n\nkmeans_smote.fit(X, y)\nprint(\n    f\"Number of generated minority class instances to rebalance dataset: {kmeans_smote.sampling_strategy_[0]}.\"\n)\n\nNumber of generated minority class instances to rebalance dataset: 145.\n\n\nOr use the fit_resample method to get the resampled data:\n\n_, y_res = kmeans_smote.fit_resample(X, y)\ncounter = Counter(y_res)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 357.\n\n\nThe clustering-based oversamplers can be used in machine learning pipelines:\n\n# Imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\n\n# Cross validation score\nclassifier = RandomForestClassifier(random_state=rnd_seed)\nclassifier = make_pipeline(kmeans_smote, classifier)\nscore = cross_val_score(estimator=classifier, X=X, y=y, scoring=\"f1\").mean()\nprint(f\"The cross-validation F-score is {score}.\")\n\nThe cross-validation F-score is 0.9664262119887302.\n\n\nFor more details and examples you can check the documentation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Prefect\n\n\n\n\n\n\nSoftware Engineering\n\n\nOpen Source\n\n\nData Engineering\n\n\n\nBuild, deploy and observe data workflows.\n\n\n\n\n\nJun 22, 2023\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\ncluster-over-sampling\n\n\n\n\n\n\nMachine Learning\n\n\nOpen Source\n\n\nImbalanced Data\n\n\n\nA general interface for clustering based over-sampling algorithms.\n\n\n\n\n\nMay 2, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric SMOTE algorithm\n\n\n\n\n\n\nMachine Learning\n\n\nPublication\n\n\nImbalanced Data\n\n\n\nExtending SMOTE’s data generation mechanism.\n\n\n\n\n\nMay 1, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\nNo matching items"
  }
]