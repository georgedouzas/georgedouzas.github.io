[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Georgios Douzas. I am a machine learning researcher at Nova IMS, University of Lisbon, and a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence, with multiple publications in machine learning and high-energy physics journals. My professional experience includes working as a software and machine learning engineer for various companies. Additionally, I often maintain or contribute to open-source projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nPh.D. in Theoretical Particle Physics\nNational Technical University of Athens\n\\(09/03\\) - \\(09/08\\)\nM.Sc. in Physics\nNational Technical University of Athens\n\\(09/01\\) - \\(09/03\\)\nB.Sc. in Physics\nNational and Kapodistrian University of Athens\n\\(09/97\\) - \\(09/01\\)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nMachine Learning Engineer\nTrasys, Greece\n\\(8/20\\) - Present\nDesigned and implemented Parallel Distribution, a software tool for the European Medicines Agency that applies OCR on PDF documents and generates a comparison report. The primary language of implementation was Python, while various text mining and machine learning libraries were used. The frontend of the tool used HTML, CSS, and JavaScript to provide an interactive dashboard of the results.\nMachine Learning Researcher\nUniversity of Lisbon Nova IMS, Portugal\n\\(09/13\\) - \\(09/14\\) & \\(09/18\\) - Present\nDesigned, implemented, and tested various new approaches for the class imbalance problem. Research focused on clustering-based over-sampling methods that deal with the within-the-class imbalance problem. Additionally, Geometric SMOTE, an extension of the SMOTE algorithm, was proposed and implemented. The final publication presented results showing a significant improvement over SMOTE and its variations. Deep learning models, particularly Conditional Generative Adversarial Networks (CGANs), were also used as over-sampling methods with great success. The frameworks of the implementation were TensorFlow, Keras, and PyTorch. Work is published in high-impact machine learning journals. Implementation of the above algorithms was developed and made available as open-source software. Work in progress includes comparative experiments between variations of CGANs as over-samplers and the investigation of novel algorithms in the context of reinforcement learning.\nMachine Learning Engineer\nTripsta, Greece\n\\(10/17\\) - \\(08/18\\)\nDesigned and implemented the main parts of the company’s automated pricing system. These parts included machine learning estimators for the add-ons and the competitor’s prices and the application of metaheuristic algorithms for the budget multi-objective optimization problem. The training data of the various estimators were at the order of TB while the prediction time of the automated pricing system was required to be less than \\(100\\) msec for the incoming \\(50\\)K requests/sec. The implementation languages were Python, Java, and Scala, while Spark, Dask, Scikit-Learn and jMetal were used as distributed data processing, machine learning, and optimization frameworks/libraries.\nData Scientist\nQuantum Retail, Remote \\(12/16\\) - \\(09/17\\)\nWorked on demand forecasting and clustering for retail companies. Proposed and applied machine learning methods to improve the company’s main forecasting solution based on exponential smoothing of the time series data and adjustments guided by a seasonality curve. Boosting trees were selected as the final machine-learning model. Applied feature extraction that integrated the business logic and extensive model hyperparameter tuning, the forecasting precision was improved by \\(30\\)% compared to the original model.\nMachine Learning Engineer\nCERN, Remote\n\\(05/16\\) - \\(09/16\\)\nDeveloped the parallelization of various features for TMVA, the Toolkit for Multivariate Data Analysis with ROOT, as a part of a project funded by Google. ROOT is the main framework developed by CERN to deal with the big data processing, statistical analysis, visualization, and storage of massive amounts of data produced from particle physics experiments. The legacy version was implemented in C++. The parallelized features included the application of brute-force and metaheuristic algorithms to the hyperparameter grid search of machine learning algorithms. The implementation was based on Python and Spark.\nScientific Software Engineer IRI, Greece\n\\(01/14\\) - \\(05/16\\)\nMember of the IRI’s “Solutions and Innovation Team” (R&D) working on the company’s transition towards Open Source and Elastic Computing. Participated in an agile team migrating IRI’s leading US “Price & Promo Analytics” Solution, generating more than \\(\\$25\\)M Annual Revenues, to Hadoop distributed storage and Spark cluster computing. Python was the core language of the implementation, but integration with R and Julia was performed to leverage unique functionality. The legacy version was implemented in SAS. The project’s main objectives were the design of the parallelization schema, the enhancement of data manipulation with the use of distributed processing, and the migration of the statistical modeling algorithms (regression mixed models). The final system processed \\(5\\) years of data for more than \\(300\\) categories containing \\(1\\) million products."
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#introduction",
    "href": "projects/software-engineering/prefect/notebooks/index.html#introduction",
    "title": "Prefect",
    "section": "Introduction",
    "text": "Introduction\nPrefect is a workflow orchestration tool. It makes accessible the creation, scheduling, and monitoring of complex data pipelines. The workflows are defined as Python code, while Prefect provides error handling, retry mechanisms, and a user-friendly dashboard for monitoring. Prefect is based on the following concepts:\n\nTasks: Functions that represent a discrete unit of work in a Prefect workflow.\nFlows: Containers for workflow logic and allow users to interact with and reason about the state of their workflows.\nResults: They represent the data returned by a flow or a task.\nArtifacts: They are formatted outputs rendered in the Prefect UI, such as markdown, tables, or links.\nStates: They represent the status of a particular task run or flow run.\nTask Runners: They allow selecting specific executors for Prefect tasks, such as concurrent, parallel, or distributed execution of tasks.\nRuntime Context: It provides information about the current flow or task run that you can refer to in your code.\nProfiles and Configuration: They are settings that can be used to interact with Prefect Cloud and a Prefect server.\nBlocks: Prefect primitives that enable the storage of configuration and provide a UI interface.\nVariables: They are named, mutable string values, much like environment variables.\nDeployments: They are server-side concepts that encapsulate flows, allowing them to be scheduled and triggered via API.\nDeployment Management: A set of files that describe how to prepare one or more flow deployments.\nWork Pools, Workers & Agents : They bridge the Prefect orchestration environment with the execution environment.\nStorage: It configures how flow code for deployments is persisted and retrieved by Prefect agents.\nFilesystems: They are blocks that allow to read and write data from paths.\nInfrastructure: They are blocks that specify infrastructure for flow runs created by the deployment.\nSchedules : They define how to create new flow runs automatically on a specified cadence.\nLogging: They log useful information about flows and tasks runs on the server."
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#extracting-the-urls",
    "href": "projects/software-engineering/prefect/notebooks/index.html#extracting-the-urls",
    "title": "Prefect",
    "section": "Extracting the URLs",
    "text": "Extracting the URLs\nThe goal is to create a data workflow that downloads soccer data from Football-Data.co.uk. The URL of each of those main leagues has the form 'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv' where season is the season of the league and league_id is the league ID. Let’s define a few of those seasons and leagues:\n\nSEASONS = [\n    '1213',\n    '1314',\n    '1516',\n    '1617',\n    '1718',\n    '1819',\n    '1920',\n    '2021',\n    '2122',\n    '2223',\n]\nLEAGUES_MAPPING = {\n    'E0': 'English',\n    'SC0': 'Scotish',\n    'D1': 'German',\n    'I1': 'Italian',\n    'SP1': 'Spanish',\n    'F1': 'French',\n    'N1': 'Dutch',\n    'B1': 'Belgian',\n    'P1': 'Portuguese',\n    'T1': 'Turkish',\n    'G1': 'Greek',\n}\n\nWe can use the above seasons and leagues to construct a mapping of (URL, league name and season) pairs:\n\nURLS_MAPPING = {\n    f'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv': (\n        league,\n        '-'.join([season[0:2], season[2:]]),\n    )\n    for season in SEASONS\n    for league_id, league in LEAGUES_MAPPING.items()\n}\nURLS_MAPPING\n\n{'https://www.football-data.co.uk/mmz4281/1213/E0.csv': ('English', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/SC0.csv': ('Scotish', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/D1.csv': ('German', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/I1.csv': ('Italian', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/SP1.csv': ('Spanish', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/F1.csv': ('French', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/N1.csv': ('Dutch', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/B1.csv': ('Belgian', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/P1.csv': ('Portuguese',\n  '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/T1.csv': ('Turkish', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1213/G1.csv': ('Greek', '12-13'),\n 'https://www.football-data.co.uk/mmz4281/1314/E0.csv': ('English', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/SC0.csv': ('Scotish', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/D1.csv': ('German', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/I1.csv': ('Italian', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/SP1.csv': ('Spanish', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/F1.csv': ('French', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/N1.csv': ('Dutch', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/B1.csv': ('Belgian', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/P1.csv': ('Portuguese',\n  '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/T1.csv': ('Turkish', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1314/G1.csv': ('Greek', '13-14'),\n 'https://www.football-data.co.uk/mmz4281/1516/E0.csv': ('English', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/SC0.csv': ('Scotish', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/D1.csv': ('German', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/I1.csv': ('Italian', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/SP1.csv': ('Spanish', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/F1.csv': ('French', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/N1.csv': ('Dutch', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/B1.csv': ('Belgian', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/P1.csv': ('Portuguese',\n  '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/T1.csv': ('Turkish', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1516/G1.csv': ('Greek', '15-16'),\n 'https://www.football-data.co.uk/mmz4281/1617/E0.csv': ('English', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/SC0.csv': ('Scotish', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/D1.csv': ('German', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/I1.csv': ('Italian', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/SP1.csv': ('Spanish', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/F1.csv': ('French', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/N1.csv': ('Dutch', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/B1.csv': ('Belgian', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/P1.csv': ('Portuguese',\n  '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/T1.csv': ('Turkish', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1617/G1.csv': ('Greek', '16-17'),\n 'https://www.football-data.co.uk/mmz4281/1718/E0.csv': ('English', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/SC0.csv': ('Scotish', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/D1.csv': ('German', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/I1.csv': ('Italian', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/SP1.csv': ('Spanish', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/F1.csv': ('French', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/N1.csv': ('Dutch', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/B1.csv': ('Belgian', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/P1.csv': ('Portuguese',\n  '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/T1.csv': ('Turkish', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1718/G1.csv': ('Greek', '17-18'),\n 'https://www.football-data.co.uk/mmz4281/1819/E0.csv': ('English', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/SC0.csv': ('Scotish', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/D1.csv': ('German', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/I1.csv': ('Italian', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/SP1.csv': ('Spanish', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/F1.csv': ('French', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/N1.csv': ('Dutch', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/B1.csv': ('Belgian', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/P1.csv': ('Portuguese',\n  '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/T1.csv': ('Turkish', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1819/G1.csv': ('Greek', '18-19'),\n 'https://www.football-data.co.uk/mmz4281/1920/E0.csv': ('English', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/SC0.csv': ('Scotish', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/D1.csv': ('German', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/I1.csv': ('Italian', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/SP1.csv': ('Spanish', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/F1.csv': ('French', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/N1.csv': ('Dutch', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/B1.csv': ('Belgian', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/P1.csv': ('Portuguese',\n  '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/T1.csv': ('Turkish', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/1920/G1.csv': ('Greek', '19-20'),\n 'https://www.football-data.co.uk/mmz4281/2021/E0.csv': ('English', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/SC0.csv': ('Scotish', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/D1.csv': ('German', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/I1.csv': ('Italian', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/SP1.csv': ('Spanish', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/F1.csv': ('French', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/N1.csv': ('Dutch', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/B1.csv': ('Belgian', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/P1.csv': ('Portuguese',\n  '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/T1.csv': ('Turkish', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2021/G1.csv': ('Greek', '20-21'),\n 'https://www.football-data.co.uk/mmz4281/2122/E0.csv': ('English', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/SC0.csv': ('Scotish', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/D1.csv': ('German', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/I1.csv': ('Italian', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/SP1.csv': ('Spanish', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/F1.csv': ('French', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/N1.csv': ('Dutch', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/B1.csv': ('Belgian', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/P1.csv': ('Portuguese',\n  '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/T1.csv': ('Turkish', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2122/G1.csv': ('Greek', '21-22'),\n 'https://www.football-data.co.uk/mmz4281/2223/E0.csv': ('English', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/SC0.csv': ('Scotish', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/D1.csv': ('German', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/I1.csv': ('Italian', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/SP1.csv': ('Spanish', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/F1.csv': ('French', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/N1.csv': ('Dutch', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/B1.csv': ('Belgian', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/P1.csv': ('Portuguese',\n  '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/T1.csv': ('Turkish', '22-23'),\n 'https://www.football-data.co.uk/mmz4281/2223/G1.csv': ('Greek', '22-23')}\n\n\nWe will use the above URLs to download and extract the data into a single dataframe. Additionally, the following imports will be required:\n\nfrom time import time\nimport httpx\nimport asyncio\nimport pandas as pd\nfrom io import StringIO\nfrom prefect import flow\nfrom prefect import task\nfrom prefect.logging import get_run_logger\nfrom prefect.task_runners import ConcurrentTaskRunner"
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#just-python-functions",
    "href": "projects/software-engineering/prefect/notebooks/index.html#just-python-functions",
    "title": "Prefect",
    "section": "Just Python functions",
    "text": "Just Python functions\nThe simplest approach to implement the data workflow is not to use Prefect and rely on Python functions. Let’s start by defining the three following functions:\n\ndef request_csv_data(client, url, **kwargs):\n    response = client.get(url=url)\n    return response\n\n\ndef download_csvs_data(urls_mapping):\n    with httpx.Client() as client:\n        responses = [\n            request_csv_data(client, url)\n            for url, (league, season) in urls_mapping.items()\n        ]\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    return csvs_data\n\n\ndef download_data(urls_mapping):\n    csvs_data = download_csvs_data(urls_mapping)\n    data = [pd.read_csv(csv_data, encoding='windows-1254') for csv_data in csvs_data]\n    data = pd.concat(data, ignore_index=True)\n    return data\n\n\nrequest_csv_data will use the parameters client and url of a CSV to request the data.\ndownload_csvs_data will use the parameter urls_mapping and the request_csv_data function to download all CSVs data and convert them to a list of StringIO objects that can be read from the pd.read_csv function as dataframes.\ndownload_data will use the parameter urls_mapping and the download_csvs_data function to download all CSVs data, read them as dataframes and combine them into a single dataframe.\n\nLet’s use the last function to run the data workflow:\n\ndata = download_data(URLS_MAPPING)\ndata\n\n\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\n...\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\nUnnamed: 105\n\n\n\n\n0\nE0\n18/08/12\nArsenal\nSunderland\n0.0\n0.0\nD\n0.0\n0.0\nD\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE0\n18/08/12\nFulham\nNorwich\n5.0\n0.0\nH\n2.0\n0.0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nE0\n18/08/12\nNewcastle\nTottenham\n2.0\n1.0\nH\n0.0\n0.0\nD\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nE0\n18/08/12\nQPR\nSwansea\n0.0\n5.0\nA\n0.0\n1.0\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nE0\n18/08/12\nReading\nStoke\n1.0\n1.0\nD\n0.0\n1.0\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n34679\nG1\n13/05/2023\nIonikos\nLamia\n2.0\n2.0\nD\n2.0\n0.0\nH\n...\n-0.50\n2.05\n1.80\n2.04\n1.72\n2.19\n1.87\n2.03\n1.76\nNaN\n\n\n34680\nG1\n13/05/2023\nLevadeiakos\nGiannina\n3.0\n3.0\nD\n2.0\n2.0\nD\n...\n-0.25\n1.82\n2.02\n1.85\n2.01\n2.04\n2.05\n1.86\n1.93\nNaN\n\n\n34681\nG1\n14/05/2023\nAEK\nVolos NFC\n4.0\n0.0\nH\n2.0\n0.0\nH\n...\n-3.50\n1.93\n1.93\n1.92\n1.93\n2.05\n1.94\n1.91\n1.87\nNaN\n\n\n34682\nG1\n14/05/2023\nPanathinaikos\nAris\n1.0\n1.0\nD\n1.0\n1.0\nD\n...\n-1.25\n1.95\n1.90\n1.95\n1.90\n1.99\n1.96\n1.89\n1.90\nNaN\n\n\n34683\nG1\n14/05/2023\nPAOK\nOlympiakos\n0.0\n1.0\nA\n0.0\n1.0\nA\n...\n-0.50\n1.85\n2.00\n1.89\n1.96\n1.98\n2.05\n1.91\n1.88\nNaN\n\n\n\n\n34684 rows × 139 columns\n\n\n\nThe above code works perfectly fine but if you would like to have properties like scheduling, retries, logging, observability etc then you would have to implement these features from scratch."
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#using-task-and-flows",
    "href": "projects/software-engineering/prefect/notebooks/index.html#using-task-and-flows",
    "title": "Prefect",
    "section": "Using task and flows",
    "text": "Using task and flows\nPrefect offers all the above functionality. It also uses some sensible defaults but we can further customize the data workflow. Based on the definitions of Prefect concepts, we can decorate the functions as follows:\n\nrequest_csv_data represents a discrete unit of work and will receive the task decorator.\ndownload_csvs_data contains the above tasks and will receive the flow decorator.\ndownload_data implements the full data workflow and will receive the flow decorator.\n\nTherefore request_csv_data represents tasks, while download_csvs_data is a subflow of the download_data flow:\n\n@task(name='Request CSV data.', retries=5)\ndef request_csv_data(client: httpx.Client, url: str, **kwargs):\n    logger = get_run_logger()\n    start_time = time()\n    response = client.get(url=url)\n    logger.info(\n        f'CSV data, {kwargs[\"league\"]} league and {kwargs[\"season\"]} season, response time: {time() - start_time}s'\n    )\n    return response\n\n\n@flow(name='Download synchronously CSVs data.', validate_parameters=True)\ndef download_csvs_data(urls_mapping: dict[str, tuple[str, str]]):\n    logger = get_run_logger()\n    start_time = time()\n    with httpx.Client() as client:\n        responses = [\n            request_csv_data(client, url, league=league, season=season)\n            for url, (league, season) in urls_mapping.items()\n        ]\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    logger.info(f'CSVs data download time: {time() - start_time}s')\n    return csvs_data\n\n\n@flow(name='Download synchronously data.', validate_parameters=True)\ndef download_data(urls_mapping: dict[str, tuple[str, str]]):\n    logger = get_run_logger()\n    start_time = time()\n    csvs_data = download_csvs_data(urls_mapping)\n    data = [pd.read_csv(csv_data, encoding='windows-1254') for csv_data in csvs_data]\n    data = pd.concat(data, ignore_index=True)\n    logger.info(f'Data download time: {time() - start_time}s')\n    return data\n\nWe run the updated flow:\ndata = download_data(URLS_MAPPING) data"
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#concurrent-task-runner",
    "href": "projects/software-engineering/prefect/notebooks/index.html#concurrent-task-runner",
    "title": "Prefect",
    "section": "Concurrent task runner",
    "text": "Concurrent task runner\nThe above code executes the tasks in a sequence. This is not optimal for downloading a large number of files. Instead, using an asynchronous httpx client will concurrently download the data. A current limitation of Prefect is that it does not allow passing the asynchronous client from the flow to the tasks. Therefore we remove the task decorator from request_csv_data. Nevertheless, we can still log the same message with the use of the print function and the log_prints parameter of the flow decorator:\n\nasync def request_csv_data(client: httpx.AsyncClient, url: str, **kwargs):\n    start_time = time()\n    response = await client.get(url=url)\n    print(\n        f'CSV data, {kwargs[\"league\"]} league and {kwargs[\"season\"]} season, response time: {time() - start_time}s'\n    )\n    return response\n\n\n@flow(name='Download asynchronously CSVs data.', validate_parameters=True)\nasync def download_csvs_data(urls_mapping: dict[str, tuple[str, str]]):\n    logger = get_run_logger()\n    start_time = time()\n    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=30)) as client:\n        requests = [\n            request_csv_data(client, url, league=league, season=season)\n            for url, (league, season) in urls_mapping.items()\n        ]\n        responses = await asyncio.gather(*requests)\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    logger.info(f'CSVs data download time: {time() - start_time}s')\n    return csvs_data\n\n\n@flow(\n    name='Download asynchronously the data.',\n    validate_parameters=True,\n    task_runner=ConcurrentTaskRunner(),\n    log_prints=True,\n)\nasync def download_data(urls_mapping: dict[str, tuple[str, str]]):\n    logger = get_run_logger()\n    start_time = time()\n    csvs_data = await download_csvs_data(urls_mapping)\n    data = [pd.read_csv(csv_data, encoding='windows-1254') for csv_data in csvs_data]\n    data = pd.concat(data, ignore_index=True)\n    logger.info(f'Data download time: {time() - start_time}s')\n    return data\n\nRunning the flow speeds up the process significantly:\n\ndata = await download_data(URLS_MAPPING)\ndata\n\n17:27:14.667 | INFO    | prefect.engine - Created flow run 'amethyst-reindeer' for flow 'Download asynchronously the data.'\n\n\n\n17:27:14.910 | INFO    | Flow run 'amethyst-reindeer' - Created subflow run 'nebulous-wren' for flow 'Download asynchronously CSVs data.'\n\n\n\n17:27:15.642 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 12-13 season, response time: 0.6336770057678223s\n\n\n\n17:27:15.652 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 12-13 season, response time: 0.6416089534759521s\n\n\n\n17:27:15.660 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 13-14 season, response time: 0.6477789878845215s\n\n\n\n17:27:15.727 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 15-16 season, response time: 0.7104108333587646s\n\n\n\n17:27:15.751 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 12-13 season, response time: 0.7429869174957275s\n\n\n\n17:27:15.760 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 12-13 season, response time: 0.7498030662536621s\n\n\n\n17:27:15.768 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 12-13 season, response time: 0.7591559886932373s\n\n\n\n17:27:15.781 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 12-13 season, response time: 0.7717771530151367s\n\n\n\n17:27:15.792 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 12-13 season, response time: 0.7832348346710205s\n\n\n\n17:27:15.818 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 15-16 season, response time: 0.8035109043121338s\n\n\n\n17:27:15.826 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 13-14 season, response time: 0.8155229091644287s\n\n\n\n17:27:15.834 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 13-14 season, response time: 0.8216331005096436s\n\n\n\n17:27:15.843 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 13-14 season, response time: 0.8309857845306396s\n\n\n\n17:27:15.849 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 15-16 season, response time: 0.8328042030334473s\n\n\n\n17:27:15.855 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 15-16 season, response time: 0.8402910232543945s\n\n\n\n17:27:15.862 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 13-14 season, response time: 0.8501789569854736s\n\n\n\n17:27:15.874 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 13-14 season, response time: 0.8621432781219482s\n\n\n\n17:27:15.879 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 12-13 season, response time: 0.8697159290313721s\n\n\n\n17:27:15.886 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 13-14 season, response time: 0.874751091003418s\n\n\n\n17:27:15.893 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 12-13 season, response time: 0.8853902816772461s\n\n\n\n17:27:15.900 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 15-16 season, response time: 0.8842580318450928s\n\n\n\n17:27:15.909 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 13-14 season, response time: 0.897454023361206s\n\n\n\n17:27:15.916 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 15-16 season, response time: 0.8997268676757812s\n\n\n\n17:27:15.924 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 13-14 season, response time: 0.912977933883667s\n\n\n\n17:27:15.928 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 13-14 season, response time: 0.9161601066589355s\n\n\n\n17:27:15.938 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 15-16 season, response time: 0.9251151084899902s\n\n\n\n17:27:15.944 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 13-14 season, response time: 0.9332690238952637s\n\n\n\n17:27:15.953 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 15-16 season, response time: 0.9398610591888428s\n\n\n\n17:27:15.961 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 15-16 season, response time: 0.944206953048706s\n\n\n\n17:27:15.968 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 15-16 season, response time: 0.9515140056610107s\n\n\n\n17:27:15.975 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 16-17 season, response time: 0.9584231376647949s\n\n\n\n17:27:15.981 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 16-17 season, response time: 0.9639420509338379s\n\n\n\n17:27:15.987 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 15-16 season, response time: 0.9702777862548828s\n\n\n\n17:27:16.021 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 21-22 season, response time: 0.9885902404785156s\n\n\n\n17:27:16.025 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 16-17 season, response time: 1.0077612400054932s\n\n\n\n17:27:16.036 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 21-22 season, response time: 1.0038988590240479s\n\n\n\n17:27:16.068 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 16-17 season, response time: 1.050347089767456s\n\n\n\n17:27:16.099 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 16-17 season, response time: 1.081061840057373s\n\n\n\n17:27:16.109 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 12-13 season, response time: 1.0996479988098145s\n\n\n\n17:27:16.118 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 21-22 season, response time: 1.0851571559906006s\n\n\n\n17:27:16.126 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 16-17 season, response time: 1.1075646877288818s\n\n\n\n17:27:16.139 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 17-18 season, response time: 1.1196048259735107s\n\n\n\n17:27:16.143 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 16-17 season, response time: 1.1241426467895508s\n\n\n\n17:27:16.178 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 16-17 season, response time: 1.1599040031433105s\n\n\n\n17:27:16.228 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 12-13 season, response time: 1.2183952331542969s\n\n\n\n17:27:16.277 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 16-17 season, response time: 1.2594189643859863s\n\n\n\n17:27:16.451 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 18-19 season, response time: 1.42592191696167s\n\n\n\n17:27:16.480 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 16-17 season, response time: 1.4612810611724854s\n\n\n\n17:27:16.487 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 17-18 season, response time: 1.4681837558746338s\n\n\n\n17:27:16.550 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 18-19 season, response time: 1.5270898342132568s\n\n\n\n17:27:16.575 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 18-19 season, response time: 1.5525729656219482s\n\n\n\n17:27:16.584 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 17-18 season, response time: 1.564594030380249s\n\n\n\n17:27:16.590 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 17-18 season, response time: 1.5696821212768555s\n\n\n\n17:27:16.595 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 18-19 season, response time: 1.5704259872436523s\n\n\n\n17:27:16.601 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 18-19 season, response time: 1.5757250785827637s\n\n\n\n17:27:16.615 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 18-19 season, response time: 1.590122938156128s\n\n\n\n17:27:16.624 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 17-18 season, response time: 1.602999210357666s\n\n\n\n17:27:16.631 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 19-20 season, response time: 1.6042449474334717s\n\n\n\n17:27:16.636 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 17-18 season, response time: 1.6165509223937988s\n\n\n\n17:27:16.643 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 17-18 season, response time: 1.6220529079437256s\n\n\n\n17:27:16.683 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 19-20 season, response time: 1.6570820808410645s\n\n\n\n17:27:16.688 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 19-20 season, response time: 1.6616299152374268s\n\n\n\n17:27:16.694 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 16-17 season, response time: 1.6760220527648926s\n\n\n\n17:27:16.699 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 18-19 season, response time: 1.674577236175537s\n\n\n\n17:27:16.718 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 18-19 season, response time: 1.694092035293579s\n\n\n\n17:27:16.725 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 19-20 season, response time: 1.6995530128479004s\n\n\n\n17:27:16.730 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 18-19 season, response time: 1.7057158946990967s\n\n\n\n17:27:16.743 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 19-20 season, response time: 1.7170801162719727s\n\n\n\n17:27:16.749 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 18-19 season, response time: 1.7246227264404297s\n\n\n\n17:27:16.751 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 17-18 season, response time: 1.731330156326294s\n\n\n\n17:27:16.760 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 17-18 season, response time: 1.7398650646209717s\n\n\n\n17:27:16.787 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 19-20 season, response time: 1.760179042816162s\n\n\n\n17:27:16.891 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 18-19 season, response time: 1.867173194885254s\n\n\n\n17:27:16.897 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 17-18 season, response time: 1.8779850006103516s\n\n\n\n17:27:17.164 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 19-20 season, response time: 2.1380579471588135s\n\n\n\n17:27:17.282 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 17-18 season, response time: 2.2602460384368896s\n\n\n\n17:27:17.294 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 20-21 season, response time: 2.2662270069122314s\n\n\n\n17:27:17.300 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 20-21 season, response time: 2.272784948348999s\n\n\n\n17:27:17.315 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 20-21 season, response time: 2.2854621410369873s\n\n\n\n17:27:17.356 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 22-23 season, response time: 2.3214991092681885s\n\n\n\n17:27:17.378 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 20-21 season, response time: 2.35066294670105s\n\n\n\n17:27:17.385 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 19-20 season, response time: 2.359006881713867s\n\n\n\n17:27:17.392 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 22-23 season, response time: 2.357428789138794s\n\n\n\n17:27:17.396 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 19-20 season, response time: 2.3701021671295166s\n\n\n\n17:27:17.403 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 20-21 season, response time: 2.375141143798828s\n\n\n\n17:27:17.415 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 21-22 season, response time: 2.382357120513916s\n\n\n\n17:27:17.428 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 20-21 season, response time: 2.4002697467803955s\n\n\n\n17:27:17.457 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 22-23 season, response time: 2.4225428104400635s\n\n\n\n17:27:17.473 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 22-23 season, response time: 2.43923020362854s\n\n\n\n17:27:17.479 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 22-23 season, response time: 2.44524884223938s\n\n\n\n17:27:17.484 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 19-20 season, response time: 2.4569339752197266s\n\n\n\n17:27:17.491 | INFO    | Flow run 'nebulous-wren' - CSV data, Scotish league and 21-22 season, response time: 2.458601951599121s\n\n\n\n17:27:17.527 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 20-21 season, response time: 2.499450922012329s\n\n\n\n17:27:17.543 | INFO    | Flow run 'nebulous-wren' - CSV data, German league and 22-23 season, response time: 2.508971929550171s\n\n\n\n17:27:17.576 | INFO    | Flow run 'nebulous-wren' - CSV data, Belgian league and 22-23 season, response time: 2.541059970855713s\n\n\n\n17:27:17.581 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 20-21 season, response time: 2.5525267124176025s\n\n\n\n17:27:17.625 | INFO    | Flow run 'nebulous-wren' - CSV data, French league and 20-21 season, response time: 2.5975940227508545s\n\n\n\n17:27:17.635 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 22-23 season, response time: 2.60119891166687s\n\n\n\n17:27:17.652 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 22-23 season, response time: 2.617156982421875s\n\n\n\n17:27:17.665 | INFO    | Flow run 'nebulous-wren' - CSV data, Italian league and 22-23 season, response time: 2.630805015563965s\n\n\n\n17:27:17.677 | INFO    | Flow run 'nebulous-wren' - CSV data, Dutch league and 21-22 season, response time: 2.6438891887664795s\n\n\n\n17:27:17.685 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 20-21 season, response time: 2.657824993133545s\n\n\n\n17:27:17.703 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 22-23 season, response time: 2.6679699420928955s\n\n\n\n17:27:17.720 | INFO    | Flow run 'nebulous-wren' - CSV data, Greek league and 21-22 season, response time: 2.6864521503448486s\n\n\n\n17:27:17.784 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 19-20 season, response time: 2.7578470706939697s\n\n\n\n17:27:17.833 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 21-22 season, response time: 2.799938917160034s\n\n\n\n17:27:17.861 | INFO    | Flow run 'nebulous-wren' - CSV data, Portuguese league and 21-22 season, response time: 2.8281331062316895s\n\n\n\n17:27:17.989 | INFO    | Flow run 'nebulous-wren' - CSV data, Spanish league and 21-22 season, response time: 2.956023693084717s\n\n\n\n17:27:17.994 | INFO    | Flow run 'nebulous-wren' - CSV data, English league and 21-22 season, response time: 2.961747884750366s\n\n\n\n17:27:18.031 | INFO    | Flow run 'nebulous-wren' - CSV data, Turkish league and 20-21 season, response time: 3.001934051513672s\n\n\n\n17:27:18.086 | INFO    | Flow run 'nebulous-wren' - CSVs data download time: 3.0944268703460693s\n\n\n\n17:27:18.183 | INFO    | Flow run 'nebulous-wren' - Finished in state Completed()\n\n\n\n17:27:19.362 | INFO    | Flow run 'amethyst-reindeer' - Data download time: 4.614122152328491s\n\n\n\n17:27:19.406 | INFO    | Flow run 'amethyst-reindeer' - Finished in state Completed()\n\n\n\n\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\n...\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\nUnnamed: 105\n\n\n\n\n0\nE0\n18/08/12\nArsenal\nSunderland\n0.0\n0.0\nD\n0.0\n0.0\nD\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE0\n18/08/12\nFulham\nNorwich\n5.0\n0.0\nH\n2.0\n0.0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nE0\n18/08/12\nNewcastle\nTottenham\n2.0\n1.0\nH\n0.0\n0.0\nD\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nE0\n18/08/12\nQPR\nSwansea\n0.0\n5.0\nA\n0.0\n1.0\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nE0\n18/08/12\nReading\nStoke\n1.0\n1.0\nD\n0.0\n1.0\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n34679\nG1\n13/05/2023\nIonikos\nLamia\n2.0\n2.0\nD\n2.0\n0.0\nH\n...\n-0.50\n2.05\n1.80\n2.04\n1.72\n2.19\n1.87\n2.03\n1.76\nNaN\n\n\n34680\nG1\n13/05/2023\nLevadeiakos\nGiannina\n3.0\n3.0\nD\n2.0\n2.0\nD\n...\n-0.25\n1.82\n2.02\n1.85\n2.01\n2.04\n2.05\n1.86\n1.93\nNaN\n\n\n34681\nG1\n14/05/2023\nAEK\nVolos NFC\n4.0\n0.0\nH\n2.0\n0.0\nH\n...\n-3.50\n1.93\n1.93\n1.92\n1.93\n2.05\n1.94\n1.91\n1.87\nNaN\n\n\n34682\nG1\n14/05/2023\nPanathinaikos\nAris\n1.0\n1.0\nD\n1.0\n1.0\nD\n...\n-1.25\n1.95\n1.90\n1.95\n1.90\n1.99\n1.96\n1.89\n1.90\nNaN\n\n\n34683\nG1\n14/05/2023\nPAOK\nOlympiakos\n0.0\n1.0\nA\n0.0\n1.0\nA\n...\n-0.50\n1.85\n2.00\n1.89\n1.96\n1.98\n2.05\n1.91\n1.88\nNaN\n\n\n\n\n34684 rows × 139 columns"
  },
  {
    "objectID": "projects/software-engineering/prefect/notebooks/index.html#prefect-ui-and-deployments",
    "href": "projects/software-engineering/prefect/notebooks/index.html#prefect-ui-and-deployments",
    "title": "Prefect",
    "section": "Prefect UI and deployments",
    "text": "Prefect UI and deployments\nYou can spin up a local Prefect server UI with the prefect server start command in the shell and explore the characteristics of the above Prefect flows we ran. The data are stored in the Prefect database which by default is a local SQLite database. To reset it, you can run the command prefect server database reset -y.\nPrefect also supports deployments i.e. packaging workflow code, settings, and infrastructure configuration so that the data workflow can be managed via the Prefect API and run remotely by a Prefect agent.\nYou can read more at the official Prefect documentation."
  },
  {
    "objectID": "projects/machine-learning/geometric-smote/notebooks/index.html",
    "href": "projects/machine-learning/geometric-smote/notebooks/index.html",
    "title": "Geometric SMOTE algorithm",
    "section": "",
    "text": "The SMOTE algorithm is the most popular oversampler, with many proposed variants. On the other hand, Geometric SMOTE is not another member of the SMOTE’s family since it expands the data generation area and does not use linear interpolation for new samples. You can check the following figure for a visual representation of the their difference:\n\nI have developed a Python implementation of the Geometric SMOTE oversampler called geometric-smote, which integrates with the Scikit-Learn and Imbalanced-Learn ecosystems. To run a comparison experiment, let’s first create various imbalanced binary datasets with different characteristics:\n\n# Imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import ParameterGrid\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced datasets\ndatasets = []\ndatasets_params = ParameterGrid(\n    {\"weights\": [[0.8, 0.2], [0.9, 0.1]], \"class_sep\": [0.01, 0.1]}\n)\nfor data_params in datasets_params:\n    datasets.append(\n        make_classification(\n            random_state=rnd_seed,\n            n_informative=10,\n            n_samples=2000,\n            n_classes=2,\n            **data_params,\n        )\n    )\n\nWe will also create pipelines of various oversamplers, classifiers and their hyperparameters:\n\n# Imports\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom gsmote import GeometricSMOTE\n\n# Pipelines\nclassifiers = [LogisticRegression(), KNeighborsClassifier()]\noversamplers = [None, RandomOverSampler(), SMOTE(), GeometricSMOTE()]\npipelines = []\noversamplers_param_grids = {\n    \"SMOTE\": {\n        \"smote__k_neighbors\": [\n            NearestNeighbors(n_neighbors=2),\n            NearestNeighbors(n_neighbors=3),\n        ]\n    },\n    \"GeometricSMOTE\": {\n        \"geometricsmote__k_neighbors\": [2, 3],\n        \"geometricsmote__deformation_factor\": [0.0, 0.25, 0.5, 0.75, 1.0],\n    },\n}\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=rnd_seed + 5)\nfor classifier in classifiers:\n    for oversampler in oversamplers:\n        oversampler_name = (\n            oversampler.__class__.__name__ if oversampler is not None else None\n        )\n        param_grid = oversamplers_param_grids.get(oversampler_name, {})\n        estimator = (\n            make_pipeline(oversampler, classifier)\n            if oversampler is not None\n            else make_pipeline(classifier)\n        )\n        pipelines.append(GridSearchCV(estimator, param_grid, cv=cv, scoring=\"f1\"))\n\nFinally, we will calculate the nested cross-validation scores of the above pipelines using F-score as evaluation metric:\n\nn_runs = 3\ncv_scores = []\nfor run_id in range(n_runs):\n    for dataset_id, (X, y) in enumerate(datasets):\n        for pipeline_id, pipeline in enumerate(pipelines):\n            for param in pipeline.get_params():\n                if param.endswith(\"__n_jobs\") and param != \"estimator__smote__n_jobs\":\n                    pipeline.set_params(**{param: -1})\n                if param.endswith(\"__random_state\"):\n                    pipeline.set_params(\n                        **{\n                            param: rnd_seed\n                            * (run_id + 1)\n                            * (dataset_id + 1)\n                            * (pipeline_id + 1)\n                        }\n                    )\n            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=10 * run_id)\n            scores = cross_val_score(\n                estimator=pipeline,\n                X=X,\n                y=y,\n                scoring=\"f1\",\n                cv=cv,\n            )\n            print(f\"Run: {run_id} | Dataset: {dataset_id} | Pipeline: {pipeline_id}\")\n            pipeline_name = '-'.join([estimator.__class__.__name__ for _, estimator in pipeline.get_params()['estimator'].get_params()['steps']])\n            cv_scores.append((run_id, dataset_id, pipeline_name, scores.mean()))\n\nLet’s see the final results of the experiment:\n\ncv_scores = (\n    pd.DataFrame(cv_scores, columns=[\"Run\", \"Dataset\", \"Pipeline\", \"Score\"])\n    .groupby([\"Dataset\", \"Pipeline\"])[\"Score\"]\n    .mean()\n    .reset_index()\n)\ncv_scores\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n1\n0\nGeometricSMOTE-LogisticRegression\n0.281554\n\n\n2\n0\nKNeighborsClassifier\n0.515543\n\n\n3\n0\nLogisticRegression\n0.001622\n\n\n4\n0\nRandomOverSampler-KNeighborsClassifier\n0.586250\n\n\n5\n0\nRandomOverSampler-LogisticRegression\n0.282944\n\n\n6\n0\nSMOTE-KNeighborsClassifier\n0.579605\n\n\n7\n0\nSMOTE-LogisticRegression\n0.280936\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n9\n1\nGeometricSMOTE-LogisticRegression\n0.188074\n\n\n10\n1\nKNeighborsClassifier\n0.316577\n\n\n11\n1\nLogisticRegression\n0.003130\n\n\n12\n1\nRandomOverSampler-KNeighborsClassifier\n0.460189\n\n\n13\n1\nRandomOverSampler-LogisticRegression\n0.188727\n\n\n14\n1\nSMOTE-KNeighborsClassifier\n0.428110\n\n\n15\n1\nSMOTE-LogisticRegression\n0.189548\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n17\n2\nGeometricSMOTE-LogisticRegression\n0.295562\n\n\n18\n2\nKNeighborsClassifier\n0.522802\n\n\n19\n2\nLogisticRegression\n0.006476\n\n\n20\n2\nRandomOverSampler-KNeighborsClassifier\n0.592432\n\n\n21\n2\nRandomOverSampler-LogisticRegression\n0.290656\n\n\n22\n2\nSMOTE-KNeighborsClassifier\n0.580532\n\n\n23\n2\nSMOTE-LogisticRegression\n0.294270\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n25\n3\nGeometricSMOTE-LogisticRegression\n0.191214\n\n\n26\n3\nKNeighborsClassifier\n0.323485\n\n\n27\n3\nLogisticRegression\n0.006260\n\n\n28\n3\nRandomOverSampler-KNeighborsClassifier\n0.454507\n\n\n29\n3\nRandomOverSampler-LogisticRegression\n0.195238\n\n\n30\n3\nSMOTE-KNeighborsClassifier\n0.428896\n\n\n31\n3\nSMOTE-LogisticRegression\n0.193547\n\n\n\n\n\n\n\nThe next table shows the pipeline with the highest F-score per dataset:\n\ncv_scores_best = cv_scores.loc[cv_scores.groupby(\"Dataset\")[\"Score\"].idxmax()]\ncv_scores_best\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n\n\n\n\n\nTherefore, Geometric SMOTE outperforms the other methods in all datasets when the F-score is used as an evaluation metric."
  },
  {
    "objectID": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#introduction",
    "href": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#introduction",
    "title": "Clustering-based oversampling",
    "section": "Introduction",
    "text": "Introduction\nSMOTE algorithm and its variants generate synthetic samples along line segments that join minority class instances. SMOTE addresses only the between-classes imbalance. On the other hand, SMOTE does nothing about areas of the input space that differ significantly in the density of a particular class, an issue known as within-classes imbalance.\nA straightforward approach is to combine oversamplers with clustering algorithms. SOMO and KMeans-SMOTE algorithms are specific realizations of this approach that have been shown to outperform other standard oversamplers in a large number of datasets."
  },
  {
    "objectID": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#implementation",
    "href": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#implementation",
    "title": "Clustering-based oversampling",
    "section": "Implementation",
    "text": "Implementation\nI have developed a Python implementation of the above clustering-based oversampling approach called cluster-over-sampling, which integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems. You can check the documentation for more details on installation and the API."
  },
  {
    "objectID": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#functionality",
    "href": "projects/machine-learning/clustering-based-oversampling/notebooks/index.html#functionality",
    "title": "Clustering-based oversampling",
    "section": "Functionality",
    "text": "Functionality\nLet’s first generate a binary class imbalanced dataset, represented by the input matrix X and the target vector y. Using a high value for the flip_y parameter, we ensure that the data are noisy thus the clustering of the input space will help the oversampling process:\n\n# Imports\nfrom sklearn.datasets import make_classification\n\n# Set random seed\nrnd_seed = 4\n\n# Generate imbalanced data\nX, y = make_classification(\n    n_samples=500,\n    n_classes=2,\n    weights=[0.9, 0.1],\n    random_state=rnd_seed,\n    n_informative=3,\n    class_sep=1.0,\n    n_features=10,\n    flip_y=0.3,\n)\n\nThe function print_characteristics extracts and prints the main characteristics of a binary class dataset. Specifically, it prints the number of samples, the number of features, the labels, and the number of samples for the majority and minority classes as well as the Imbalance Ratio, defined as the ratio between the number of instances of the majority and minority classes.\n\n# Imports\nfrom collections import Counter\n\n\n# Define function to print dataset's characteristics\ndef print_characteristics(X, y):\n    n_samples, n_features = X.shape\n    count_y = Counter(y)\n    (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()\n    ir = n_samples_maj / n_samples_min\n    print(\n        f'Number of samples: {n_samples}',\n        f'Number of features: {n_features}',\n        f'Majority class label: {maj_label}',\n        f'Number of majority class samples: {n_samples_maj}',\n        f'Minority class label: {min_label}',\n        f'Number of minority class samples: {n_samples_min}',\n        f'Imbalance Ratio: {ir:.1f}',\n        sep='\\n',\n    )\n\nI use the above function to print the main characteristics of the generated imbalanced dataset:\n\nprint_characteristics(X, y)\n\nNumber of samples: 500\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 379\nMinority class label: 1\nNumber of minority class samples: 121\nImbalance Ratio: 3.1\n\n\nI will combine the SMOTE oversampler and the KMeans clusterer to rebalance the above dataset. The combined clusterer-oversampler can be constructed by importing the SMOTE oversampler from Imbalanced-Learn, KMeans from Scikit-Learn, and ClusterOverSampler from cluster-over-sampling. Then following the Imbalanced Learn API, we can use the fit_resample method to resample the imbalanced dataset:\n\n# Imports\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom clover.over_sampling import ClusterOverSampler\n\n# Create KMeans-SMOTE instance\nsmote = SMOTE(random_state=rnd_seed + 1)\nkmeans = KMeans(n_clusters=10, random_state=rnd_seed + 3, n_init=50)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\n# Fit and resample imbalanced data\nX_res, y_res = kmeans_smote.fit_resample(X, y)\n\nAgain we can print the main characteristics of the rebalanced dataset:\n\nprint_characteristics(X_res, y_res)\n\nNumber of samples: 759\nNumber of features: 10\nMajority class label: 1\nNumber of majority class samples: 380\nMinority class label: 0\nNumber of minority class samples: 379\nImbalance Ratio: 1.0\n\n\nThe default behavior is to generate the appropriate number of minority class samples so that the resampled dataset is perfectly balanced (although this sometimes may result in an approximately balanced dataset). Also, cluster-over-sampling provides for convenience, the clustering-based oversamplers SOMO and KMeans-SMOTE, as well as G-SOMO that uses Geometric SMOTE as the oversampler in place of SMOTE.\nAs mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function calculate_cv_scores calculates the average 5-fold cross-validation F and accuracy scores across 5 runs of a RandomForestClassifier that is optionally combined with an oversampler through a pipeline:\n\n# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\nfrom imblearn.pipeline import make_pipeline\n\n\n# Define function that calculates out-of-sample scores\ndef calculate_cv_scores(oversampler, X, y):\n    cv_scores = []\n    scorers = {\n        'f_score': make_scorer(f1_score),\n        'accuracy': make_scorer(accuracy_score),\n    }\n    n_runs = 5\n    for ind in range(n_runs):\n        rnd_seed = 8 * ind\n        classifier = RandomForestClassifier(random_state=rnd_seed)\n        if oversampler is not None:\n            classifier = make_pipeline(\n                oversampler.set_params(random_state=rnd_seed + 5), classifier\n            )\n        scores = cross_validate(\n            estimator=classifier,\n            X=X,\n            y=y,\n            scoring=scorers,\n            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=rnd_seed + 6),\n        )\n        cv_scores.append([scores[f'test_{scorer}'].mean() for scorer in scorers])\n    return np.mean(cv_scores, axis=0)\n\nUsing the above function, we can calculate the out-of-sample performance when no oversampling is applied, as well as when SMOTE and DBSCAN-SMOTE are used as oversamplers:\n\n# Imports\nfrom sklearn.cluster import DBSCAN\nfrom clover.over_sampling import ClusterOverSampler\n\n# Calculate cross-validation scores\nmapping = {\n    'No oversampling': None,\n    'SMOTE': SMOTE(),\n    'DBSCAN-SMOTE': ClusterOverSampler(oversampler=SMOTE(), clusterer=DBSCAN()),\n}\ncv_scores = {}\nfor name, oversampler in mapping.items():\n    cv_scores[name] = calculate_cv_scores(oversampler, X, y)\ncv_scores = pd.DataFrame(cv_scores, index=['F-score', 'Accuracy'])\ncv_scores\n\n\n\n\n\n\n\n\nNo oversampling\nSMOTE\nDBSCAN-SMOTE\n\n\n\n\nF-score\n0.250849\n0.349954\n0.3656\n\n\nAccuracy\n0.773200\n0.715600\n0.7260\n\n\n\n\n\n\n\nNotice that using accuracy as an evaluation metric is not a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, the F-score is an appropriate evaluation metric for imbalanced data since it considers the accuracies per class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Prefect\n\n\n\n\n\n\nSoftware Engineering\n\n\nOpen Source\n\n\nData Engineering\n\n\n\nBuild, deploy and observe data workflows.\n\n\n\n\n\nJun 22, 2023\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\nClustering-based oversampling\n\n\n\n\n\n\nMachine Learning\n\n\nOpen Source\n\n\nImbalanced Data\n\n\n\nCombining clustering and oversampling to increase classification performance.\n\n\n\n\n\nMay 2, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric SMOTE algorithm\n\n\n\n\n\n\nMachine Learning\n\n\nPublication\n\n\nImbalanced Data\n\n\n\nExtending SMOTE’s data generation mechanism.\n\n\n\n\n\nMay 1, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\nNo matching items"
  }
]