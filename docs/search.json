[
  {
    "objectID": "presentations/reinforcement-learning/index.html#introduction",
    "href": "presentations/reinforcement-learning/index.html#introduction",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "Introduction",
    "text": "Introduction\nThis presentation follows (Zhao 2025).\n\nReinforcement Learning\n\nAn agent learns to optimize a specified target through trial and error interactions with the environment\nTraditional RL methods struggle with high-dimensional state spaces and complex environments\nIntegration of deep learning techniques with RL has led to significant breakthroughs\n\n\n\nLarge Language Models\n\nLLMs typically refer to the Transformer-based language models containing hundreds of billions of parameters\nThey tend to exhibit emergent abilities that are not present in small models\n\nIn-context learning\n\nTask-related texts are included in the prompts as context information\n\nInstruction following\n\nDiverse task-specific datasets formatted with natural language descriptions\n\nStep-by-step reasoning\n\nChain of Thought\nTree of Thought\nGraph of Thought\n\n\n\n\n\nLLM-enhanced Reinforcement Learning\n\nMethods that utilize the high-level cognitive capabilities of pre-trained LLM models to assist the RL paradigm\nTaxonomy\n\nInformation processor\n\nFeature representations extraction\nInformation translation to DSL\n\nReward designer\n\nImplicit\nExplicit\n\nDecision-maker\n\nAction making\nAction guiding\n\nGenerator\n\nWorld model simulator\nPolicy interpreter"
  },
  {
    "objectID": "presentations/reinforcement-learning/index.html#introduction-to-markov-decision-process",
    "href": "presentations/reinforcement-learning/index.html#introduction-to-markov-decision-process",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "Introduction to Markov Decision Process",
    "text": "Introduction to Markov Decision Process\n\nDescription\nA Markov Decision Process is a mathematical framework used to describe decision-making in situations where outcomes are partly random and partly controlled by an agent.\nAt each time step, the agent:\n\nObserves the current state of the environment\nTakes an action, which may lead to a transition to a new state\nReceives a reward based on the action taken and the resulting state\n\nThe goal of the agent is to maximize the cumulative reward over time by choosing the best possible actions based on the current state.\n\n\nAutonomous driving\n\nState space: \\(\\mathscr{S} = \\left\\{ (\\text{position}, \\text{speed}, \\text{lane}, \\text{proximity to vehicles}, \\text{traffic signals})_1, \\dots \\right\\}\\)\nAction space: \\(\\mathscr{A} = \\{ \\text{accelerate}, \\text{brake}, \\text{left}, \\text{right}, \\text{stay} \\}\\)\nReward space: \\(\\mathscr{R} = \\{ \\text{+1 for safe driving}, \\text{-10 for collision}, \\text{+5 for reaching the goal} \\}\\)\nModel: The dynamics describe the effects of the vehicle’s actions on its next state, such as how acceleration changes speed and position.\n\n\n\nHealthcare treatment planning\n\nState Space: \\(\\mathscr{S} = \\left\\{ \\big(\\text{disease state}, \\text{treatment response}, \\text{vital signs}, \\text{age} \\big)_1, \\dots \\right\\}\\)\nAction Space: \\(\\mathscr{A} = \\{ \\text{drug A}, \\text{drug B}, \\text{surgery}, \\text{no treatment} \\}\\)\nReward Space: \\(\\mathscr{R} = \\{ \\text{+10 for recovery}, \\text{-5 for side effects}, \\text{-10 for relapse} \\}\\)\nModel: The dynamics describe how treatments affect the patient’s health, with probabilistic transitions between health states based on treatment and health conditions."
  },
  {
    "objectID": "presentations/reinforcement-learning/index.html#probability-theory",
    "href": "presentations/reinforcement-learning/index.html#probability-theory",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "Probability Theory",
    "text": "Probability Theory\n\nRandom experiment\nA Random experiment is a process that produces an unpredictable outcome.\n\n\n\n\n\n\nExample\n\n\n\nA rolling a fair die.\n\n\n\n\nProbability space\nA probability space \\((S, F, P)\\) is defined as follows:\n\nSample space \\(S\\)\n\nSet of possible outcomes\n\nSigma-algebra \\(F\\)\n\n\\(A \\in F \\Rightarrow A^c \\in F\\)\n\\(\\{A_i\\}_{i=1}^{\\infty} \\subseteq F \\Rightarrow \\bigcup_{i=1}^{\\infty} A_i \\in F\\)\n\nProbability measure \\(P\\)\n\n\\(P: F \\rightarrow \\mathbb{R}\\)\n\nNon-negativity \\(P(A) \\geq 0\\) for all \\(A \\in F\\)\nNormalization \\(P(S) = 1\\)\nCountable additivity \\(P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i)\\), \\(A_i\\) disjoint\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nSample space \\(S\\)\n\n\\(S = \\{1, 2, 3, 4, 5, 6\\}\\)\n\nSigma-algebra \\(F\\)\n\n\\(F = \\{\\emptyset, \\{1\\}, \\{2\\}, \\dots, S\\}\\)\n\nProbability measure \\(P\\)\n\n\\(P(\\{i\\}) = \\frac{1}{6}\\) for each \\(i \\in S\\)\n\n\n\n\n\n\nRandom variable\nA random variable \\(X\\) is a function \\(X: S \\to \\mathbb{R}\\) such that \\(\\forall x \\in \\mathbb{R}\\):\n\\[X^{-1}((-\\infty, x]) \\in F\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(X(s) = s\\)\n\\(Y(s) = \\begin{cases} 1, & \\text{if } s \\text{ is even} \\\\ 0, & \\text{if } s \\text{ is odd} \\end{cases}\\)\n\\(Z(s) = \\begin{cases} 1, & \\text{if } s \\leq 3 \\\\ 0, & \\text{if } s &gt; 3 \\end{cases}\\)\n\n\n\n\n\nPMF\nThe PMF of a discrete random variable \\(X\\) is a function \\(p_X: \\mathbb{R} \\to [0, 1]\\) such that:\n\\[p_X(x) = P(X = x)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_X(x) = \\frac{1}{6}\\) for \\(x = 1, 2, 3, 4, 5, 6\\)\n\\(p_Y(0) = p_Y(1) = \\frac{3}{6} = 0.5\\)\n\\(p_Z(0) = p_Z(1) = \\frac{3}{6} = 0.5\\)\n\n\n\n\n\nJoint PMF\nThe joint PMF of two discrete random variables \\(X\\) and \\(Y\\) is defined as follows:\n\\[p_{X,Y}(x, y) = P(X = x, Y = y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_{X,Y}(x, y) =\\begin{cases} \\frac{1}{6}, & \\text{if } (x, y) \\in \\{(1, 0), (2, 1), (3, 0), (4, 1), (5, 0), (6, 1)\\} \\\\ 0, & \\text{otherwise} \\end{cases}\\)\n\n\n\n\n\nConditional PMF\nThe conditional PMF of two discrete random variables \\(X\\) and \\(Y\\) is defined as follows:\n\\[p_{Y|X}(y | x) = \\frac{P_{X,Y}(x, y)}{P_X(x)}\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_{Y|X}(y | x=2) = \\begin{cases} 1, & \\text{if } y = 1 \\\\ 0, & \\text{if } y = 0 \\end{cases}\\)\n\n\n\n\n\nIndependence\nTwo random variables \\(X\\) and \\(Y\\) are independent when for all \\(x, y\\) the following condition holds:\n\\[p_{X,Y}(x, y) = P_X(x)P_Y(y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_{X,Y}(x, y) \\neq p_X(x)p_Y(y)\\) and \\(X\\), \\(Y\\) are not independent\n\n\n\n\n\nConditional independence\nTwo random variables \\(X\\) and \\(Y\\) are conditionally independent given the random variable \\(Z\\) when for all \\(x, y, z\\) the following condition holds:\n\\[p_{Y|X, Z}(y | x, z) = P_{Y|Z}(y | z)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_{Y|X,Z}(y | x, z) = p_{Y|X}(y | x) = \\begin{cases} 1, & \\text{if } x \\text{ is even and } y = 1 \\\\ 1, & \\text{if } x \\text{ is odd and } y = 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\)\n\n\n\n\n\nJoint PMF law of total probability\nThe joint PMF of two random variables \\(X\\) and \\(Y\\) satisfies the following relation:\n\\[p_X(x) = \\sum_y p_{X, Y}(x, y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\sum_y p_{X, Y}(x, y) = p_{X, Y}(x, 0) + p_{X, Y}(x, 1) = p_X(x)\\)\n\\(\\sum_x p_{X, Y}(x, y) = p_{X, Y}(1, y) + p_{X, Y}(2, y) + \\cdots = p_Y(y)\\)\n\n\n\n\n\nConditional PMF chain rule\nThe chain rule of conditional PMFs is the following:\n\\[p_{X|Y}(x \\mid y)=\\sum_z p_{X|Z,Y}(x \\mid z, y) p_{Z|Y}(z \\mid y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(p_{X|Z,Y}(x \\mid z, y) =\\begin{cases} \\frac{1}{2}, & \\text{if } x = 4 \\text{ or } 6, z = 1, y = 1 \\\\ 1, & \\text{if } x = 2, z = 0, y = 1 \\\\ 1, & \\text{if } x = 5, z = 1, y = 0 \\\\ \\frac{1}{2}, & \\text{if } x = 1 \\text{ or } 3, z = 0, y = 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\)\n\\(p_{Z|Y}(z \\mid y) = \\begin{cases} \\frac{2}{3}, & \\text{if } z = 1 \\text{ and } y = 1 \\\\ \\frac{1}{3}, & \\text{if } z = 0 \\text{ and } y = 1 \\\\ \\frac{1}{3}, & \\text{if } z = 1 \\text{ and } y = 0 \\\\ \\frac{2}{3}, & \\text{if } z = 0 \\text{ and } y = 0 \\\\0, & \\text{otherwise} \\end{cases}\\)\n\\(p_{X|Y}(x \\mid y) = \\begin{cases} \\frac{1}{3}, & \\text{if } x \\text{ is even and } y = 1 \\\\ \\frac{1}{3}, & \\text{if } x \\text{ is odd and } y = 0 \\\\ 0, & \\text{otherwise} \\end{cases}\\)\n\n\n\n\n\nExpectation\nThe expectation of a discrete random variable \\(X\\) is defined as follows:\n\\[\\mathbb{E}[X]=\\sum_x p_X(x) x\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\mathbb{E}[X]= (1 + 2 + 3 + 4 + 5 + 6) \\cdot \\frac{1}{6} = 3.5\\)\n\n\n\n\n\nConditional expectation\nThe conditional expectation of two discrete random variables \\(X\\) and \\(Y\\) is defined as follows:\n\\[\\mathbb{E}[X \\mid Y=y]=\\sum_x x p_{X|Y}(x \\mid y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\mathbb{E}[X \\mid Y=1] = \\sum_x x p_{X|Y}(x \\mid y=1) = (2 + 4 + 6) \\cdot \\frac{1}{3} = 4\\)\n\n\n\n\n\nLaw of total expectation\nThe law of total expectation of two discrete random variables \\(X\\) and \\(Y\\) is the following:\n\\[\\mathbb{E}[X]=\\sum_y \\mathbb{E}[X \\mid Y=y] p_Y(y)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\mathbb{E}[X]= \\mathbb{E}[X \\mid Y=1] p_Y(1) + \\mathbb{E}[X \\mid Y=0] p_Y(0) = \\frac{4 + 3}{2} = 3.5\\)\n\n\n\n\n\nVariance\nThe variance of a random variable \\(X\\) is defined as follows:\n\\[\\operatorname{var}(X)=\\mathbb{E}\\left[(X-\\bar{X})^2\\right] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\operatorname{var}(X) = \\frac{1^2 + \\cdots + 6^2}{6} - 3.5^2 \\approx 2.92\\)\n\n\n\n\n\nCovariance\nThe covariance of two random variables \\(X\\) and \\(Y\\) is defined as follows:\n\\[\\operatorname{cov}(X, Y)=\\mathbb{E}[(X-\\bar{X})(Y-\\bar{Y})] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\mathbb{E}[Y]\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\operatorname{cov}(X, Y)=\\sum_{x,y} p_{X, Y}(x, y) xy - \\mathbb{E}[X] \\mathbb{E}[Y] = 0.25\\)\n\n\n\n\n\nCovariance matrix\nThe covariance of a random vector \\(\\vec{X}\\) is defined as follows:\n\\[\\Sigma=\\mathbb{E} \\left[(\\vec{X}- \\bar{\\vec{X}})(\\vec{X}-\\bar{\\vec{X}})^T\\right] \\in \\mathbb{R}^{n \\times n}\\]\nIn components form:\n\\[[\\Sigma]_{i j} = \\operatorname{cov}\\left(X_i, X_j\\right)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\Sigma = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X, Y) & \\text{Cov}(X, Z) \\\\ \\text{Cov}(Y, X) & \\text{Var}(Y) & \\text{Cov}(Y, Z) \\\\ \\text{Cov}(Z, X) & \\text{Cov}(Z, Y) & \\text{Var}(Z) \\end{pmatrix} = \\begin{pmatrix} 2.92 & 0.25 & -0.75 \\\\ 0.25 & 0.25 & \\frac{1}{12} \\\\ -0.75 & \\frac{1}{12} & 0.25 \\end{pmatrix}\\)"
  },
  {
    "objectID": "presentations/reinforcement-learning/index.html#basics-of-markov-decision-process",
    "href": "presentations/reinforcement-learning/index.html#basics-of-markov-decision-process",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "Basics of Markov Decision Process",
    "text": "Basics of Markov Decision Process\n\n\nDefinition\n\nSets\n\nState space: \\(\\mathscr{S}=\\left\\{s_1, s_2, \\ldots \\right\\}\\)\nAction space: \\(\\mathscr{A}(s)=\\left\\{a_1, a_2, \\ldots, \\right\\}\\)\nReward space: \\(\\mathscr{R}(s, a)=\\left\\{r_1, r_2, \\ldots \\right\\}\\)\n\nModel\n\nState transition: \\(p(s' \\mid s, a)\\)\nReward: \\(p(r \\mid s, a)\\)\n\nPolicy\n\n\\(\\pi(a \\mid s) = p(a \\mid s)\\)\n\nMarkov property\n\n\\(p\\left(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots \\right)=p\\left(s_{t+1} \\mid s_t, a_t\\right)\\)\n\\(p\\left(r_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots \\right)=p\\left(r_{t+1} \\mid s_t, a_t\\right)\\)\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSets\n\nState space: \\(\\mathscr{S} = \\{ s_1, s_2, s_3, s_4 \\}\\)\nAction space: \\(\\mathscr{A} = \\{ \\uparrow, \\downarrow, \\leftarrow, \\rightarrow, \\bigcirc \\}\\)\nReward space: \\(\\mathscr{R} = \\{ -1, +1, 0 \\}\\)\n\nModel\n\nState transition: \\(p(s' \\mid s, a) = 0 \\text{ or } 1\\)\nReward: \\(p(r \\mid s, a) = 0 \\text{ or } 1\\)\n\nPolicy\n\n\\(\\pi(a \\mid s) = 0 \\text{ or } 1\\)\n\nMarkov property\n\n\\(p\\left(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots \\right)=p\\left(s_{t+1} \\mid s_t, a_t\\right)\\)\n\\(p\\left(r_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots \\right)=p\\left(r_{t+1} \\mid s_t, a_t\\right)\\)\n\n\n\n\n\n\nState-action-reward trajectory\nThe state-action-reward trajectory is defined as follows:\n\\[\\ldots S_t, R_{t} \\overset{A_t} \\rightarrow S_{t+1}, R_{t+1} \\overset{A_{t+1}} \\rightarrow S_{t+2}, R_{t+2} \\ldots\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(\\ldots s_1, 0 \\overset{\\downarrow} \\rightarrow s_3, 0 \\overset{\\rightarrow} \\rightarrow s_4, 1 \\overset{\\bigcirc} \\rightarrow s_4,\n1 \\overset{\\bigcirc} \\rightarrow \\ldots\\)\n\n\n\n\n\nDiscounted return\nThe discounted return is defined as follows:\n\\[G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + \\gamma^2 \\cdot R_{t+3} + \\cdots\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\\(G_t = 0 + \\gamma \\cdot 1 + \\gamma^2 \\cdot 1 + \\cdots = \\frac{\\gamma}{1-\\gamma}\\)\n\n\n\n\n\nTerminal state\n\nExists \\(s_{T}\\) that resets the next state to a starting state\n\nEpisodic task: There are terminal states\nContinuing task: There are no terminal states\n\n\n\n\n\n\n\n\nExample"
  },
  {
    "objectID": "presentations/reinforcement-learning/index.html#state-value-and-bellman-equation",
    "href": "presentations/reinforcement-learning/index.html#state-value-and-bellman-equation",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "State Value and Bellman Equation",
    "text": "State Value and Bellman Equation\n\nState value\nThe state value is defined as follows:\n\\[v_\\pi(s) \\doteq \\mathbb{E}\\left[G_t \\mid S_t=s\\right]\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(v_\\pi(s_1) = -0.5 + \\frac{\\gamma}{1-\\gamma}\\)\n\\(v_\\pi(s_2) = \\frac{1}{1-\\gamma}\\)\n\\(v_\\pi(s_3) = \\frac{1}{1-\\gamma}\\)\n\\(v_\\pi(s_4) = \\frac{1}{1-\\gamma}\\)\n\n\n\n\n\nBellman equation\nThe Bellman equation of the state values is the following:\n\\[v_\\pi(s) = \\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s)\\left[\\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r+\\gamma \\sum_{s^{\\prime} \\in\n\\mathscr{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_\\pi\\left(s^{\\prime}\\right)\\right]\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(v_\\pi(s_1) = -0.5 + 0.5 \\gamma (v_\\pi(s_2) + v_\\pi(s_3))\\)\n\\(v_\\pi(s_2) = 1 + \\gamma v_\\pi(s_4)\\)\n\\(v_\\pi(s_3) = 1 + \\gamma v_\\pi(s_4)\\)\n\\(v_\\pi(s_4) = 1 + \\gamma v_\\pi(s_4)\\)\n\n\n\n\n\nBellman equation equivalent forms\nThe Bellman equation can be written as follows:\n\\[v_\\pi(s)=\\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathscr{S}} \\sum_{r \\in \\mathscr{R}}\np\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_\\pi\\left(s^{\\prime}\\right)\\right]\\]\nIf the reward depends only on the next state the Bellman equation can be written as follows:\n\\[v_\\pi(s)=\\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathscr{S}} p\\left(s^{\\prime} \\mid s,\na\\right)\\left[r\\left(s^{\\prime}\\right)+\\gamma v_\\pi\\left(s^{\\prime}\\right)\\right]\\]\n\n\nBellman equation matrix-vector form\nThe Bellman equation in matrix vector form is written as follows:\n\\[v_\\pi=r_\\pi+\\gamma P_\\pi v_\\pi\\]\nwhere\n\\[\\left[r_\\pi\\right]_s \\doteq \\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) \\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r\\]\n\\[\\left[P_\\pi\\right]_{s s^{\\prime}} \\doteq \\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) p\\left(s^{\\prime} \\mid s, a\\right)\\]\nThe state transition matrix \\(P\\) has the following properties:\n\\[P_\\pi \\geq 0\\]\n\\[P_\\pi \\mathbf{1}=\\mathbf{1}\\]\n\\[\\exists (I-\\gamma P_\\pi)^{-1}\\]\n\\[\\left(I-\\gamma P_\\pi\\right)^{-1} \\geq I\\]\n\\[\\left(I-\\gamma P_\\pi\\right)^{-1} r \\geq , r \\geq 0\\]\n\\[\\left(I-\\gamma P_\\pi\\right)^{-1} r_1 \\geq\\left(I-\\gamma P_\\pi\\right)^{-1} r_2, r_1 \\geq r_2\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(r_\\pi = \\left[\\begin{array}{c}0.5(0)+0.5(-1) \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right]\\)\n\\(P_\\pi = \\left[\\begin{array}{cccc}0 & 0.5 & 0.5 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 1\\end{array}\\right]\\)\n\n\n\n\n\nPolicies and state values\n\n\n\n\n\n\nExample\n\n\n\nThe following “good” policies are different but they have the same state values:\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe following “bad” policies have lower state values:\n\n\n\n\n\nBellman equation closed-form solution\n\\[v_\\pi=\\left(I-\\gamma P_\\pi\\right)^{-1} r_\\pi\\]\n\n\nBellman equation iterative solution\n\\[v_{k+1}=r_\\pi+\\gamma P_\\pi v_k, \\quad k=0,1,2, \\ldots\\]\n\n\nState transition probability\n\\[[P]_{(s, a), s^{\\prime}}=p\\left(s^{\\prime} \\mid s, a\\right)\\]\n\nimport numpy as np\nP = np.array(\n    [\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 0., 1.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 0., 0., 1.]\n    ]\n)\n\n\n\nReward probability\n\\[[R]_{(s, a), r}=p\\left(r \\mid s, a\\right)\\]\n\nR = np.array(\n    [\n        [1., 0., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [1., 0., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.],\n        [0., 1., 0.]\n    ]\n)\n\n\n\nPolicy probability\n\\[[\\Pi]_{a, s} = p(a \\mid s)\\]\n\nPi = np.array([\n    [0.0, 0.0, 0.0, 0.0],\n    [0.5, 1.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\n\n\n\nExpected reward under policy\n\\[\\left[r_\\pi\\right]_s = \\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) \\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r\\]\n\\[\\left[r_\\pi\\right]_s = \\sum_{a \\in \\mathscr{A}} \\big( R \\cdot \\left[\\begin{array}{c}-1 & 1 & 0 \\end{array}\\right]^T \\big)_{s, a} \\cdot [\\Pi]_{a, s}\\]\n\nr_space = np.array([-1, 1, 0])\nr_p = np.diagonal(\n    np.matmul(\n        np.dot(R, r_space).reshape(4, 5), \n        Pi\n    )\n)\nr_p\n\narray([-0.5,  1. ,  1. ,  1. ])\n\n\n\n\nState transition probability under policy\n\\[\\left[P_\\pi\\right]_{s s^{\\prime}} = \\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) p\\left(s^{\\prime} \\mid s, a\\right)\\]\n\\[\\left[P_\\pi\\right]_{s s^{\\prime}} = \\sum_{a \\in \\mathscr{A}} [\\Pi^T]_{s, a} [P]_{(s, a), s^{\\prime}}\\]\n\nP_p = np.diagonal(\n    np.matmul(\n        P.T.reshape(4,4,5), \n        Pi\n    ).T\n).T\nP_p\n\narray([[0. , 0.5, 0.5, 0. ],\n       [0. , 0. , 0. , 1. ],\n       [0. , 0. , 0. , 1. ],\n       [0. , 0. , 0. , 1. ]])\n\n\n\n\nImplementing Bellman equation closed-form solution\n\\[v_\\pi=\\left(I-\\gamma P_\\pi\\right)^{-1} r_\\pi\\]\n\ngamma = 0.9\nI = np.identity(r_p.size)\nv_cfs = np.dot(np.linalg.inv(I - gamma * P_p), r_p)\nv_cfs\n\narray([ 8.5, 10. , 10. , 10. ])\n\n\n\n\nImplementing Bellman equation iterative solution\n\\[v_{k+1}=r_\\pi+\\gamma P_\\pi v_k, \\quad k=0,1,2, \\ldots\\]\n\ntol = 1e-5\nv_is = np.zeros_like(r_p)\nwhile True:\n    v_is_current = v_is\n    v_is = r_p + gamma * np.dot(P_p, v_is)\n    diff = np.abs(v_is - v_is_current)\n    if np.any(diff &lt; tol):\n        break\nv_is\n\narray([8.49991665, 9.99991665, 9.99991665, 9.99991665])"
  },
  {
    "objectID": "presentations/reinforcement-learning/index.html#action-value-and-bellman-equation",
    "href": "presentations/reinforcement-learning/index.html#action-value-and-bellman-equation",
    "title": "Reinforcement Learning and LLM-based Agents",
    "section": "Action Value and Bellman Equation",
    "text": "Action Value and Bellman Equation\n\nAction value\nThe action value is defined as follows:\n\\[q_\\pi(s, a) \\doteq \\mathbb{E}\\left[G_t \\mid S_t=s, A_t=a\\right]\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(q_\\pi(s_1, \\rightarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_1, \\downarrow) = \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_1, \\leftarrow) = -1 + \\gamma (-0.5 + \\frac{\\gamma}{1-\\gamma})\\)\n\\(q_\\pi(s_1, \\uparrow) = -1 + \\gamma (-0.5 + \\frac{\\gamma}{1-\\gamma})\\)\n\\(q_\\pi(s_1, \\bigcirc) = \\gamma (-0.5 + \\frac{\\gamma}{1-\\gamma})\\)\n\\(q_\\pi(s_2, \\rightarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_2, \\downarrow) = 1 + -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_2, \\leftarrow) = \\gamma (-0.5 + \\frac{\\gamma}{1-\\gamma})\\)\n\\(q_\\pi(s_2, \\uparrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_2, \\bigcirc) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_3, \\rightarrow) = 1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_3, \\downarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_3, \\leftarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_3, \\uparrow) = 1 + \\gamma (-0.5 + \\frac{\\gamma}{1-\\gamma})\\)\n\\(q_\\pi(s_3, \\bigcirc) = \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_4, \\rightarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_4, \\downarrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_4, \\leftarrow) = \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_4, \\uparrow) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\\(q_\\pi(s_4, \\bigcirc) = 1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\n\n\n\n\nAction value to state value\nThe following formula provides the conversion from action to state value:\n\\[v_\\pi(s)=\\sum_{a \\in \\mathscr{A}} \\pi(a \\mid s) q_\\pi(s, a)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(v_\\pi(s_1) = 0.5  + 0.5 \\gamma \\frac{1}{1-\\gamma} = -0.5 + \\frac{\\gamma}{1-\\gamma}\\)\n\n\n\n\n\nState value to action value\nThe following formula provides the conversion from state to action value:\n\n\\(q_\\pi(s, a)=\\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r+\\gamma \\sum_{s^{\\prime} \\in \\mathscr{S}} p\\left(s^{\\prime} \\mid s,\na\\right) v_\\pi\\left(s^{\\prime}\\right)\\)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(q_\\pi(s_1, \\rightarrow) = 1(-1) + \\gamma v_\\pi(s_2) = -1 + \\gamma \\frac{1}{1-\\gamma}\\)\n\n\n\n\n\nBellman equations\nThe Bellman equation of the action values is the following:\n\\[q_\\pi(s, a)=\\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r+\\gamma \\sum_{s^{\\prime} \\in \\mathscr{S}} p\\left(s^{\\prime} \\mid s,\na\\right) \\sum_{a^{\\prime} \\in \\mathscr{A}\\left(s^{\\prime}\\right)} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right)\nq_\\pi\\left(s^{\\prime}, a^{\\prime}\\right)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(q_\\pi(s_1, \\rightarrow) = -1 + \\gamma q_\\pi(s_2, \\downarrow)\\)\n\\(q_\\pi(s_1, \\downarrow) = \\gamma q_\\pi(s_3, \\rightarrow)\\)\n\\(q_\\pi(s_1, \\leftarrow) = -1 + \\gamma (q_\\pi(s_1, \\rightarrow) + q_\\pi(s_1, \\downarrow))\\)\n\\(q_\\pi(s_1, \\uparrow) = -1 + \\gamma (q_\\pi(s_1, \\rightarrow) + q_\\pi(s_1, \\downarrow))\\)\n\\(q_\\pi(s_1, \\bigcirc) = \\gamma (q_\\pi(s_1, \\rightarrow) + q_\\pi(s_1, \\downarrow))\\)\n\\(q_\\pi(s_2, \\rightarrow) = -1 + \\gamma q_\\pi(s_2, \\downarrow)\\)\n\\(q_\\pi(s_2, \\downarrow) = 1 + + \\gamma q_\\pi(s_4, \\bigcirc)\\)\n\\(q_\\pi(s_2, \\leftarrow) = \\gamma (q_\\pi(s_1, \\rightarrow) + q_\\pi(s_4, \\downarrow))\\)\n\\(q_\\pi(s_2, \\uparrow) = -1 + \\gamma q_\\pi(s_2, \\downarrow)\\)\n\\(q_\\pi(s_2, \\bigcirc) = -1 + \\gamma q_\\pi(s_2, \\downarrow)\\)\n\\(q_\\pi(s_3, \\rightarrow) = 1 + \\gamma q_\\pi(s_4, \\bigcirc)\\)\n\\(q_\\pi(s_3, \\downarrow) = -1 + \\gamma q_\\pi(s_3, \\rightarrow)\\)\n\\(q_\\pi(s_3, \\leftarrow) = -1 + \\gamma q_\\pi(s_3, \\rightarrow)\\)\n\\(q_\\pi(s_3, \\uparrow) = 1 + \\gamma (q_\\pi(s_1, \\rightarrow) + q_\\pi(s_1, \\downarrow))\\)\n\\(q_\\pi(s_3, \\bigcirc) = \\gamma q_\\pi(s_3, \\rightarrow)\\)\n\\(q_\\pi(s_4, \\rightarrow) = -1 + \\gamma q_\\pi(s_4, \\bigcirc)\\)\n\\(q_\\pi(s_4, \\downarrow) = -1 + \\gamma q_\\pi(s_4, \\bigcirc)\\)\n\\(q_\\pi(s_4, \\leftarrow) = \\gamma q_\\pi(s_3, \\rightarrow)\\)\n\\(q_\\pi(s_4, \\uparrow) = -1 + \\gamma q_\\pi(s_2, \\downarrow)\\)\n\\(q_\\pi(s_4, \\bigcirc) = 1 + \\gamma q_\\pi(s_4, \\bigcirc)\\)\n\n\n\n\n\nBellman equation matrix-vector form\nThe Bellman equation in matrix vector form is written as follows:\n\\[q_\\pi=\\tilde{r}+\\gamma P \\Pi q_\\pi\\]\nwhere\n\\[\\left[q_\\pi\\right]_{(s, a)}=q_\\pi(s, a)\\]\n\\[[\\tilde{r}]_{(s, a)}=\\sum_{r \\in \\mathscr{R}} p(r \\mid s, a) r\\]\n\\[[P]_{(s, a), s^{\\prime}}=p\\left(s^{\\prime} \\mid s, a\\right)\\]\n\\[\\Pi_{s^{\\prime},\\left(s^{\\prime}, a^{\\prime}\\right)}=\\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right)\\]\n\n\n\n\n\n\nExample\n\n\n\n\n\n\\(\\tilde{r} = \\left[\\begin{array}{c} -1 \\\\ 0 \\\\ -1 \\\\ -1 \\\\ 0 \\\\ \\vdots\\end{array}\\right]\\)\n\\(P = \\left[\\begin{array}{cccc}1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{array}\\right]\\)\n\\(\\Pi = \\left[\\begin{array}{cccc}0 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 & 0 & 0 &\\dots \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 &\\dots \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\dots \\end{array}\\right]\\)"
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "",
    "text": "Investors aim to minimize the risks involved in the trading process and maximize profits. Investing in market assets requires successful prediction of prices or trends, as well as optimal allocation of capital among the selected assets in order to meet this objective. For a human being, it is extremely difficult to consider all relevant factors in such a complex and dynamic environment. Thus, the development of adaptive automated trading systems that can meet investor objectives and create profitable trading strategies has been a significant research topic.\nIn the last decade, there have been numerous attempts to develop automated trading systems. Most of these efforts have been focused on using Supervised Learning (SL) techniques (Patel et al. 2015), (Tsantekidis et al. 2017), (Ntakaris et al. 2020), (Hao and Gao 2020), (Vargas et al. 2018)]. Their focus is to create forecasting models trained on historical data. Even though these techniques have gained popularity, they suffer from various limitations, which has lead to suboptimal results (Lopez de Prado 2018).\nOnline decision-making is a core component of financial trading (Deng et al. 2017). Additionally, it is highly time-dependent in nature, making it an ideal candidate for Markov Decision Processes (MDP) (Puterman 1994). In MDP, all the available information is included in the agent’s current state, (Chakraborty 2019). By combining the prediction and allocation steps of trading activity into one unified process, RL deals with a major disadvantage of SL methods: There is no need to define an additional strategy on top of the prediction model since the trading agent interacts with the environment to make optimal decisions in order to optimize the investor’s objective (Meng and Khushi 2019). As demonstrated in previous works (Moody and Saffell 2001), (Dempster and Leemans 2006), RL has the potential to be profitable in trading activities.\nAs the title suggests, we are focusing on intraday trading, or trading “within the day” where all positions are closed before the market closes. Intraday price fluctuations are used by day traders to determine when they should purchase a security and then sell it to take advantage of short-term price fluctuations. There are a variety of intraday trading strategies identified by practitioners, including scalping (high-speed trading), range trading (which utilizes support levels and resistance levels as indicators for buying and selling), and news-based trading (which capitalizes on market volatility resulting from news-based events). Trading strategies that utilize automated trading techniques face three challenges associated with intraday trading:\n\nShort-term financial movements are sometimes associated with short-term noise oscillations.\nThere is a high computational complexity associated with making decisions in the daily continuous-value price range.\nAn early stop of an order when applying intraday trading strategies based on Target Profit (TP) or Stop Loss (SL) signals.\n\nThe majority of works studying RL’s applications in financial markets considered discrete action spaces, such as buying, holding, and selling a fixed amount to trade a single asset (Vargas et al. 2018), (Corazza and Bertoluzzo 2014), (Tan, Quek, and Cheng 2011), and (Deng et al. 2017). To achieve better agent-environment interaction and faster convergence, a continuous action space approach is adopted to gradually adjust the portfolio’s positions with each time step. Additionally, the approach allows to manage multiple assets rather than just one. Based on market constraints, such as liquidity and transaction costs, we first formulate the trading problem as MDP. Specifically, we enhance the state representation with ten different TIs in order to use high-level signals often utilized by traders. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm is then used and a policy is learned in a high-dimensional and continuous action space. Finally, we evaluate our proposed approach by performing back-testing, which is the process traders and analysts use to test a trading strategy on historical data in order to determine its viability.\nIn summary, our main contributions are the following:\n\nWe propose a novel end-to-end daytrade DRL model that directly learns the optimal trading strategy, thus dealing with the early stop of strategies based on TP and SL signals.\nWe constraint the DRL agent’s action space via the utilization of TI.\nCompared to state-of-the-art rule-based and SL-based strategies, our approach is more profitable and robust."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#background",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#background",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Background",
    "text": "Background\nMDP (Alagoz et al. 2010) is a framework that is used to model stochastic processes. They include random variables that transition from one state to another while certain assumptions and probabilistic rules apply. MDPs are used to define RL problems. In MDP, the agent interacts with the environment and the learning process ensues from this interaction. At each time step \\(t\n\\in\\{1,2,3, \\ldots, T\\}\\) the agent receives information of its current state \\(S_{t} \\in \\mathcal{S}\\), and selects an action \\(A_{t}\n\\in \\mathcal{A}\\) to perform. As a result of its action, the agent finds itself in a new state, and the environment returns a reward \\(R_{t+1} \\in \\mathcal{R}\\) to the agent as a feedback regarding the quality of its action (Sutton and Barto 2018).\nIn any RL problem, the goal is to maximize the cumulative reward it receives over time, rather than the immediate reward \\(R_{t}\\):\n\\[\\mathbb{G}_{t} = R_{t+1}+R_{t+2}+R_{t+3}+\\ldots+R_{T}\\]\nThe term \\(R_{T}\\) in the previous formula indicates the reward that is received at the terminal state \\(t=T\\), which means the equation at hand is valid only if the problem is episodic, that is, it ends in a terminal state. In the case of continuous tasks that do not have terminal states, a discount factor known as gamma is introduced:\n\\[\\mathbb{G}_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots+\\gamma^{k-1} R_{t+k}+\\ldots =\\sum_{0}^{\\infty} \\gamma^{k} R_{t+k+1}\\]\nValue functions are being used by RL methods to estimate evaluate states or to state-action pairs. This evaluation is based on the future expected sum of rewards. We call the selection of actions in a given state as s Policy \\(\\pi\\) (Sutton and Barto 2018) which maps states to probabilities of selecting each possible action.\nBellman equations (Bellman and Dreyfus 2010) define linear equations among value functions used in Dynamic Programming. They are fundamental to understand how RL algorithms work. The value function \\(v_{\\pi}(s)\\) of a state \\(s\\) satisifies the following equation:\n\\(v_{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} Pr\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]\\)\nSimilarly, for the action-value function \\(q_{\\pi}(s, a)\\):\n\\(q_{\\pi}(s, a)=\\sum_{s^{\\prime}} \\sum_{r} Pr\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\sum_{a^{\\prime}} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\)\nBellman equations can be used to derive what is referred to as the Bellman Optimality Equations. The Bellman optimality equation expresses the fact that the value of a state under an optimal policy \\(\\pi_{*}\\) must be equal the expected return for the best action from that state (Sutton and Barto 2018). The optimal state-value function \\(v_{*}\\) equals to:\n\\[v_{*}(s)=\\max _{a} \\sum_{s^{\\prime}} \\sum_{r} Pr\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right]\\]\nSimilarly, for the action-value function \\(q_{*}\\) as:\n\\[q_{*}(s, a)=\\max _{\\pi} q_{\\pi}(s, a)=\\sum_{s^{\\prime}} \\sum_{r} Pr\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\]\nIt is possible to classify RL algorithms into three main categories:\n\nCritic-Only\nThis family of algorithms learns to estimate the value function by utilizing a method called Generalized Policy Iteration (GPI). GPI includes two steps. The first one is the policy-evaluation. The primary goal of this step is to collect information under the given policy and evaluate it. The second step is to improve the policy by choosing greedy actions based on the value functions computed from the policy-evaluation step. Once the value functions and policies stabilize, the process has reached an optimal policy when the two steps alternate in a sequential manner.\nThere are two different ways the agent learns the value function of the system. The first is the Tabular Solution Method that finds exact solutions: The value functions are represented as tables and updated after each iteration as the agent collects more experience. However, it requires that the state and action spaces must be small enough to be stored in tables. The second possible way in the critic-only approach is called Approximate Solution Method and it is capable of learning the value function of systems with large state and action spaces. Approximate methods achieve this generalization by combining RL with SL algorithms. DRL is an approximate method that combines Deep Neural Networks with RL (Mnih et al. 2013), (Mnih et al. 2015).\n\n\nActor-Only\nActor-Only methods, also known as Policy Gradient Methods, estimate the gradient of the objective, which is maximizing rewards with respect to the policy parameters and adjust the policy parameters \\(\\theta\\):\n\\[\\theta_{t+1}=\\theta_{t}+\\alpha \\nabla \\ln \\pi\\left(a_{t} \\mid s_{t}, \\theta_{t}\\right) G_{t}\\]\nIn contrast to Critic-Only methods, the parameterized policy function takes state and action as an input and returns the probability of taking that action in that state.\n\n\nActor-Critic\nAn improvement of the original DQN (Hasselt, Guez, and Silver 2015) proposed to use two networks instead of one Q-network to choose the action and the other to evaluate the action taken to solve the deviation problem in DQN. The proposed architecture was called Double-DQN. In this approach, known as Actor-Critic, the actor selects actions at each time step to form the policy, whereas the critic evaluates these actions. In this approach, the policy parameters \\(\\theta\\) are gradually adjusting in order to maximize the total reward predicted by the critic. The error \\(\\delta\\) calculated by the critic to evaluate the action is as follows:\n\\[\\delta=R_{t+1}+\\gamma \\hat{v}\\left(s_{t+1}, w\\right)-\\hat{v}\\left(s_{t}, w\\right)\\]\nThe value function estimation of the current state \\(\\hat{v}\\left(s_{t}, w\\right)\\) is added as a baseline to make the learning faster. The equation to update the gradient at each time step \\(t\\) as the following:\n\\[\\theta_{t+1}=\\theta_{t}+\\alpha \\nabla \\ln \\pi\\left(a_{t} \\mid s_{t}, \\theta_{t}\\right)\\left(R_{t+1}+\\gamma \\hat{v}\\left(s_{t+1}, w\\right)-\\hat{v}\\left(s_{t}, w\\right)\\right)\\]\n(Lillicrap et al. 2015) proposed a variation of Double-DQN, an algorithm based on the deterministic policy gradient (DDPG), for continuous action spaces. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm which we use this work, was proposed in (Fujimoto, Hoof, and Meger 2018) to tackle the problem of the approximation error in DDPG.\n\n\nDeep Reinforcement Learning in trading\nAlgorithmic trading has been applied various subareas, including risk control, portfolio optimization (Giudici, Pagnottoni, and Polinesi 2020), and trading strategies (Vella and Ng 2015), (Chen et al. 2021). Both academia and industry are increasingly interested in AI-based trading, particularly with the RL approach. Nevertheless, RL agents have not been adequately addressed in intraday trading, primarily due to the difficulty in designing an action space for frequent trading strategies. (Moody and Saffell 2001) proposed a RL algorithm as a trading agent and performed a detailed comparison between the Q-learning with the policy gradient method. (Bertoluzzo and Corazza 2012) evaluated the performance of different RL algorithms in day-trading for an Italian stock. Q-learning algorithm and Kernel-based RL were compared, concluding that Q-learning outperformed Kernel-based RL. (Corazza and Bertoluzzo 2014), explored the effect of various reward functions such as Sharpe ratio and average log return on the performance of Q-learning. (Huang et al. 2016) further proposed a robust trading agent based on the DQN architecture. (Deng et al. 2017) proposed a combination of Deep Learning (DL) with Recurrent Reinforcement Learning to directly approximate a policy function. Their method is called Deep Recurrent Reinforcement Learning (DRRL). The DL algorithm extracts 45 useful features from the market. Then a Recurrent Neural Network (RNN) is used as a trading agent to interact with the state features and make decisions. (Conegundes and Pereira 2020) used Deep Deterministic Policy Gradient (DDPG) algorithm to deal with the asset allocation problem. They back-tested their method on the Brazilian Stock Exchange datasets, considering different constraints such as liquidity, latency, slippage, and transaction costs, obtaining \\(311 \\%\\) cumulative return in three years with an annual average maximum drawdown around \\(19 \\%\\).\n\n\nTechnical Indicators in trading\nTrading requires the analysis of various charts and the extraction of strategies based on patterns and indicators. It is accepted among industry practitioners that TI play an important role in market analysis since the financial market is dynamic. The hypothesis of technical analysis states that the future behavior of financial markets is conditioned on its past. Hence TI are being used to provide useful information about market trends and help maximize the returns. We selected the ten most popular TI used often by practitioners (Kirkpatrick and Dahlquist 2011). The following is a brief description of them:\n\nRelative Strength Index (RSI): A momentum indicator to measure the speed and magnitude of an assets’ recent price changes to evaluate overvalued or undervalued conditions in the price.\nSimple Moving Average (SMA): An important indicator to identify current price trends and the potential for a change in an established trend.\nExponential Moving Average (EMA): EMA is considered an improved version of SMA by giving more weight to the recent prices considering old price history less relevant.\nStochastic Oscillator (SO): A momentum indicator comparing the closing price of the asset to a range of its prices in a look-back window period.\nMoving Average Convergence/Divergence (MACD): A popular momentum indicator to identify the relationship between two moving averages of the assets’ price.\nAccumulation/Distribution Oscillator (AD): A volume-based cumulative momentum indicator that assess whether the asset is being accumulated or distributed.\nOn-Balance Volume Indicator (OBVI): A volume-based momentum indicator that uses volume flow to predict the changes in assets’s price.\nPrice Rate Of Change (ROC): A momentum based indicator that measures the speed of assets’ price changes over the look-back window.\nWilliams Percent Range (WPR): A momentum indicator used to spot entry and exit points in the market by comparing the closing price of the asset to the high-low range of prices in the look-back window.\nDisparity Index (DI): It is the percentage equal to the relative position of the current closing price of the asset to a selected moving average."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#states",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#states",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "States",
    "text": "States\nThe state-space is designed to support multiple assets by representing the state as a \\((1 + 12 \\times \\mathcal{N})\\)-dimensional vector, where \\(\\mathcal{N}\\) is the number of assets. Therefore, the state space increases linearly with the number of assets available to be traded. There are two main parts of the state presentation. The first part holds the current cash balance and shares owned by each asset in the portfolio and it is a \\(\\mathbb{R}_{+}^{1+2\\mathcal{N}}\\) vector. The second part of the state is the TI information and it is represented by a \\(\\mathbb{R}^{10 \\times \\mathcal{N}}\\) vector. Therefore, the final state vector at each time step is provided to the agent as follows:\n\\[S_t=\\left[balance_t, share_t^1, \\ldots, share_t^{\\mathcal{N}}, price_t^1, \\ldots, price_t^{\\mathcal{N}}, TI_t^{1,1}, \\ldots, TI_t^{\\mathcal{N},10} \\right]\\]\nEach component of the state vector is defined as follows:\n\n\\(balance_t \\in \\mathbb{R}_{+}\\): The available cash balance in the portfolio at time step \\(t\\).\n\\(share_t^i \\in \\mathbb{Z}_{+}\\): The number of shares owned for each asset \\(i \\in \\mathcal{N}\\) at time step \\(t\\).\n\\(price_t^i \\in \\mathbb{R}_{+}\\): The close price for each asset \\(i \\in \\mathcal{N}\\) at time step \\(t\\).\n\\(TI_t^{i,j}\\) : The \\(j\\)th Technical Indicator for asset \\(i\\) in the portfolio at time step \\(t\\) using the past prices of the asset in a specified look-back window.\n\nTo provide and example of the state space, let’s assume that we have three different assets \\(\\mathcal{N}=3\\) and an initial capital of \\(10000\\) to be invested. Then the state vector would be a 37-dimensional vector with the following initial state:\n\\[S_t=\\left[10000, 0, 0, 0, price_t^1, price_t^2, price_t^3, TI_t^{1,1}, \\ldots, TI_t^{3,10} \\right]\\]"
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#actions",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#actions",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Actions",
    "text": "Actions\nThe agent receives the state \\(S_{t}\\) at each time step \\(t\\) as input and selectes action in the range \\(A_{t}^i \\in[-1,1]\\). Then the action is re-scaled using a constrain \\(K_{\\max }\\), which represents the maximum allocation (buy/sell shares), transforming \\(A_t^i\\) to an integer \\(K \\in\\left[-K_{\\max }, \\ldots,-1,0,1, \\ldots, K_{\\max }\\right]\\), which stands for the number of shares to be executed, resulting in decreasing, increasing or holding of the current position of the corresponding asset. There are two important conditions regarding the action execution in our approach:\n\nIf the current capital in the portfolio is insufficient to execute the buy action, the action will be partially executed.\nIf the number of shares for a specific asset \\(share_{t}^{i}\\) in the portfolio is less than the number of shares to be sold \\(K &lt; 0\\), the agent will sell all the remaining shares of this asset.\n\nThe action vector is expressed as the follows:\n\\[\nA_{t}=\\left[A_{t}^{0}, A_{t}^{1}, \\ldots, A_{t}^{\\mathcal{N}}\\right]\n\\]\nThe dimensionality of the action space depends on the number of assets available in the portfolio and it’s given as \\(\\left(2\n\\times K_{\\max}+1\\right)^{\\mathcal{N}}\\). Hence its dimensionality increases exponentially by increasing \\(\\mathcal{N}\\)."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#rewards",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#rewards",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Rewards",
    "text": "Rewards\nThe portfolio value \\(\\mathcal{value}\\) at each time step is calculated as follows:\n\\[value_{t}=balance_{t} + \\sum_i share_{t}^i \\cdot price_{t}^i\\]\nThe difference between the portfolio value \\(\\mathcal{value}_{t}\\) at the end of period \\(t\\) and the value at the end of previous period \\(t-1\\) represents the immediate reward received by the agent after each action.\nThe transaction cost varies from one broker to another. To simulate more accurately the trading process, transaction costs are included into the immediate reward calculation. We set the transaction cost as a fixed percentage of the total closed deal cash amount, where \\(d_{\\text{buy}}\\) represents the commission percentage when buying is performed, and \\(d_{\\text{sell}}\\) is the commission percentage for selling:\n\\[\n\\begin{gathered}\nd_{t}=\\left[d_{t}^{0}, d_{t}^{1}, \\ldots, d_{t}^{\\mathcal{N}}\\right] \\\\\nd_{t}^{i}= \\begin{cases}d_{\\text {buy }}, & \\text { if } A_{t}^{i}&gt;0 \\\\\n0, & \\text { if } A_{t}^{i}=0 \\\\\nd_{\\text {sell }}, & \\text { if } A_{t}^{i}&lt;0\\end{cases}\n\\end{gathered}\n\\]\nThe commission vector \\(d_{t}\\) is incorporated into the immediate reward function by excluding the commission amount paid from the portfolio value. Consequently, the agent would avoid excessive trading that results in a high commission rate and therefore avoids a negative reward:\n\\[value_{t}=balance_{t} + \\sum_i share_{t}^i \\cdot \\left(price_{t}^i - price_{t-1}^i \\cdot d_{t}^i\\right)\\]\nThe action of buying/selling occurred in the previous state and therefore commission should be calculated using the closing prices on that state. Therefore in the above equation, the amount paid for the commission is calculated by taking the product of the commission vector and the closing price of the previous period."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#assumptions",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#assumptions",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Assumptions",
    "text": "Assumptions\nIn order to simulate as realistically as possible the trading process, we impose the following assumptions on the MDP environment:\n\nNon-negative balance\nThe cash balance in any state is not allowed to be negative. To achieve that, the environment prioritizes the execution of sell actions in the action vector. If the buy action still results in a negative balance, it is fulfilled partially as explained above.\n\n\nShort-selling\nShort selling is prohibited in the designed environment, i.e. all portfolio’s positions must be strictly non-negative.\n\n\nZero slippage\nWhen the market volatility is high, slippage occurs between the price at which the trade was ordered and the price at which it’s completed. In this study, the market liquidity is assumed to be high enough to meet the transaction at the same price when it was ordered.\n\n\nZero market impact\nIn financial markets, a market participant impacts the market when it buys or sells an asset which causes the price change. The impact provoked by the agent in this study is assumed to have no effect on the market when it performs its actions."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#trading-agent",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#trading-agent",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Trading agent",
    "text": "Trading agent\nActor-Critic-based algorithms deal with the continuous action space by utilizing function approximation and policy gradient methods. Deep Deterministic Policy Gradient algorithm (DDPG) is a popular actor-critic, off-policy algorithm. Despite its excellent performance in continuous control problems, it has a significant drawback similar to many RL algorithms: it tends to overestimate the action values as a result of function approximation error. In this study, we use Twin Delayed Deep Deterministic Policy Gradient (TD3) (Fujimoto, Hoof, and Meger 2018) algorithm, which improves the overestimation problem if DDPG. TD3 introduces three main components into DDPG:\n\nClipped double critic networks: It is a variant of Double Q-learning (Hasselt, Guez, and Silver 2015) to replace the single critic. It utilizes two different critic networks to make an independent estimate of the value function.\nDelayed Updates: The second component delays the policy network update and allows the value network to stabilize before it can be used to update the policy gradient. This results in a lower variance of estimates and, therefore, better policy.\nTarget Policy Smoothing Regularization: A regularization strategy is applied to the target policy by adding a small random noise and averaging over mini-batches. This reduces the variance of the target values when updating the critic."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#data",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#data",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Data",
    "text": "Data\nThe following ten cryptocurrencies with high trading volumes are selected:\n\nAAVE\nAVAX\nBTC\nNEAR\nLINK\nETH\nLTC\nMATIC\nUNI\nSOL\n\nA five-minute-level data from 02/02/2022 to 06/27/2022 is used. We split it into a training period (from 02/02/2022 to 04/30/2022) and a testing period (from 05/01/2022 to 06/27/2022), corresponding to approximately 25000 and 16000 observations per asset, respectively. All datasets utilized in experiments are free to download and described in detail in Appendix."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#evaluation-metrics",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#evaluation-metrics",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\nWe use two metrics to evaluate our results. The first metric is the cumulative price return (CPR):\n\\(\\mathrm{CPR}=\\sum \\left(\\mathrm{return}_{\\mathrm{t}}^{\\text {(holding) }}-\\mathrm{return}_{\\mathrm{t}}^{\\text {(settlement) }}\\right)\\)\nThe second metric is the annualized Sharpe Ratio (SR) (Sharpe 1994) which combines the return and the risk to give the average of the risk-free return by the portfolio’s deviation:\n\\(\\mathrm{SR}=\\frac{\\text { Average }(CPR)}{\\text { StandardDeviation }(CPR)}\\)\nIn general, a SR above \\(1.0\\) is considered to be “good” by investors because this suggests that the portfolio is offering excess returns relative to its volatility. A Sharpe ratio higher than \\(2.0\\) is rated as “very good”, while a ratio above \\(3.0\\) is considered “excellent”."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#comparison-methods",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#comparison-methods",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Comparison methods",
    "text": "Comparison methods\nTo compare the performance of the proposed DRL agent, various methods are selected:\n\nMB: Market baseline performance. A strategy used to measure the overall performance of the market during the testing period, by holding the assets consistently. We assume a portfolio of equal assets allocation.\nSLTP: Stop Loss and Take Profit strategy. A simple strategy that consists of using a fixed percentage to determine SL and TP levels.\nMVP: Minimum Variance Portfolio. A popular strategy that aims to maximizes performance while minimizing risk.\nTI: Each one of the 10 TI is used to define a strategy i.e. entry, exit and trade management rules.\nFCM: A SL forecasting model based on a Recurrent Neural Network, consisting of a multi-layer Long Short-Term Memory architecture. It utilizes the Buy-Winner-Sell-Loser strategy.\nRF: Similar to FCM but uses as forecasting model the Random Forest algorithm.\nFDRNN (Deng et al. 2017): A state-of-the-art DRL trader as described above.\n\nA detailed description of each of the above comparison methods is presented in the Appendix."
  },
  {
    "objectID": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#results",
    "href": "publications/intraday-trading-via-deep-reinforcement-learning-and-technical-indicators/index.html#results",
    "title": "Intraday trading via Deep Reinforcement Learning and Technical Indicators",
    "section": "Results",
    "text": "Results\nThe results below are presented for the unseen data of the testing period 01/05/2022 to 06/27/2022.\nTable 1 presents the evaluation of each methods performance, with the metrics of CPR and the SR:\n\n\n\nTable 1: CPR and SR of various strategies compared to proposed method.\n\n\n\n\n\nStrategy\nCPR\nSR\n\n\n\n\nMB\n-42.53\n-0.12\n\n\nSLTP\n-10.91\n-0.03\n\n\nMVP\n-39.13\n-0.07\n\n\nRSI\n-13.23\n-0.02\n\n\nSMA\n-4.56\n-0.03\n\n\nEMA\n-6.23\n-0.03\n\n\nSO\n3.45\n0.04\n\n\nMACD\n1.56\n0.06\n\n\nAD\n2.89\n0.05\n\n\nOBVI\n-3.41\n-0.01\n\n\nROC\n11.62\n0.12\n\n\nWPR\n5.34\n0.08\n\n\nDI\n-7.84\n-0.13\n\n\nFCM\n12.67\n0.58\n\n\nRF\n8.56\n0.43\n\n\nFDRNN\n13.45\n0.64\n\n\nProposed method\n76.34\n2.36\n\n\n\n\n\n\nTable 2 presents the net profit of each strategy:\n\n\n\nTable 2: Net profit of strategies compared to proposed method.\n\n\n\n\n\nStrategy\nCPR\nSR\n\n\n\n\nMB\n-4253.0\n\n\n\nSLTP\n-1290.3\n\n\n\nMVP\n-3913\n\n\n\nRSI\n-1150.4\n\n\n\nSMA\n-389.7\n\n\n\nEMA\n-612.3\n\n\n\nSO\n378.8\n\n\n\nMACD\n136.4\n\n\n\nAD\n293.2\n\n\n\nOBVI\n-341.8\n\n\n\nROC\n1342.3\n\n\n\nWPR\n504.2\n\n\n\nDI\n-584.7\n\n\n\nFCM\n1303.3\n\n\n\nRF\n834.6\n\n\n\nFDRNN\n1401.3\n\n\n\nProposed method\n7348.1\n\n\n\n\n\n\n\nThe CPR of the testing period for the market baseline and the three best methods is presented in Figure 1:\n\n\n\n\n\n\nFigure 1: CPR of market baseline and best methods.\n\n\n\nThe result of MB and MVP strategies denote that the crypto market was in a downtrend and actually crashed twice during the testing period. Excluding SLTP, all day-trading strategies are less affected by this market trend. Particularly, the backtesting results of Table show the good generalization of the proposed method: It is the strategy with the highest CPR and SR. Although it uses the same information as TI, the proposed DRL trading agent seems to form a profitable strategy that utilizes the combined signals and outperforms strategies from single TI. Similarly it outperforms the SL-based strategies FCM and RF as well the state-of-the-art DRL trading agent FDRNN."
  },
  {
    "objectID": "posts/prefect/index.html",
    "href": "posts/prefect/index.html",
    "title": "Prefect",
    "section": "",
    "text": "Introduction\nPrefect is a workflow orchestration tool. It makes accessible the creation, scheduling, and monitoring of complex data pipelines. The workflows are defined as Python code, while Prefect provides error handling, retry mechanisms, and a user-friendly dashboard for monitoring.\n\n\nWorkflow for soccer data\nAs an example, let’s assume that we would like to create a data workflow that downloads, stores and updates historical and fixtures soccer data from Football-Data.co.uk. The URL of each of those main leagues has the following form:\n\nbase_url = 'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv'\nbase_url\n\n'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv'\n\n\nwhere season is the season of the league and league_id is the league ID. Let’s select a few of those seasons and leagues:\n\nSEASONS = ['1819', '1920', '2021', '2122', '2223', '2324']\nLEAGUES_MAPPING = {\n    'E0': 'English',\n    'SC0': 'Scotish',\n    'D1': 'German',\n    'I1': 'Italian',\n    'SP1': 'Spanish',\n    'F1': 'French',\n    'N1': 'Dutch',\n}\nURLS_MAPPING = {\n    f'https://www.football-data.co.uk/mmz4281/{season}/{league_id}.csv': (\n        league,\n        '-'.join([season[0:2], season[2:]]),\n    )\n    for season in SEASONS\n    for league_id, league in LEAGUES_MAPPING.items()\n}\nFIXTURES_URL = 'https://www.football-data.co.uk/fixtures.csv'\n\nOur workflow will include the following tasks:\n\nCheck if a local SQLite database exists. If not, then create it.\nCheck if the database is updated with the latest historical data. If the historical data do not exist, download all the data and store them to the database while if the historical data are not updated, download only the latest data and update the database.\nDownload the latest fixtures data and store them to the database.\n\n\nTasks\nThe above tasks represent discrete units of work, and they will receive the task decorator. We will also use an asynchronous httpx client to concurrently download the data since we have multiple files.\nThe function create_db implements the first task:\n\nimport sqlite3\nfrom prefect import task\nfrom prefect.logging import get_run_logger\nfrom pathlib import Path\nfrom tempfile import mkdtemp\n\nTEMP_DIR = Path(mkdtemp())\n\n\n@task(name='Create database', description='Create the database to store the data')\ndef create_db():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    try:\n        con = sqlite3.connect(f'file:{db_path}?mode=rw', uri=True)\n        logger.info('Database exists.')\n    except sqlite3.OperationalError:\n        con = sqlite3.connect(db_path)\n        logger.info('Database created.')\n    finally:\n        con.close()\n\nThe function update_historical_data implements the second task:\n\nimport httpx\nimport asyncio\nimport pandas as pd\nfrom io import StringIO\n\n\nasync def request_csv_data(client: httpx.Client, url: str, **kwargs):\n    return await client.get(url=url)\n\n\nasync def download_csvs_data(urls_mapping: dict[str, tuple[str, str]]):\n    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=30)) as client:\n        requests = [\n            request_csv_data(client, url, league=league, season=season)\n            for url, (league, season) in urls_mapping.items()\n        ]\n        responses = await asyncio.gather(*requests)\n    csvs_data = [\n        StringIO(str(response.content, encoding='windows-1254'))\n        for response in responses\n    ]\n    return csvs_data\n\n\n@task(\n    name='Update historical data',\n    description='Fetch latest data to update historical data',\n)\nasync def update_historical_data(urls_mapping):\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    with sqlite3.connect(db_path) as con:\n        try:\n            data = pd.read_sql('SELECT * FROM historical', con)\n            logger.info(f'Table with historical data exists. Shape: {data.shape}')\n        except pd.errors.DatabaseError:\n            logger.info('Table with historical data does not exist.')\n            csvs_data = await download_csvs_data(urls_mapping)\n            data = pd.concat(\n                [\n                    pd.read_csv(csv_data, encoding='windows-1254')\n                    for csv_data in csvs_data\n                ],\n                ignore_index=True,\n            )\n            data.to_sql('historical', con=con, index=False)\n            logger.info(f'Table with historical data was created. Shape: {data.shape}')\n            return None\n    urls_mapping = {\n        url: (league, season)\n        for url, (league, season) in urls_mapping.items()\n        if season == '23-24'\n    }\n    latest_csvs_data = await download_csvs_data(urls_mapping)\n    latest_data = pd.concat(\n        [\n            pd.read_csv(csv_data, encoding='windows-1254')\n            for csv_data in latest_csvs_data\n        ],\n        ignore_index=True,\n    )\n    data = pd.concat([data, latest_data], ignore_index=True).drop_duplicates(\n        subset=['Div', 'Date', 'HomeTeam', 'AwayTeam', 'Time'], ignore_index=True\n    )\n    data.to_sql('historical', con=con, index=False, if_exists='replace')\n    logger.info(f'Table with historical data was updated. Shape: {data.shape}')\n\nThe function update_fixtures_data implements the third task:\n\n@task(name='Update fixtures data', description='Fetch latest fixtures data')\nasync def update_fixtures_data():\n    logger = get_run_logger()\n    db_path = TEMP_DIR / 'soccer_data.db'\n    data = pd.read_csv(FIXTURES_URL)\n    with sqlite3.connect(db_path) as con:\n        data.to_sql('fixtures', con=con, index=False, if_exists='replace')\n        logger.info(f'Fixtures data were updated. Shape: {data.shape}')\n\n\n\nFlow\nThe full data workflow will receive the flow decorator.\n\nfrom prefect import flow\nfrom prefect.task_runners import ConcurrentTaskRunner\n\n\n@flow(\n    name='Download asynchronously the data and update the database',\n    validate_parameters=True,\n    task_runner=ConcurrentTaskRunner(),\n    log_prints=True,\n)\nasync def update_db(urls_mapping: dict[str, tuple[str, str]]):\n    create_db()\n    await update_historical_data(urls_mapping)\n    await update_fixtures_data()\n\n\n\n\nResults\nWe can run the above flow:\n\nawait update_db(URLS_MAPPING)\n\n17:41:23.827 | INFO    | prefect.engine - View at https://app.prefect.cloud/account/b258155a-2005-491c-96d1-e0ffa5f1d8f1/workspace/89cadadb-5990-4d0d-9536-3c50d73b387a/runs/flow-run/fd4b6143-6604-4032-b2e4-75673657503e\n\n\n\n17:41:24.261 | INFO    | Flow run 'chirpy-elk' - Beginning flow run 'chirpy-elk' for flow 'Download asynchronously the data and update the database'\n\n\n\n17:41:24.265 | INFO    | Flow run 'chirpy-elk' - View at https://app.prefect.cloud/account/b258155a-2005-491c-96d1-e0ffa5f1d8f1/workspace/89cadadb-5990-4d0d-9536-3c50d73b387a/runs/flow-run/fd4b6143-6604-4032-b2e4-75673657503e\n\n\n\n17:41:24.303 | INFO    | Task run 'Create database-25b' - Database created.\n\n\n\n17:41:24.304 | INFO    | Task run 'Create database-25b' - Finished in state Completed()\n\n\n\n17:41:24.309 | INFO    | Task run 'Update historical data-0b5' - Table with historical data does not exist.\n\n\n\n17:41:26.100 | INFO    | Task run 'Update historical data-0b5' - Table with historical data was created. Shape: (13862, 124)\n\n\n\n17:41:26.102 | INFO    | Task run 'Update historical data-0b5' - Finished in state Completed()\n\n\n\n17:41:26.434 | INFO    | Task run 'Update fixtures data-92f' - Fixtures data were updated. Shape: (195, 102)\n\n\n\n17:41:26.436 | INFO    | Task run 'Update fixtures data-92f' - Finished in state Completed()\n\n\n\n17:41:26.691 | INFO    | Flow run 'chirpy-elk' - Finished in state Completed()\n\n\n\nLet’s read the data from the database:\n\nfrom shutil import rmtree\n\ndb_path = TEMP_DIR / 'soccer_data.db'\nwith sqlite3.connect(db_path) as con:\n    historical_data = pd.read_sql('SELECT * FROM historical', con)\n    fixtures_data = pd.read_sql('SELECT * FROM fixtures', con)\nrmtree(TEMP_DIR)\n\nThe historical data:\n\nhistorical_data\n\n\n\n\n\n\n\n\nDiv\nDate\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\n...\nAvgC&lt;2.5\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\n\n\n\n\n0\nE0\n10/08/2018\nMan United\nLeicester\n2\n1\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE0\n11/08/2018\nBournemouth\nCardiff\n2\n0\nH\n1\n0\nH\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nE0\n11/08/2018\nFulham\nCrystal Palace\n0\n2\nA\n0\n1\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nE0\n11/08/2018\nHuddersfield\nChelsea\n0\n3\nA\n0\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nE0\n11/08/2018\nNewcastle\nTottenham\n1\n2\nA\n1\n2\nA\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13857\nN1\n19/05/2024\nPSV Eindhoven\nWaalwijk\n3\n1\nH\n1\n1\nD\n...\n5.69\n-2.50\n1.73\n2.08\n1.77\n2.09\n1.98\n2.17\n1.85\n1.99\n\n\n13858\nN1\n19/05/2024\nSparta Rotterdam\nHeerenveen\n2\n1\nH\n0\n0\nD\n...\n2.87\n-0.75\n1.85\n2.05\n1.86\n2.03\n1.90\n2.12\n1.83\n2.00\n\n\n13859\nN1\n19/05/2024\nVitesse\nAjax\n2\n2\nD\n1\n1\nD\n...\n3.43\n1.00\n1.84\n2.06\n1.84\n2.06\n1.88\n2.11\n1.82\n2.02\n\n\n13860\nN1\n19/05/2024\nVolendam\nGo Ahead Eagles\n1\n2\nA\n1\n1\nD\n...\n3.52\n1.25\n1.78\n2.03\n1.83\n2.07\n1.85\n2.12\n1.81\n2.02\n\n\n13861\nN1\n19/05/2024\nZwolle\nTwente\n1\n2\nA\n0\n0\nD\n...\n3.30\n1.50\n1.97\n1.93\n1.97\n1.92\n2.14\n1.93\n2.01\n1.83\n\n\n\n\n13862 rows × 124 columns\n\n\n\nThe fixtures data:\n\nfixtures_data\n\n\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nReferee\nB365H\nB365D\nB365A\nBWH\n...\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\nBFECAHH\nBFECAHA\n\n\n\n\n0\nB1\n24/01/2025\n19:45\nSt Truiden\nAntwerp\nNone\n2.55\n3.60\n2.60\n2.50\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n1\nB1\n25/01/2025\n15:00\nWesterlo\nGenk\nNone\n3.30\n3.75\n2.05\n3.25\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n2\nB1\n25/01/2025\n17:15\nCharleroi\nCercle Brugge\nNone\n1.92\n3.50\n3.75\n1.98\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n3\nB1\n25/01/2025\n19:45\nClub Brugge\nKortrijk\nNone\n1.18\n6.50\n15.00\n1.21\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n4\nB1\n26/01/2025\n12:30\nStandard\nDender\nNone\n2.20\n3.25\n3.40\n2.20\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n190\nT1\n26/01/2025\n10:30\nRizespor\nAd. Demirspor\nNone\n1.33\n5.00\n9.00\n1.31\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n191\nT1\n26/01/2025\n13:00\nAntalyaspor\nBesiktas\nNone\n3.90\n3.75\n1.85\n3.75\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n192\nT1\n26/01/2025\n13:00\nBodrumspor\nEyupspor\nNone\n2.70\n3.25\n2.70\n2.70\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n193\nT1\n26/01/2025\n16:00\nFenerbahce\nGoztep\nNone\n1.40\n4.75\n7.00\n1.37\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n194\nT1\n27/01/2025\n17:00\nKasimpasa\nHatayspor\nNone\n1.70\n4.00\n4.50\n1.70\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\n\n195 rows × 102 columns\n\n\n\n\n\nFinal thoughts\nYou can spin up a local Prefect server UI with the prefect server start command in the shell and explore the characteristics of the above Prefect flow we ran. The data are stored in the Prefect database which by default is a local SQLite database.\nPrefect also supports deployments i.e. packaging workflow code, settings, and infrastructure configuration so that the data workflow can be managed via the Prefect API and run remotely by a Prefect agent.\nYou can read more at the official Prefect documentation."
  },
  {
    "objectID": "posts/mechanics_affine_spaces/index.html",
    "href": "posts/mechanics_affine_spaces/index.html",
    "title": "Mechanics",
    "section": "",
    "text": "Introduction\nIt’s quite common to think of physical space as \\(\\mathbb{R}^3\\), but this perspective can be misleading, both from a mathematical and a physical standpoint. The main issue is that the structure of \\(\\mathbb{R}^3\\) isn’t naturally invariant under displacements or other critical transformations, known as isometries, which are fundamental to Euclidean geometry. Instead, the three-dimensional space we encounter in Classical Physics (and even the four-dimensional spacetime in Special Relativity) is more accurately captured by the concept of affine spaces.\n\n\nAffine spaces\nTo understand this better, let’s dive into what a real affine space is. A real affine space of finite dimension \\(n\\), denoted by \\(\\mathbb{A}^n\\), is a collection of elements known as points, but with some additional structure:\n\nThere’s an associated \\(n\\)-dimensional vector space \\(V\\), which we call the space of displacements or the space of free vectors.\nA mapping \\(\\mathbb{A}^n \\times \\mathbb{A}^n \\ni (P, Q) \\mapsto P - Q \\in V\\) that respects certain conditions:\n\nFor every point \\(Q\\in \\mathbb{A}^n\\) and every vector \\(\\mathbf{v} \\in V\\), there’s a unique point \\(P \\in \\mathbb{A}^n\\) such that \\(P - Q =\n  \\mathbf{v}\\).\nFor any three points \\(P, Q, R \\in \\mathbb{A}^n\\), the equation \\(P - Q + Q - R = P - R\\) always holds.\n\n\n\nAdditional definitions\nLet’s define a few useful concepts:\n\nGiven a point \\(Q \\in \\mathbb{A}^n\\) and a vector \\(\\mathbf{v} \\in V\\), the unique point \\(P \\in \\mathbb{A}^n\\) that satisfies \\(P - Q\n= \\mathbf{v}\\) is denoted as \\(Q + \\mathbf{v}\\).\nA line in \\(\\mathbb{A}^n\\) originating from a point \\(P\\) with direction given by a vector \\(\\mathbf{v} \\in V\\) is described by the map \\(\\mathbb{R} \\ni t \\mapsto P + t\\mathbf{v} \\in \\mathbb{A}^n\\).\nA line segment is simply a restriction of the above map to a specific interval.\n\n\n\nProperties\nThe following interesting properties hold for any points \\(P, Q \\in \\mathbb{A}^n\\) and vectors \\(\\mathbf{u}, \\mathbf{v} \\in V\\):\n\n\\(P - P = \\mathbf{0}\\).\n\\((Q + \\mathbf{u}) + \\mathbf{v} = Q + (\\mathbf{u} + \\mathbf{v})\\).\n\\(P - Q = -(Q - P)\\).\n\\(P - Q = (P + \\mathbf{u}) - (Q + \\mathbf{u})\\).\n\n\nYou can try proving these properties to deepen your understanding.\n\n\n\n\nCoordinate systems on affine spaces\nNow that we’ve established what an affine space is, let’s discuss how we can introduce coordinate systems on such spaces. A local coordinate system on an affine space \\(\\mathbb{A}^n\\) is a map \\(\\psi: U \\subset \\mathbb{A}^n \\rightarrow \\mathbb{R}^n\\) that satisfies:\n\nThe map \\(\\psi\\) is injective.\nThe image \\(\\psi(U)\\) is an open subset of \\(\\mathbb{R}^n\\).\n\nIf \\(U = \\mathbb{A}^n\\), we call the coordinate system global.\n\nCartesian coordinate systems\nEvery affine space has a family of global coordinate systems known as Cartesian coordinate systems. To define one, we need to:\n\nChoose a point \\(O\\) (the origin) and a basis \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\) for the vector space \\(V\\).\nDefine a map \\(f: \\mathbb{A}^n \\rightarrow \\mathbb{R}^n\\) by \\(f(P) = \\left((P - O)^1, \\ldots, (P - O)^n\\right)\\).\n\n\nTry proving that this map \\(f\\) is bijective, which will confirm that \\(f\\) is indeed a global coordinate system.\n\nNon-Cartesian local coordinate systems, on the other hand, are referred to as curvilinear coordinate systems.\n\n\nProperties of Cartesian coordinate systems\nSuppose we have two Cartesian coordinate systems: \\(\\left(\\mathbb{A}^n, f\\right)\\) with coordinates \\(x^1, \\cdots, x^n\\), and another \\(\\left(\\mathbb{A}^n, g\\right)\\) with coordinates \\(x^{\\prime 1}, \\cdots, x^{\\prime n}\\), origin \\(O^{\\prime}\\), and basis vectors \\(\\mathbf{e}_1^{\\prime}, \\ldots, \\mathbf{e}_n^{\\prime}\\). If the relation between the basis vectors is:\n\\[\n\\mathbf{e}_i = \\sum_j B^j{}_i \\mathbf{e}_j^{\\prime}\n\\]\nand \\(O - O^{\\prime} = \\sum_i b^i \\mathbf{e}_i\\), then the following properties hold:\n\nThe transformation map \\(g \\circ f^{-1}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is given by \\(x^{\\prime j} = \\sum_{i=1}^n\nB^j{}_i\\left(x^i + b^i\\right)\\).\nConversely, the map \\(f \\circ g^{-1}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is expressed as \\(x^i = -b^i +\n\\sum_{j=1}^n\\left(B^{-1}\\right)^i{}_j x^{\\prime j}\\).\n\n\nAgain, try proving these transformations to see how coordinate systems relate to each other.\n\n\n\n\nAffine transformations between affine spaces\nLet’s now explore how we can map one affine space to another. A map \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^m\\) between two affine spaces, with associated vector spaces \\(V_1\\) and \\(V_2\\), is called an affine transformation if:\n\nThe map \\(\\psi\\) is invariant under displacements, meaning that for any points \\(P, Q \\in \\mathbb{A}_1^n\\) and any vector \\(\\mathbf{u} \\in V_1\\), the equality \\(\\psi(P + \\mathbf{u}) - \\psi(Q + \\mathbf{u}) = \\psi(P) - \\psi(Q)\\) holds.\nThe derivative map \\(d\\psi: V_1 \\rightarrow V_2\\), defined by \\(d\\psi(P - Q) = \\psi(P) - \\psi(Q)\\), is a linear transformation between the vector spaces \\(V_1\\) and \\(V_2\\).\n\n\nProving that \\(d\\psi\\) is well-defined and linear will help you understand the relationship between these spaces.\n\n\nIsomorphism of affine spaces\nAn affine transformation \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^m\\) is called an isomorphism if it is bijective. This concept is particularly important because it tells us when two affine spaces are structurally the same.\n\n\nProperties of affine transformations\nAffine transformations have several key properties:\n\nThe inverse of an isomorphism is itself an affine transformation, making it an isomorphism as well.\nIf \\(\\psi: \\mathbb{A}_1^n \\rightarrow \\mathbb{A}_2^n\\) is an isomorphism, then the derivative map \\(d\\psi: V_1 \\rightarrow V_2\\) is a vector space isomorphism.\nAffine transformations map straight lines to straight lines. Specifically, if \\(P(t) := P + t \\mathbf{u}\\) describes a line in \\(\\mathbb{A}_1^n\\), then \\(\\psi(P(t))\\) defines a line in \\(\\mathbb{A}_2^m\\).\nGiven Cartesian coordinate systems on \\(\\mathbb{A}_1^n\\) and \\(\\mathbb{A}_2^m\\), the affine transformation \\(\\psi\\) has a particularly simple form in these coordinates: \\(x_2^i = c^i + \\sum_{j=1}^n L^i{}_j x_1^j\\), where the coefficients \\(L^i{}_j\\) and \\(c^i\\) depend on \\(\\psi\\) and the chosen coordinate systems.\n\n\nProving these properties gives you a clearer view of how affine spaces relate to each other under transformations.\n\n\n\n\nGroup of displacements of affine spaces\nIn the context of affine spaces, the concept of displacements is crucial. For a given vector \\(\\mathbf{v} \\in V\\), we can define a mapping \\(T_{\\mathbf{v}}: \\mathbb{A}^n \\rightarrow \\mathbb{A}^n\\) that shifts every point \\(P\\) in the affine space by \\(\\mathbf{v}\\). Formally, this map is defined as \\(T_{\\mathbf{v}}(P) = P + \\mathbf{v}\\). The collection of all such transformations, denoted \\(\\left\\{T_{\\mathbf{v}}\\right\\}_{\\mathbf{v} \\in V}\\), forms what is known as the group of displacements of \\(\\mathbb{A}^n\\). This group operates under the composition of mappings.\n\nTo explore this concept further, you can try proving that the set \\(\\left\\{T_{\\mathbf{v}}\\right\\}_{\\mathbf{v} \\in V}\\) indeed forms a group under composition.\n\n\nProperties\nThe group of displacements comes with several interesting properties:\n\nThe group of displacements of \\(\\mathbb{A}^n\\) is abelian, meaning that the order in which you apply displacements doesn’t matter; that is, \\(T_{\\mathbf{v}} \\circ T_{\\mathbf{u}} = T_{\\mathbf{u}} \\circ T_{\\mathbf{v}}\\) for any vectors \\(\\mathbf{u}, \\mathbf{v} \\in\nV\\).\nThe map \\(V \\ni \\mathbf{v} \\mapsto T_{\\mathbf{v}}\\) is injective, which means that each vector \\(\\mathbf{v}\\) corresponds to a unique transformation. Moreover, this mapping is a group isomorphism when \\(V\\) is viewed as an abelian group under vector addition.\nOnly the zero vector \\(\\mathbf{v} = \\mathbf{0}\\) satisfies \\(T_{\\mathbf{v}}(P) = P\\) for some \\(P \\in \\mathbb{A}^n\\). In fact, this holds for every point \\(P\\), so the action of the group of displacements is free.\nFor any two points \\(P, Q \\in \\mathbb{A}^n\\), there exists a displacement \\(T_{\\mathbf{v}}\\) such that \\(T_{\\mathbf{v}}(P) = Q\\). This property highlights the fact that the group of displacements acts transitively on the affine space.\n\n\nThese properties are foundational to understanding the structure of affine spaces. You might find it helpful to prove these properties yourself.\n\n\n\nGroup action\nTo generalize the idea of how a group can interact with a set, we use the concept of a group action. Let’s consider a set \\(S\\) and a group \\(G\\) with a neutral element \\(e\\) and a group operation \\(\\circ\\). A group action is a map \\(A: G \\times S \\ni (g, s) \\mapsto\nA_g(s) \\in S\\), where \\(A_g \\in \\mathcal{G}_S\\) (the group of bijections on \\(S\\) under composition). For this map to be a valid group action, it must satisfy two key properties:\n\nThe action of the neutral element is the identity: \\(A_e = \\text{id}\\).\nThe action respects the group operation: \\(A_g \\circ A_{g^{\\prime}} = A_{g \\circ g^{\\prime}}\\) for all \\(g, g^{\\prime} \\in G\\).\n\nThere are a few special types of group actions that are worth noting:\n\nThe action is called free if \\(A_g(s) = s\\) for some \\(s \\in S\\) implies \\(g = e\\).\nThe action is transitive if for any two elements \\(s, s^{\\prime} \\in S\\), there exists a group element \\(g \\in G\\) such that \\(A_g(s)\n= s^{\\prime}\\).\nThe action is faithful if the map \\(G \\ni g \\mapsto A_g \\in \\mathcal{G}_S\\) is injective, meaning that different elements of \\(G\\) induce different bijections on \\(S\\).\n\nWhenever we have a group action, it naturally defines a group homomorphism from \\(G\\) to \\(\\mathcal{G}_S\\). The image of this homomorphism, denoted \\(G_S = \\left\\{A_g\\right\\}_{g \\in G}\\), is a subgroup of \\(\\mathcal{G}_S\\). If the action is faithful, then this homomorphism is actually an isomorphism between \\(G\\) and \\(G_S\\).\n\nTo deepen your understanding, try proving the property that an action defines a group homomorphism and explore the implications of free, transitive, and faithful actions."
  },
  {
    "objectID": "posts/imbalanced-learn-extra/index.html",
    "href": "posts/imbalanced-learn-extra/index.html",
    "title": "imbalanced-learn-extra",
    "section": "",
    "text": "Introduction\nThe library imbalanced-learn-extra is a Python package that extends imbalanced-learn. It implements algorithms that are not included in imbalanced-learn due to their novelty or lower citation number. The current version includes the following:\n\nA general interface for clustering-based oversampling algorithms.\nThe Geometric SMOTE algorithm.\n\n\n\nClustering-based oversampling\nClustering-based oversampling algorithms deal with the within-classes imbalance issue, since SMOTE and its variants addresses only the between-classes imbalance. To present the API, let’s first load some data:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\n\nThe data are imbalanced:\n\n# Imports\nfrom collections import Counter\n\n# Classes distribution\ncounter = Counter(y)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 212.\n\n\nI will use KMeans and SMOTE to create a clustering-based oversampler, but any other combination would work:\n\n# Imports\nfrom sklearn.datasets import load_breast_cancer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom imblearn_extra.clover.over_sampling import ClusterOverSampler\n\n# Create KMeans-SMOTE instance\nrnd_seed = 14\nsmote = SMOTE(random_state=rnd_seed + 1)\nkmeans = KMeans(n_clusters=10, random_state=rnd_seed + 3, n_init=50)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\nNow we can use the fit_resample method to get the resampled data:\n\n_, y_res = kmeans_smote.fit_resample(X, y)\ncounter = Counter(y_res)\nprint(\n    f\"Number of majority class samples: {counter[1]}.\",\n    f\"Number of minority class samples: {counter[0]}.\",\n    sep=\"\\n\",\n)\n\nNumber of majority class samples: 357.\nNumber of minority class samples: 357.\n\n\nThe clustering-based oversamplers can be used in machine learning pipelines:\n\n# Imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\n\n# Cross validation score\nclassifier = RandomForestClassifier(random_state=rnd_seed)\nclassifier = make_pipeline(kmeans_smote, classifier)\nscore = cross_val_score(estimator=classifier, X=X, y=y, scoring=\"f1\").mean()\nprint(f\"The cross-validation F-score is {score}.\")\n\nThe cross-validation F-score is 0.9664262119887302.\n\n\n\n\nGeometric SMOTE\nGeometric SMOTE is not just another member of the SMOTE’s family since it expands the data generation area and does not just use linear interpolation of existing samples to generate for new samples. To test its performance, let’s first simulate various imbalanced datasets:\n\n# Imports\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import ParameterGrid\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced datasets\ndatasets = []\ndatasets_params = ParameterGrid(\n    {\"weights\": [[0.8, 0.2], [0.9, 0.1]], \"class_sep\": [0.01, 0.1]}\n)\nfor data_params in datasets_params:\n    datasets.append(\n        make_classification(\n            random_state=rnd_seed,\n            n_informative=10,\n            n_samples=2000,\n            n_classes=2,\n            **data_params,\n        )\n    )\n\nWe will also create pipelines of various oversamplers, classifiers and their hyperparameters:\n\n# Imports\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn_extra.gsmote import GeometricSMOTE\n\n# Pipelines\nclassifiers = [LogisticRegression(), KNeighborsClassifier()]\noversamplers = [None, RandomOverSampler(), SMOTE(), GeometricSMOTE()]\npipelines = []\noversamplers_param_grids = {\n    \"SMOTE\": {\n        \"smote__k_neighbors\": [\n            NearestNeighbors(n_neighbors=2),\n            NearestNeighbors(n_neighbors=3),\n        ]\n    },\n    \"GeometricSMOTE\": {\n        \"geometricsmote__k_neighbors\": [2, 3],\n        \"geometricsmote__deformation_factor\": [0.0, 0.25, 0.5, 0.75, 1.0],\n    },\n}\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=rnd_seed + 5)\nfor classifier in classifiers:\n    for oversampler in oversamplers:\n        oversampler_name = (\n            oversampler.__class__.__name__ if oversampler is not None else None\n        )\n        param_grid = oversamplers_param_grids.get(oversampler_name, {})\n        estimator = (\n            make_pipeline(oversampler, classifier)\n            if oversampler is not None\n            else make_pipeline(classifier)\n        )\n        pipelines.append(GridSearchCV(estimator, param_grid, cv=cv, scoring=\"f1\"))\n\nFinally, we will calculate the nested cross-validation scores of the above pipelines using F-score as evaluation metric:\n\nn_runs = 3\ncv_scores = []\nfor run_id in range(n_runs):\n    for dataset_id, (X, y) in enumerate(datasets):\n        for pipeline_id, pipeline in enumerate(pipelines):\n            for param in pipeline.get_params():\n                if param.endswith(\"__n_jobs\") and param != \"estimator__smote__n_jobs\":\n                    pipeline.set_params(**{param: -1})\n                if param.endswith(\"__random_state\"):\n                    pipeline.set_params(\n                        **{\n                            param: rnd_seed\n                            * (run_id + 1)\n                            * (dataset_id + 1)\n                            * (pipeline_id + 1)\n                        }\n                    )\n            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=10 * run_id)\n            scores = cross_val_score(\n                estimator=pipeline,\n                X=X,\n                y=y,\n                scoring=\"f1\",\n                cv=cv,\n            )\n            print(f\"Run: {run_id} | Dataset: {dataset_id} | Pipeline: {pipeline_id}\")\n            pipeline_name = '-'.join(\n                [\n                    estimator.__class__.__name__\n                    for _, estimator in pipeline.get_params()['estimator'].get_params()[\n                        'steps'\n                    ]\n                ]\n            )\n            cv_scores.append((run_id, dataset_id, pipeline_name, scores.mean()))\n\nLet’s see the final results of the experiment:\n\ncv_scores = (\n    pd.DataFrame(cv_scores, columns=[\"Run\", \"Dataset\", \"Pipeline\", \"Score\"])\n    .groupby([\"Dataset\", \"Pipeline\"])[\"Score\"]\n    .mean()\n    .reset_index()\n)\ncv_scores\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n1\n0\nGeometricSMOTE-LogisticRegression\n0.281625\n\n\n2\n0\nKNeighborsClassifier\n0.515543\n\n\n3\n0\nLogisticRegression\n0.001622\n\n\n4\n0\nRandomOverSampler-KNeighborsClassifier\n0.586250\n\n\n5\n0\nRandomOverSampler-LogisticRegression\n0.282728\n\n\n6\n0\nSMOTE-KNeighborsClassifier\n0.579605\n\n\n7\n0\nSMOTE-LogisticRegression\n0.281004\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n9\n1\nGeometricSMOTE-LogisticRegression\n0.186105\n\n\n10\n1\nKNeighborsClassifier\n0.316577\n\n\n11\n1\nLogisticRegression\n0.003130\n\n\n12\n1\nRandomOverSampler-KNeighborsClassifier\n0.460189\n\n\n13\n1\nRandomOverSampler-LogisticRegression\n0.188722\n\n\n14\n1\nSMOTE-KNeighborsClassifier\n0.428110\n\n\n15\n1\nSMOTE-LogisticRegression\n0.189665\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n17\n2\nGeometricSMOTE-LogisticRegression\n0.296189\n\n\n18\n2\nKNeighborsClassifier\n0.522802\n\n\n19\n2\nLogisticRegression\n0.006476\n\n\n20\n2\nRandomOverSampler-KNeighborsClassifier\n0.592432\n\n\n21\n2\nRandomOverSampler-LogisticRegression\n0.290737\n\n\n22\n2\nSMOTE-KNeighborsClassifier\n0.580532\n\n\n23\n2\nSMOTE-LogisticRegression\n0.294199\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n25\n3\nGeometricSMOTE-LogisticRegression\n0.191214\n\n\n26\n3\nKNeighborsClassifier\n0.323485\n\n\n27\n3\nLogisticRegression\n0.006260\n\n\n28\n3\nRandomOverSampler-KNeighborsClassifier\n0.454507\n\n\n29\n3\nRandomOverSampler-LogisticRegression\n0.195133\n\n\n30\n3\nSMOTE-KNeighborsClassifier\n0.428896\n\n\n31\n3\nSMOTE-LogisticRegression\n0.192810\n\n\n\n\n\n\n\nThe next table shows the pipeline with the highest F-score per dataset:\n\ncv_scores_best = cv_scores.loc[cv_scores.groupby(\"Dataset\")[\"Score\"].idxmax()]\ncv_scores_best\n\n\n\n\n\n\n\n\nDataset\nPipeline\nScore\n\n\n\n\n0\n0\nGeometricSMOTE-KNeighborsClassifier\n0.617232\n\n\n8\n1\nGeometricSMOTE-KNeighborsClassifier\n0.487351\n\n\n16\n2\nGeometricSMOTE-KNeighborsClassifier\n0.619463\n\n\n24\n3\nGeometricSMOTE-KNeighborsClassifier\n0.460700\n\n\n\n\n\n\n\nTherefore, Geometric SMOTE outperforms the other methods in all datasets when the F-score is used as an evaluation metric."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "",
    "text": "The imbalanced learning problem describes the case wherein a machine learning classification task, using datasets with binary or multi-class targets, one of the classes, called the majority class, has a significantly higher number of samples compared to the remaining classes, called the minority class(es) (Nitesh V. Chawla et al. 2003). Learning from imbalanced data is a non-trivial problem for both academic researchers and industry practitioners that can be frequently found in multiple domains such as chemical and biochemical engineering, financial management, information technology, security, business, agriculture or emergency management (Haixiang et al. 2017).\nA bias towards the majority class is induced when imbalanced data are used to train standard machine learning algorithms. This results in low classification accuracy, especially for the minority class(es), when the classifier is evaluated on unseen data. An important measure for the degree of data imbalance is the Imbalance Ratio (\\(IR\\)), defined as the ratio between the number of samples of the majority class and each of the minority classes. Using a rare disease detection task as an example, with 1% of positive cases corresponding to an \\(IR=\\frac{0.99}{0.01}=99\\), a trivial classifier that always labels a person as healthy will score a classification accuracy of 99%. However, in this case, all positive cases remain undetected. The observed values of \\(IR\\) are often between 100 and 100.000 (N. V. Chawla et al. 2002), (Barua et al. 2014). Figure 1 presents an example of imbalanced data in two dimensions as well as the decision boundary identified by a typical classifier that uses them as training data.\n\n\n\n\n\n\nFigure 1: Imbalanced data in two dimensions. The decision boundary of a typical classifier shows a bias towards the majority class.\n\n\n\nIn this paper, we present imbalanced-learn-extra, Python library that implements novel oversampling algorithms, including clustering-based oversampling and Geometric SMOTE. The clustering-based approach allows for any combination of a Scikit-Learn (Pedregosa et al. 2012) compatible clustering algorithm and an Imbalanced-Learn (Lemaitre, Nogueira, and Aridas 2016) compatible oversampler. This approach identifies clusters within the input space and applies oversampling individually to each cluster. Additonally, Geometric SMOTE serves as a direct replacement for SMOTE, expanding the data generation mechanism to provide greater flexibility and improved performance.\nIn Theoretical background section various concepts related to oversampling are presented, while in Implementation and architecture section a description of the software’s implementation and architecture is presented."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Geometric SMOTE",
    "text": "Geometric SMOTE\nThe Geometric SMOTE (G-SMOTE) oversampling algorithm (Douzas and Bacao 2019) uses a different approach compared to existing SMOTE’s variations. More specifically, G-SMOTE oversampling algorithm substitutes the data generation mechanism of SMOTE by defining a flexible geometric region around each minority class instance and generating synthetic instances inside the boundaries of this region. The algorithm requires the selection of the hyperparameters truncation_factor , deformation_factor, selection_strategy and k_neighbors. The first three of them, called geometric hyperparameters, control the shape of the geometric region while the later adjusts its size. Figure Figure 3 presents a visual comparison between the data generation mechanisms of SMOTE and G-SMOTE.\n\n\n\n\n\n\nFigure 3: Comparison between the data generation mechanisms of SMOTE and G-SMOTE. SMOTE uses linear interpolation, while G-SMOTE defines a circle as the permissible data generation area."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Clustering-based oversampling",
    "text": "Clustering-based oversampling\nIn addition to between-class imbalance, within-class imbalance refers to the case where areas of sparse and dense minority class instances exist. As the first step of generating synthetic samples, the SMOTE data generation mechanism selects randomly, with uniform probability, minority class instances. Consequently, dense minority class areas have a high probability of being inflated further, while sparsely populated are likely to remain sparse. This allows for combating between-class imbalance, while the issue of within-class imbalance is ignored (Prati, Batista, and Monard 2004).\nOn the other hand, clustering-based oversampling, as presented in (Douzas and Bacao 2017) and (Douzas, Bacao, and Last 2018), aims to deal with both between-class and within-class imbalance problems. Initially, a clustering algorithm is applied to the input space. The resulting clusters allow the identification of sparse and dense minority class(es) areas. A small IR, relative to a threshold, of a particular cluster, is used as an indicator that it can be safely selected as a data generation area, i.e. noise generation is avoided. Furthermore, sparse minority clusters are assigned more synthetic samples, which alleviates within-class imbalance.\nSpecific realizations of the above approach are SOMO (Douzas and Bacao 2017), KMeans-SMOTE (Douzas, Bacao, and Last 2018) and G-SOMO (Douzas, Rauch, and Bacao 2021) algorithms. Empirical studies have shown that the three algorithms outperform SMOTE and its variants across multiple imbalanced datasets, classifiers and evaluation metrics."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#imbalanced-learn-extra",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#imbalanced-learn-extra",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "imbalanced-learn-extra",
    "text": "imbalanced-learn-extra\nA Python implementation of SMOTE and several of its variants is available in the Imbalanced-Learn library (Lemaitre, Nogueira, and Aridas 2016), which is fully compatible with the popular machine learning toolbox Scikit-Learn (Pedregosa et al. 2012). In this paper, we present imbalanced-learn-extra a Python implementation of G-SMOTE and clustering-based oversampling algorithms."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-1",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-1",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Geometric SMOTE",
    "text": "Geometric SMOTE\nThe main module of gsmote is called geometric-smote.py. It contains the class GeometricSMOTE that implements the G-SMOTE algorithm. The initialization of a GeometricSMOTE instance includes G-SMOTE’s hyperparameters that control the generation of synthetic data. Additionally, GeometricSMOTE inherits from the BaseOverSampler class of Imbalanced-Learn library. Therefore, an instance of GeometricSMOTE class provides the fit and fit_resample methods, the two main methods for resampling as explained in subsection. This is achieved by implementing the fit_resample abstract method of the parent class BaseOverSampler. More specifically, the function _make_geometric_sample implements the data generation mechanism of G-SMOTE as shortly described in subsection. This function is called in the _make_geometric_samples method of the GeometricSMOTE class in order to generate the appropriate number of synthetic data for a particular minority class. Finally, the method _make_geometric_samples is called in _fit_resample method to generate synthetic data for all minority classes. Figure #fig-class_diagram provides a visual representation of the above classes and functions hierarchy.\n\n\n\n\n\n\nFigure 4: UML class diagrams and callgraphs of main classes and methods."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-1",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-1",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Clustering-based oversampling",
    "text": "Clustering-based oversampling\nThe main module of clover are distribution and over_sampling. The distribution module implements the functionality related to the distribution of the generated samples to the identified clusters, while over_sampling implements the functionality related to the generation of artificial samples. Both of them are presented in detail below.\n\nDistribution\nThe module distribution contains the files base.py and _density.py. The former provides the implementation of the BaseDistributor class, the base class for distributors, while the latter includes the DensityDistributor class, a generalization of the density-based distributor presented in (Douzas and Bacao 2017) and (Douzas, Bacao, and Last 2018), that inherits from BaseDistributor. Following the Scikit-Learn API, BaseDistributor includes the public method fit. Also the fit_distribute method is also implemented as the main method of the class.\nThe fit_distribute method calls the fit method and returns two Python dictionaries that describe the distribution of generated samples inside each cluster and between clusters, respectively. Specifically, the fit method calculates various statistics related to the distribution process, while it calls the _fit method to calculate the actual intra-cluster and inter-cluster distributions. This is achieved by invoking the _intra_distribute and _inter_distribute methods. The BaseDistributor class provides a trivial implementation of them, that should be overwritten when a realization of a distributor class is considered. Therefore, DensityDistributor overwrites both methods as well as the _fit method. The later calls the methods _identify_filtered_clusters and _calculate_clusters_density that identify the clusters used for data generation and calculate their density, respectively. Subsection Software funtionalities provides a detailed description of the initialization and functionality of the DensityDistributor class. Figure 5 shows a visual representation of the above classes and functions hierarchy.\n\n\n\n\n\n\nFigure 5: UML BaseDistributor and DensityDistributor class diagrams and callgraphs of main classes and methods.\n\n\n\n\n\nOversampling\nThe module over_sampling contains the files _cluster.py, _kmeans_smote.py, _somo.py and _gsomo.py. The former provides the ClusterOverSampler class, an extension of the Imbalanced-Learn’s BaseOverSampler class, and implements the functionality required by clustering-based oversampling. The rest of the files _kmeans_smote.py, _somo.py and _gsomo.py utilize the ClusterOverSampler class to provide implementations of KMeans SMOTE, SOMO and Geometric SOMO algorithms, respectively. The initializer of ClusterOverSampler, compared to the base class of oversamplers that is implemented in Imbalanced-Learn BaseOverSampler, includes the extra parameters clusterer and distributor and inherits from it. Also following the Imbalanced-Learn API, ClusterOverSampler includes the public methods fit and fit_resample.\nThe fit method calculates various statistics related to the resampling process, while the fit_resample method returns an enhanced version of the input data by appending the artificially generated samples. Specifically, fit_resample calls the _fit_resample method that in turn calls the _intra_sample and _inter_sample methods to generate the intra-cluster and inter-cluster artificial samples, respectively. This is achieved by invoking the _fit_resample_cluster method that implements the data generation mechanism. Therefore every oversampler that inherits from the ClusterOverSampler class should overwrite _fit_resample_cluster, providing a concrete implementation of the oversampling process. Software functionalities provides a detailed description of the initialization and functionality of the various oversamplers, enhanced by the clustering process. Figure 6 shows a visual representation of the above classes and functions hierarchy.\n\n\n\n\n\n\nFigure 6: UML BaseOverSampler and BaseClusterOversampler class diagrams and callgraphs of main classes and methods."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-2",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-2",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Geometric SMOTE",
    "text": "Geometric SMOTE\nAs it was mentioned in subsection, the class GeometricSMOTE represents the G-SMOTE oversampler. The intializer of GeometricSMOTE includes the following G-SMOTE’s hyperparameters: truncation_factor, deformation_factor, selection_strategy and k_neighbors as explained in subsection. Once the GeometricSMOTE object is initialized with a specific parametrization, it can be used to resample the imbalanced data represented by the input matrix X and the target labels y. Following the Scikit-Learn API, both X, y are array-like objects of appropriate shape.\nResampling is achieved by using the two main methods of fit and fit_resample of the GeometricSMOTE object. More specifically, both of them take as input parameters the X and y. The first method computes various statistics which are used to resample X while the second method does the same but additionally returns a resampled version of X and y.\nThe geometric-smote project has been designed to integrate with the Imbalanced-Learn toolbox and Scikit-Learn ecosystem. Therefore the GeometricSMOTE object can be used in a machine learning pipeline, through Imbalanced-Learn’s class Pipeline, that automatically combines samplers, transformers and estimators. The next section provides examples of the above functionalities."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-2",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-2",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Clustering-based oversampling",
    "text": "Clustering-based oversampling\nAs it was mentioned in section Theoretical background, clustering-based oversampling initially applies a clustering algorithm to the input space before oversampling is applied to each cluster. This is achieved through the implementation of the ClusterOverSampler class, an extension of Imbalanced-Learn’s BaseOverSampler class. Oversamplers that inherit from ClusterOverSampler, compared to oversamplers inheriting from BaseOverSampler, require two additional initialization parameters: clusterer and distributor. Their default values are for both parameters equal to None, a case that corresponds to the usual oversampling procedure i.e. no clustering applied to the input space. On the other hand if the parameter clusterer is equal to any Scikit-Learn compatible clustering algorithm then clustering of the input space is initially applied, followed by oversampling in each cluster with the distribution of generated samples calculated by the distributor parameter. The default distributor value is an instance of DensityDistributor class as described in subsection Distribution.\nThe initializer of DensityDistributor includes the following parameters: filtering_threshold, distances_exponent, sparsity_based and distribution_factor. The first parameter is used to identify the filtered clusters, i.e. clusters of samples that are included in the data generation process. The second parameter modifies the density calculation of the filtered clusters by increasing the effect of euclidean distances between samples. The third parameter selects whether generated samples are assigned to filtered clusters inversely proportional to their density. Finally, the last parameter adjusts the intra-cluster to the inter-cluster proportion of generated samples, while it applies only to clusterers that support a neighborhood structure. Once the DensityDistributor object is initialized with a specific parametrization, it can be used to distribute the generated samples to the clusters identified by any clustering algorithm.\nResampling is achieved by using the two main methods of fit and fit_resample of any oversampler inheriting from ClusterOverSampler. More specifically, both of them take as input parameters the input matrix X and target labels y. Following the Scikit-Learn API, both X, y are array-like objects of appropriate shape. The first method computes various statistics which are used to resample X, while the second method does the same but additionally returns a resampled version of X and y.\nThe cluster-over-sampling project has been designed to integrate with the Imbalanced-Learn toolbox and the Scikit-Learn ecosystem. Therefore all oversamplers that inherit from ClusterOverSampler can be used in machine learning pipelines, through Imbalanced-Learn’s class Pipeline, that automatically combines samplers, transformers and estimators. The next section provides examples of the above functionalities."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-3",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#geometric-smote-3",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Geometric SMOTE",
    "text": "Geometric SMOTE\nAn example of resampling multi-class imbalanced data using the fit_resample method is presented in the next listing. Initially, a 3-class imbalanced dataset is generated. Next, GeometricSMOTE object is initialized with default values for the hyperparameters, i.e. truncation_factor=1.0, deformation_factor=0.0, selection_strategy='combined'. Finally, the object’s fit_resample method is used to resample the data. Printing the class distribution before and after resampling confirms that the resampled data X_res, y_res are perfectly balanced. X_res, y_res can be used as training data for any classifier in the place of X, y.\n\n# Import classes and functions.\nfrom collections import Counter\nfrom imblearn_extra.gsmote import GeometricSMOTE\nfrom sklearn.datasets import make_classification\n\n# Generate an imbalanced 3-class dataset.\nX, y = make_classification(\n  random_state=23, \n  n_classes=3, \n  n_informative=5,\n  n_samples=500, \n  weights=[0.8, 0.15, 0.05]\n)\n\n# Create a GeometricSMOTE object with default hyperparameters.\ngsmote = GeometricSMOTE(random_state=10)\n\n# Resample the imbalanced dataset.\nX_res, y_res = gsmote.fit_resample(X, y)\n\n# Print number of samples per class for initial and resampled data.\ninit_count = list(Counter(y).values()) \nresampled_count = list(Counter(y_res).values())\nprint(f'Initial class distribution: {init_count}.')\nprint(f'Resampled class distribution: {resampled_count}.')\n\nInitial class distribution: [400, 75, 25].\nResampled class distribution: [400, 400, 400]."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#machine-learning-pipeline",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#machine-learning-pipeline",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Machine learning pipeline",
    "text": "Machine learning pipeline\nAs mentioned before, the GeometricSMOTE object can be used as a part of a machine learning pipeline. The next listing presents a pipeline composed by a G-SMOTE oversampler, a PCA tranformation and a decision tree classifier. The pipeline is trained on imbalanced binary-class data and evaluated on a hold-out set. The user applies the process in a simple way while the internal details of the calculations are hidden.\n\n# Import classes and functions.\nfrom imblearn_extra.gsmote import GeometricSMOTE \nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom imblearn.pipeline import make_pipeline\n\n# Generate an imbalanced binary-class dataset.\nX, y = make_classification(\n  random_state=23,\n  n_classes=2,\n  n_samples=500,\n  weights=[0.8, 0.2],\n)\n\n# Split the data to training and hold-out sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Create the pipeline's objects with default hyperparameters.\ngsmote = GeometricSMOTE(random_state=11)\npca = PCA()\nclf = DecisionTreeClassifier(random_state=3)\n\n# Create the pipeline.\npip = make_pipeline(gsmote, pca, clf)\n\n# Fit the pipeline to the training set.\npip.fit(X_train, y_train)\n\n# Evaluate the pipeline on the hold-out set using the F-score.\ntest_score = f1_score(y_test, pip.predict(X_test))\n\nprint(f'F-score on hold-out set: {test_score}.')\n\nF-score on hold-out set: 0.7."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-3",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#clustering-based-oversampling-3",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Clustering-based oversampling",
    "text": "Clustering-based oversampling\nExamples of cluster-over-sampling usage are given below and include a basic example and a machine learning pipeline. Both use clustering-based oversamplers to generate artificial data."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#basic-example",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#basic-example",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Basic example",
    "text": "Basic example\nAn example of resampling an imbalanced dataset using the fit_resample method is presented. Initially, a binary-class imbalanced dataset is generated. Next, KMeansSMOTE oversampler is initialized with the default parameters. This corresponds to the KMeans-SMOTE algorithm as presented in (Douzas, Bacao, and Last 2018). Finally, the oversampler’s fit_resample method is used to resample the data. Printing the class distribution before and after resampling confirms that the resampled data X_res, y_res are perfectly balanced. X_res, y_res can be used as training data for any classifier in the place of X, y.\n\n# Import classes and functions.\nfrom collections import Counter\nfrom imblearn_extra.clover.over_sampling import KMeansSMOTE\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_classification\n\n# Generate an imbalanced binary class dataset.\nX, y = make_classification(\n    random_state=23, \n    n_classes=2,\n    n_features=5,\n    n_samples=1000,\n    weights=[0.8, 0.2]\n)\n\n# Create KMeans-SMOTE object with default hyperparameters.\nkmeans_smote = KMeansSMOTE(random_state=10)\n\n# Resample the imbalanced dataset.\nX_res, y_res = kmeans_smote.fit_resample(X, y) \n\n# Print number of samples per class for initial and resampled data. \ninit_count = list(Counter(y).values())\nresampled_count = list(Counter(y_res).values())\n\nprint(f'Initial class distribution: {init_count}.') \nprint(f'Resampled class distribution: {resampled_count}.')\n\nInitial class distribution: [792, 208].\nResampled class distribution: [792, 792]."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#machine-learning-pipeline-1",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#machine-learning-pipeline-1",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Machine learning pipeline",
    "text": "Machine learning pipeline\nAs mentioned before, any clustering-based oversampler can be used as a part of a machine learning pipeline. A a pipeline is presented, composed by the combination of Borderline SMOTE oversampler and hierarchical clustering, a PCA tranformation and a decision tree classifier. The pipeline is trained on multi-class imbalanced data and evaluated on a hold-out set. The user applies the process in a simple way while the internal details of the calculations are hidden.\n\n# Import classes and functions.\nfrom imblearn_extra.clover.over_sampling import ClusterOverSampler\nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import AgglomerativeClustering\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import make_pipeline\n\n# Generate an imbalanced multi-class dataset.\nX, y = make_classification(\n    random_state=23, \n    n_classes=3, \n    n_informative=10,\n    n_samples=500,\n    weights=[0.8, 0.1, 0.1]\n)\n\n# Split the data to training and hold-out sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n\n# Create the pipeline's objects with default hyperparameters.\nhclusterer_bsmote = ClusterOverSampler(oversampler=BorderlineSMOTE(random_state=5), clusterer=AgglomerativeClustering(), random_state=19)\npca = PCA()\nclf = DecisionTreeClassifier(random_state=3)\n\n# Create the pipeline.\npip = make_pipeline(hclusterer_bsmote, pca, clf)\n\n# Fit the pipeline to the training set.\npip.fit(X_train, y_train)\n\n# Evaluate the pipeline on the hold-out set using the F-score.\ntest_score = f1_score(y_test, pip.predict(X_test), average='micro')\n\nprint(f'F-score on hold-out set: {test_score:.2f}.')\n\nF-score on hold-out set: 0.75."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#operating-system",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#operating-system",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Operating system",
    "text": "Operating system\nAny system (GNU/Linux, Mac OSX, Windows) capable of running Python ≥ 3.10."
  },
  {
    "objectID": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#programming-language",
    "href": "publications/imbalanced-learn-implementation-of-novel-oversampling-algorithms/index.html#programming-language",
    "title": "imbalanced-learn-extra: A Python package for novel oversampling algorithms",
    "section": "Programming language",
    "text": "Programming language\nPython 3.10, or higher.\n\nDependencies\n\nscipy &gt;= 1.7.2\nnumpy &gt;= 1.22\nscikit-learn &gt;= 1.1.1\nimbalanced-learn &gt;= 0.9.0\n\n\n\nList of contributors\nThe software was created by Georgios Douzas.\n\n\nSoftware location\n\nZenodo\n\nName: cluster-over-sampling\nPersistent identifier: https://doi.org/10.5281/zenodo.3370372\nLicence: MIT License\nPublisher: Zenodo\nVersion published: 0.4.0\nDate published: 01/10/2021\n\n\n\nGitHub\n\nName: cluster-over-sampling\nPersistent identifier: https://github.com/georgedouzas/imbalanced-learn-extra\nLicence: MIT\nDate published: 01/10/2021\n\n\n\n\nLanguage\nEnglish"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Georgios Douzas",
    "section": "",
    "text": "My articles on open-source, machine learning, mathematics, and more.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to affine spaces and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild, deploy and observe data workflows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of novel oversampling algorithms.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Georgios Douzas",
    "section": "",
    "text": "My articles on open-source, machine learning, mathematics, and more.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to affine spaces and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild, deploy and observe data workflows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of novel oversampling algorithms.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Georgios Douzas",
    "section": "Publications",
    "text": "Publications\nPublished work and work in progress.\n\n\n\n\n\n\n\n\n\n\nIntraday trading via Deep Reinforcement Learning and Technical Indicators\n\n\nWork In Progress.\n\n\n\n\n\n\n\n\n\n\n\n\n\nimbalanced-learn-extra: A Python package for novel oversampling algorithms\n\n\nWork In Progress.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#presentations",
    "href": "index.html#presentations",
    "title": "Georgios Douzas",
    "section": "Presentations",
    "text": "Presentations\nSlides and materials I have created.\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning and LLM-based Agents\n\n\n\nArtificial Intelligence\n\n\nReinforcement Learning\n\n\nLLM\n\n\n\nLeveraging Reinforcement Learning and LLMs for building smart, adaptive agents.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]