[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "\\(8/20\\) - Present\nTrasys, Greece\nDesigned and implemented Parallel Distribution, a software tool for the European Medicines Agency that applies OCR on PDF documents and generates a comparison report. The primary language of implementation was Python, while various text mining and machine learning libraries were used. The frontend of the tool used HTML, CSS, and JavaScript to provide an interactive dashboard of the results.\n\n\n\n\\(09/13\\) - \\(09/14\\) & \\(09/18\\) - Present\nUniversity of Lisbon Nova IMS, Portugal\nDesigned, implemented, and tested various new approaches for the class imbalance problem. Research focused on clustering-based over-sampling methods that deal with the within-the-class imbalance problem. Additionally, Geometric SMOTE, an extension of the SMOTE algorithm, was proposed and implemented. The final publication presented results showing a significant improvement over SMOTE and its variations. Deep learning models, particularly Conditional Generative Adversarial Networks (CGANs), were also used as over-sampling methods with great success. The frameworks of the implementation were TensorFlow, Keras, and PyTorch. Work is published in high-impact machine learning journals. Implementation of the above algorithms was developed and made available as open-source software. Work in progress includes comparative experiments between variations of CGANs as over-samplers and the investigation of novel algorithms in the context of reinforcement learning.\n\n\n\n\\(10/17\\) - \\(08/18\\)\nTripsta, Greece\nDesigned and implemented the main parts of the company’s automated pricing system. These parts included machine learning estimators for the add-ons and the competitor’s prices and the application of metaheuristic algorithms for the budget multi-objective optimization problem. The training data of the various estimators were at the order of TB while the prediction time of the automated pricing system was required to be less than \\(100\\) msec for the incoming \\(50\\)K requests/sec. The implementation languages were Python, Java, and Scala, while Spark, Dask, Scikit-Learn and jMetal were used as distributed data processing, machine learning, and optimization frameworks/libraries.\n\n\n\n\\(12/16\\) - \\(09/17\\)\nQuantum Retail\nWorked on demand forecasting and clustering for retail companies. Proposed and applied machine learning methods to improve the company’s main forecasting solution based on exponential smoothing of the time series data and adjustments guided by a seasonality curve. Boosting trees were selected as the final machine-learning model. Applied feature extraction that integrated the business logic and extensive model hyperparameter tuning, the forecasting precision was improved by \\(30\\)% compared to the original model.\n\n\n\n\\(05/16\\) - \\(09/16\\)\nCERN, Remote\nDeveloped the parallelization of various features for TMVA, the Toolkit for Multivariate Data Analysis with ROOT, as a part of a project funded by Google. ROOT is the main framework developed by CERN to deal with the big data processing, statistical analysis, visualization, and storage of massive amounts of data produced from particle physics experiments. The legacy version was implemented in C++. The parallelized features included the application of brute-force and metaheuristic algorithms to the hyperparameter grid search of machine learning algorithms. The implementation was based on Python and Spark.\n\n\n\n\\(01/14\\) - \\(05/16\\)\nIRI, Greece\nMember of the IRI’s “Solutions and Innovation Team” (R&D) working on the company’s transition towards Open Source and Elastic Computing. Participated in an agile team migrating IRI’s leading US “Price & Promo Analytics” Solution, generating more than \\(\\$25\\)M Annual Revenues, to Hadoop distributed storage and Spark cluster computing. Python was the core language of the implementation, but integration with R and Julia was performed to leverage unique functionality. The legacy version was implemented in SAS. The project’s main objectives were the design of the parallelization schema, the enhancement of data manipulation with the use of distributed processing, and the migration of the statistical modeling algorithms (regression mixed models). The final system processed \\(5\\) years of data for more than \\(300\\) categories containing \\(1\\) million products.\n\n\n\n\n\n\n\\(09/03\\) - \\(09/08\\)\nNational Technical University of Athens\n\n\n\n\\(09/01\\) - \\(09/03\\)\nNational Technical University of Athens\n\n\n\n\\(09/97\\) - \\(09/01\\)\nNational and Kapodistrian University of Athens"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "CV",
    "section": "",
    "text": "\\(8/20\\) - Present\nTrasys, Greece\nDesigned and implemented Parallel Distribution, a software tool for the European Medicines Agency that applies OCR on PDF documents and generates a comparison report. The primary language of implementation was Python, while various text mining and machine learning libraries were used. The frontend of the tool used HTML, CSS, and JavaScript to provide an interactive dashboard of the results.\n\n\n\n\\(09/13\\) - \\(09/14\\) & \\(09/18\\) - Present\nUniversity of Lisbon Nova IMS, Portugal\nDesigned, implemented, and tested various new approaches for the class imbalance problem. Research focused on clustering-based over-sampling methods that deal with the within-the-class imbalance problem. Additionally, Geometric SMOTE, an extension of the SMOTE algorithm, was proposed and implemented. The final publication presented results showing a significant improvement over SMOTE and its variations. Deep learning models, particularly Conditional Generative Adversarial Networks (CGANs), were also used as over-sampling methods with great success. The frameworks of the implementation were TensorFlow, Keras, and PyTorch. Work is published in high-impact machine learning journals. Implementation of the above algorithms was developed and made available as open-source software. Work in progress includes comparative experiments between variations of CGANs as over-samplers and the investigation of novel algorithms in the context of reinforcement learning.\n\n\n\n\\(10/17\\) - \\(08/18\\)\nTripsta, Greece\nDesigned and implemented the main parts of the company’s automated pricing system. These parts included machine learning estimators for the add-ons and the competitor’s prices and the application of metaheuristic algorithms for the budget multi-objective optimization problem. The training data of the various estimators were at the order of TB while the prediction time of the automated pricing system was required to be less than \\(100\\) msec for the incoming \\(50\\)K requests/sec. The implementation languages were Python, Java, and Scala, while Spark, Dask, Scikit-Learn and jMetal were used as distributed data processing, machine learning, and optimization frameworks/libraries.\n\n\n\n\\(12/16\\) - \\(09/17\\)\nQuantum Retail\nWorked on demand forecasting and clustering for retail companies. Proposed and applied machine learning methods to improve the company’s main forecasting solution based on exponential smoothing of the time series data and adjustments guided by a seasonality curve. Boosting trees were selected as the final machine-learning model. Applied feature extraction that integrated the business logic and extensive model hyperparameter tuning, the forecasting precision was improved by \\(30\\)% compared to the original model.\n\n\n\n\\(05/16\\) - \\(09/16\\)\nCERN, Remote\nDeveloped the parallelization of various features for TMVA, the Toolkit for Multivariate Data Analysis with ROOT, as a part of a project funded by Google. ROOT is the main framework developed by CERN to deal with the big data processing, statistical analysis, visualization, and storage of massive amounts of data produced from particle physics experiments. The legacy version was implemented in C++. The parallelized features included the application of brute-force and metaheuristic algorithms to the hyperparameter grid search of machine learning algorithms. The implementation was based on Python and Spark.\n\n\n\n\\(01/14\\) - \\(05/16\\)\nIRI, Greece\nMember of the IRI’s “Solutions and Innovation Team” (R&D) working on the company’s transition towards Open Source and Elastic Computing. Participated in an agile team migrating IRI’s leading US “Price & Promo Analytics” Solution, generating more than \\(\\$25\\)M Annual Revenues, to Hadoop distributed storage and Spark cluster computing. Python was the core language of the implementation, but integration with R and Julia was performed to leverage unique functionality. The legacy version was implemented in SAS. The project’s main objectives were the design of the parallelization schema, the enhancement of data manipulation with the use of distributed processing, and the migration of the statistical modeling algorithms (regression mixed models). The final system processed \\(5\\) years of data for more than \\(300\\) categories containing \\(1\\) million products."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "",
    "text": "\\(09/03\\) - \\(09/08\\)\nNational Technical University of Athens\n\n\n\n\\(09/01\\) - \\(09/03\\)\nNational Technical University of Athens\n\n\n\n\\(09/97\\) - \\(09/01\\)\nNational and Kapodistrian University of Athens"
  },
  {
    "objectID": "posts/geometric-smote/index.html",
    "href": "posts/geometric-smote/index.html",
    "title": "Geometric SMOTE algorithm",
    "section": "",
    "text": "Geometric SMOTE algorithm in action"
  },
  {
    "objectID": "posts/geometric-smote/index.html#implementation",
    "href": "posts/geometric-smote/index.html#implementation",
    "title": "Geometric SMOTE algorithm",
    "section": "Implementation",
    "text": "Implementation\nI have developed a Python implementation of the Geometric SMOTE oversampler called geometric-smote, which integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems. You can check the documentation for more details on installation and the API."
  },
  {
    "objectID": "posts/geometric-smote/index.html#functionality",
    "href": "posts/geometric-smote/index.html#functionality",
    "title": "Geometric SMOTE algorithm",
    "section": "Functionality",
    "text": "Functionality\nLet’s first generate a binary class imbalanced dataset, represented by the input matrix X and the target vector y:\n\n# Imports\nfrom sklearn.datasets import make_classification\n\n# Set random seed\nrnd_seed = 43\n\n# Generate imbalanced data\nX, y = make_classification(\n  n_samples=1000,\n  n_features=10,\n  n_classes=2,\n  weights=[0.95, 0.05],\n  random_state=rnd_seed,\n  n_informative=7,\n  class_sep=0.1\n)\n\nThe function print_characteristics extracts and prints the main characteristics of a binary class dataset. Specifically, it prints the number of samples, the number of features, the labels, and the number of samples for the majority and minority classes as well as the Imbalance Ratio, defined as the ratio between the number of instances of the majority and minority classes.\n\n# Imports\nfrom collections import Counter\n\n# Define function to print dataset's characteristics\ndef print_characteristics(X, y):\n  n_samples, n_features = X.shape\n  count_y = Counter(y)\n  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()\n  ir = n_samples_maj / n_samples_min\n  print(\n    f'Number of samples: {n_samples}',\n    f'Number of features: {n_features}',\n    f'Majority class label: {maj_label}',\n    f'Number of majority class samples: {n_samples_maj}',\n    f'Minority class label: {min_label}',\n    f'Number of minority class samples: {n_samples_min}',\n    f'Imbalance Ratio: {ir:.1f}',\n    sep='\\n'\n  )\n\nI use the above function to print the main characteristics of the generated imbalanced dataset:\n\nprint_characteristics(X, y)\n\nNumber of samples: 1000\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 946\nMinority class label: 1\nNumber of minority class samples: 54\nImbalance Ratio: 17.5\n\n\nThe class that represents the Geometric SMOTE oversampler is called GeometricSMOTE. The most basic functionality is to resample an imbalanced dataset. Following the Imbalanced-Learn API, the fit_resample method of a GeometricSMOTE instance can be used to resample the imbalanced dataset:\n\n# Imports\nfrom gsmote import GeometricSMOTE\n\n# Create GeometricSMOTE instance\ngeometric_smote = GeometricSMOTE(random_state=rnd_seed + 5)\n\n# Fit and resample imbalanced data\nX_res, y_res = geometric_smote.fit_resample(X, y)\n\nAgain we can print the main characteristics of the rebalanced dataset:\n\nprint_characteristics(X_res, y_res)\n\nNumber of samples: 1892\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 946\nMinority class label: 1\nNumber of minority class samples: 946\nImbalance Ratio: 1.0\n\n\nAs expected, the default behavior of the GeometricSMOTE instance is to generate the appropriate number of minority class samples so that the resampled dataset is perfectly balanced.\nAs mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function calculate_cv_scores calculates the average 5-fold cross-validation geometric mean and accuracy scores across 10 runs of a logistic regression classifier that is optionally combined with an oversampler through a pipeline:\n\n# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.metrics import geometric_mean_score\n\n# Define function that calculates out-of-sample scores\ndef calculate_cv_scores(oversampler, X, y):\n  cv_scores= []\n  scorers = {'g_mean': make_scorer(geometric_mean_score), 'accuracy': make_scorer(accuracy_score)}\n  n_runs = 10\n  for ind in range(n_runs):\n    rnd_seed = 10 * ind\n    classifier = LogisticRegression(random_state=rnd_seed)\n    if oversampler is not None:\n      classifier = make_pipeline(\n        oversampler.set_params(random_state=rnd_seed + 4),\n        classifier\n      )\n    scores = cross_validate(\n      estimator=classifier,\n      X=X,\n      y=y,\n      scoring=scorers,\n      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)\n    )\n    cv_scores.append([scores[f'test_{scorer}'].mean() for scorer in scorers])\n  return np.mean(cv_scores, axis=0)\n\nUsing the above function, we can calculate the out-of-sample performance when no oversampling is applied as well as when random oversampling, SMOTE and Geometric SMOTE are used as oversamplers:\n\n# Imports\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\n\n# Calculate cross-validation scores\nmapping = {'No oversampling': None, 'Random Oversampling': RandomOverSampler(), 'SMOTE': SMOTE(), 'Geometric SMOTE': GeometricSMOTE()}\ncv_scores = {}\nfor name, oversampler in mapping.items():\n  cv_scores[name] = calculate_cv_scores(oversampler, X, y)\ncv_scores = pd.DataFrame(cv_scores, index = ['Geometric Mean', 'Accuracy'])\ncv_scores\n\n\n\n\n\n\n\n\nNo oversampling\nRandom Oversampling\nSMOTE\nGeometric SMOTE\n\n\n\n\nGeometric Mean\n0.132235\n0.592189\n0.583983\n0.602286\n\n\nAccuracy\n0.949400\n0.670400\n0.673700\n0.662700\n\n\n\n\n\n\n\nGeometric SMOTE outperforms the other methods when the geometric mean score is used as an evaluation metric. At the same time, the highest accuracy is achieved when no oversampling is applied, although if the goal is to consider performance equally on all classes, it is not a suitable metric for imbalanced data."
  },
  {
    "objectID": "posts/clustering-based-oversampling/index.html",
    "href": "posts/clustering-based-oversampling/index.html",
    "title": "Clustering-based oversampling",
    "section": "",
    "text": "Clustering-based oversampling"
  },
  {
    "objectID": "posts/clustering-based-oversampling/index.html#introduction",
    "href": "posts/clustering-based-oversampling/index.html#introduction",
    "title": "Clustering-based oversampling",
    "section": "Introduction",
    "text": "Introduction\nSMOTE algorithm and its variants generate synthetic samples along line segments that join minority class instances. SMOTE addresses only the between-classes imbalance. On the other hand, SMOTE does nothing about areas of the input space that differ significantly in the density of a particular class, an issue known as within-classes imbalance.\nA straightforward approach is to combine oversamplers with clustering algorithms. SOMO and KMeans-SMOTE algorithms are specific realizations of this approach that have been shown to outperform other standard oversamplers in a large number of datasets."
  },
  {
    "objectID": "posts/clustering-based-oversampling/index.html#implementation",
    "href": "posts/clustering-based-oversampling/index.html#implementation",
    "title": "Clustering-based oversampling",
    "section": "Implementation",
    "text": "Implementation\nI have developed a Python implementation of the above clustering-based oversampling approach called cluster-over-sampling, which integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems. You can check the documentation for more details on installation and the API."
  },
  {
    "objectID": "posts/clustering-based-oversampling/index.html#functionality",
    "href": "posts/clustering-based-oversampling/index.html#functionality",
    "title": "Clustering-based oversampling",
    "section": "Functionality",
    "text": "Functionality\nLet’s first generate a binary class imbalanced dataset, represented by the input matrix X and the target vector y. Using a high value for the flip_y parameter, we ensure that the data are noisy thus the clustering of the input space will help the oversampling process:\n\n# Imports\nfrom sklearn.datasets import make_classification\n\n# Set random seed\nrnd_seed = 4\n\n# Generate imbalanced data\nX, y = make_classification(\n  n_samples=500,\n  n_classes=2,\n  weights=[0.9, 0.1],\n  random_state=rnd_seed,\n  n_informative=3,\n  class_sep=1.0,\n  n_features=10,\n  flip_y=0.3\n)\n\nThe function print_characteristics extracts and prints the main characteristics of a binary class dataset. Specifically, it prints the number of samples, the number of features, the labels, and the number of samples for the majority and minority classes as well as the Imbalance Ratio, defined as the ratio between the number of instances of the majority and minority classes.\n\n# Imports\nfrom collections import Counter\n\n# Define function to print dataset's characteristics\ndef print_characteristics(X, y):\n  n_samples, n_features = X.shape\n  count_y = Counter(y)\n  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()\n  ir = n_samples_maj / n_samples_min\n  print(\n    f'Number of samples: {n_samples}',\n    f'Number of features: {n_features}',\n    f'Majority class label: {maj_label}',\n    f'Number of majority class samples: {n_samples_maj}',\n    f'Minority class label: {min_label}',\n    f'Number of minority class samples: {n_samples_min}',\n    f'Imbalance Ratio: {ir:.1f}',\n    sep='\\n'\n  )\n\nI use the above function to print the main characteristics of the generated imbalanced dataset:\n\nprint_characteristics(X, y)\n\nNumber of samples: 500\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 379\nMinority class label: 1\nNumber of minority class samples: 121\nImbalance Ratio: 3.1\n\n\nI will combine the SMOTE oversampler and the KMeans clusterer to rebalance the above dataset. The combined clusterer-oversampler can be constructed by importing the SMOTE oversampler from Imbalanced-Learn, KMeans from Scikit-Learn, and ClusterOverSampler from cluster-over-sampling. Then following the Imbalanced Learn API, we can use the fit_resample method to resample the imbalanced dataset:\n\n# Imports\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.cluster import KMeans\nfrom clover.over_sampling import ClusterOverSampler\n\n# Create KMeans-SMOTE instance\nsmote = SMOTE(random_state=rnd_seed + 1)\nkmeans = KMeans(n_clusters=10, random_state=rnd_seed + 3, n_init=50)\nkmeans_smote = ClusterOverSampler(oversampler=smote, clusterer=kmeans)\n\n# Fit and resample imbalanced data\nX_res, y_res = kmeans_smote.fit_resample(X, y)\n\nAgain we can print the main characteristics of the rebalanced dataset:\n\nprint_characteristics(X_res, y_res)\n\nNumber of samples: 757\nNumber of features: 10\nMajority class label: 0\nNumber of majority class samples: 379\nMinority class label: 1\nNumber of minority class samples: 378\nImbalance Ratio: 1.0\n\n\nThe default behavior is to generate the appropriate number of minority class samples so that the resampled dataset is perfectly balanced (although this sometimes may result in an approximately balanced dataset). Also, cluster-over-sampling provides for convenience, the clustering-based oversamplers SOMO and KMeans-SMOTE, as well as G-SOMO that uses Geometric SMOTE as the oversampler in place of SMOTE.\nAs mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function calculate_cv_scores calculates the average 5-fold cross-validation F and accuracy scores across 5 runs of a RandomForestClassifier that is optionally combined with an oversampler through a pipeline:\n\n# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\nfrom imblearn.pipeline import make_pipeline\n\n# Define function that calculates out-of-sample scores\ndef calculate_cv_scores(oversampler, X, y):\n  cv_scores = []\n  scorers = {'f_score': make_scorer(f1_score), 'accuracy': make_scorer(accuracy_score)}\n  n_runs = 5\n  for ind in range(n_runs):\n    rnd_seed = 8 * ind\n    classifier = RandomForestClassifier(random_state=rnd_seed)\n    if oversampler is not None:\n      classifier = make_pipeline(\n        oversampler.set_params(random_state=rnd_seed + 5),\n        classifier\n      )\n    scores = cross_validate(\n      estimator=classifier,\n      X=X,\n      y=y,\n      scoring=scorers,\n      cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=rnd_seed + 6)\n    )\n    cv_scores.append([scores[f'test_{scorer}'].mean() for scorer in scorers])\n  return np.mean(cv_scores, axis=0)\n\nUsing the above function, we can calculate the out-of-sample performance when no oversampling is applied, as well as when SMOTE and DBSCAN-SMOTE are used as oversamplers:\n\n# Imports\nfrom sklearn.cluster import DBSCAN\nfrom clover.over_sampling import ClusterOverSampler\n\n# Calculate cross-validation scores\nmapping = {'No oversampling': None, 'SMOTE': SMOTE(), 'DBSCAN-SMOTE': ClusterOverSampler(oversampler=SMOTE(), clusterer=DBSCAN())}\ncv_scores = {}\nfor name, oversampler in mapping.items():\n  cv_scores[name] = calculate_cv_scores(oversampler, X, y)\ncv_scores = pd.DataFrame(cv_scores, index = ['F-score', 'Accuracy'])\ncv_scores\n\n\n\n\n\n\n\n\nNo oversampling\nSMOTE\nDBSCAN-SMOTE\n\n\n\n\nF-score\n0.250849\n0.349954\n0.35735\n\n\nAccuracy\n0.773200\n0.715600\n0.72120\n\n\n\n\n\n\n\nNotice that using accuracy as an evaluation metric is not a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, the F-score is an appropriate evaluation metric for imbalanced data since it considers the accuracies per class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Clustering-based oversampling\n\n\n\n\n\n\n\nProject\n\n\nImbalanced Learning\n\n\n\n\nCombining clustering and oversampling to increase classification performance.\n\n\n\n\n\n\nMay 2, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\n  \n\n\n\n\nGeometric SMOTE algorithm\n\n\n\n\n\n\n\nProject\n\n\nImbalanced Learning\n\n\n\n\nExtending SMOTE’s data generation mechanism.\n\n\n\n\n\n\nMay 1, 2022\n\n\nGeorgios Douzas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Georgios Douzas. I am a machine learning researcher at Nova IMS, University of Lisbon, and a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence, with multiple publications in machine learning and high-energy physics journals. My professional experience includes working as a software and machine learning engineer for various companies. Additionally, I often maintain or contribute to open-source projects."
  }
]